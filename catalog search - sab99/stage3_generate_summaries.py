# -*- coding: utf-8 -*-
"""
Stage 3: Generate Document Summaries using Custom GPT

This script processes Markdown files generated by Stage 2.
For each Markdown file, it:
1.  Authenticates with a custom GPT service using OAuth 2.0 client credentials.
2.  Sends the Markdown content to the GPT model.
3.  Instructs the model (via tool calling) to generate two summaries:
    - document_description: A brief 1-2 sentence overview.
    - document_usage: A more detailed, formatted usage description for cataloging.
4.  Retrieves metadata associated with the original source file from Stage 1 outputs.
5.  Combines the generated summaries and metadata into a catalog entry.
6.  Appends the entry to a JSON file (`3A_catalog_entries.json`) on the NAS,
    saving progress after each file to allow for resuming after interruptions.
"""

import os
import sys
import json
import time
import re  # For natural boundary detection in sections
import requests # For OAuth token request
# --- Use pysmb instead of smbclient ---
from smb.SMBConnection import SMBConnection
from smb import smb_structs
import io # For reading/writing strings/bytes to NAS
import socket # For gethostname
# --- End pysmb import ---
import pandas as pd
import tempfile # Added for temporary certificate file
from datetime import datetime, timezone
from openai import OpenAI # Assuming standard openai library v1.x+

# ==============================================================================
# --- Configuration ---
# ==============================================================================

# --- NAS Configuration (Should match Stage 1/2 or be loaded) ---
# Network attached storage connection parameters
NAS_PARAMS = {
    "ip": "your_nas_ip",
    "share": "your_share_name",
    "user": "your_nas_user",
    "password": "your_nas_password",
    "port": 445 # Default SMB port (can be 139)
}
# Base path on the NAS share where Stage 1/2 output files were stored
NAS_OUTPUT_FOLDER_PATH = "path/to/your/output_folder"

# --- pysmb Configuration ---
# Increase timeout for potentially slow NAS operations
smb_structs.SUPPORT_SMB2 = True # Enable SMB2/3 support if available
smb_structs.MAX_PAYLOAD_SIZE = 65536 # Can sometimes help with large directories
CLIENT_HOSTNAME = socket.gethostname() # Get local machine name for SMB connection

# --- Processing Configuration ---
# Document sources configuration - each line contains source name and detail level
DOCUMENT_SOURCES = """
internal_sab_99,detailed
"""

def load_document_sources():
    """Parse document sources configuration - works for all stages"""
    sources = []
    for line in DOCUMENT_SOURCES.strip().split('\n'):
        line = line.strip()
        if line and not line.startswith('#'):
            parts = line.split(',')
            if len(parts) == 2:
                source_name = parts[0].strip()
                detail_level = parts[1].strip()
                sources.append({
                    'name': source_name,
                    'detail_level': detail_level
                })
            else:
                print(f"Warning: Invalid config line ignored: {line}")
    return sources

# Define the type for this batch of documents (used in catalog entry)
DOCUMENT_TYPE = 'POLICY_DOCUMENT' # Example: Replace with appropriate type

# --- OAuth Configuration ---
# OAuth authentication parameters
OAUTH_CONFIG = {
    "token_url": "YOUR_OAUTH_TOKEN_ENDPOINT_URL",
    "client_id": "YOUR_CLIENT_ID",
    "client_secret": "YOUR_CLIENT_SECRET",
    "scope": "api://YourResource/.default"
}

# --- Custom GPT Configuration ---
# GPT API configuration parameters
GPT_CONFIG = {
    "base_url": "YOUR_CUSTOM_GPT_API_BASE_URL",
    "model_name": "your-gpt-model-deployment-name",
    "api_version": "2024-02-01"
}

# --- Embedding Configuration ---
# Embedding model configuration
EMBEDDING_CONFIG = {
    "model_name": "text-embedding-3-large",
    "dimensions": 2000
}

# --- New System Prompt Template ---
# Using CO-STAR framework and XML tags, embedded directly
SYSTEM_PROMPT_TEMPLATE = """<CONTEXT>
<PROJECT_CONTEXT>
This project processes SAB99 documents from '{document_source}' to create structured catalog entries for a database. These entries contain 'usage' and 'description' fields intended for an agentic RAG (Retrieval-Augmented Generation) system. The 'usage' field allows the AI agent to assess document relevance for retrieval, while the 'description' field provides a detailed, structured extract that enables comprehensive analysis when up to 25 documents are loaded via file search mode.
</PROJECT_CONTEXT>

<FILENAME_CONTEXT>
Original filename: {filename}
This filename is provided for reference but may contain spelling errors or be incomplete. Prioritize names and identifiers found within the document content itself.
</FILENAME_CONTEXT>
</CONTEXT>

You are an expert financial analyst specializing in SAB99 documentation, SOX compliance, and financial reporting issues. You are analyzing SAB99 documents to generate structured catalog entries.

<OBJECTIVE>
Your goal is to generate two distinct fields based *only* on the provided <DOCUMENT_CONTENT>:
1.  `description`: A DETAILED, STRUCTURED extract of SAB99 information formatted for display and analysis. This must contain specific fields extracted from the document to enable comprehensive analysis when multiple documents are loaded.
2.  `usage`: A hybrid field combining structured SAB99 facts with LLM selection guidance to optimize document retrieval.
</OBJECTIVE>

<STYLE>
Precise, structured, factual, and comprehensive. Use clear formatting with consistent field labels. Extract all relevant data points systematically.
</STYLE>

<TONE>
Professional financial/regulatory analysis tone.
</TONE>

<AUDIENCE>
- Primary: Financial analysts and SOX compliance teams reviewing SAB99 issues
- Secondary: AI retrieval agents selecting relevant documents for analysis
</AUDIENCE>

<TASK>
Analyze the provided SAB99 document content and generate the `usage` and `description` fields according to the specifications.

<DOCUMENT_CONTENT>
{markdown_content}
</DOCUMENT_CONTENT>

<INSTRUCTIONS>
PHASE 1 - CONTENT ANALYSIS:
1.  Read the entire <DOCUMENT_CONTENT> carefully, identifying this as a SAB99 document.

PHASE 2 - SYSTEMATIC DATA EXTRACTION:
2.  **EXTRACT SAB99 FIELDS:** Systematically identify and extract the following information:
    - **SAB99 ID/Reference:** Unique identifier if specified in document
    - **Segment:** CM (Capital Markets), CNB (City National Bank), FRR (Financial and Regulatory Reporting), Insurance, PB&CB (Personal and Commercial Banking), WM (Wealth Management), or other as specified
    - **Region:** APAC, Barbados, Canada, Canada/US, Europe, or US
    - **Title:** Clear, condensed title based on the issue
    - **Internal/External:** Whether impact is External, Internal, or Both
    - **Issue Status:** Identified, Under Investigation, Remediation in Progress, Resolved, or Closed
    - **Description of Issue:** Core issues/problems (bullet points, max 100 chars each)
    - **Financial Impacts:** Dollar amounts and nature (use format $XXX.Xmm or $X.Xbn)
    - **SOX Impact:** N, Y, or Y with specific SII number
    - **ICFR Assessment:** Material weakness, Significant deficiency, or None
    - **Summary Analysis:** Key analysis points from document
    - **Business Contact:** Name and role if specified
    - **SOX Office Allocation:** SOX personnel assigned
    - **Root Cause:** Choose from: Accounting interpretation error, System/technology failure, Manual processing error, Data integrity issue, Third-party vendor error, Regulatory guidance change, Control design deficiency, Training/knowledge gap, Process gap, or specify if other
    - **Period Impacted:** Specific quarters/years (e.g., Q1 2024, FY 2023)
    - **Impact Type:** P&L Impact, BS Impact, MD&A, Note Disclosure, P&L Reclass, SCF, or Classification Error
    - **Materiality Assessment:** Quantitative impact with percentage of threshold
    - **EUDA Related:** Y/N
    - **Remediation Status:** Not started, In progress, Complete, or N/A
    - **Target Resolution Date:** If specified in document

PHASE 3 - DESCRIPTION FIELD GENERATION:
3.  Generate the `description` field with BOTH a natural language summary AND structured extract:

    Start with a natural language summary (2-3 sentences) for optimal embedding:
    "This SAB99 document details a [segment] [root cause type] issue in [region] that resulted in [$ amount] [impact type] adjustments during [period], with [SOX impact description]. [Additional context about the issue and its resolution]."

    Follow with the structured extract using this EXACT format:
    ```
    **IDENTIFICATION:**
    **SAB99 ID:** [value or "Not specified"]
    **Segment:** [value]
    **Region:** [value]
    **Title:** [value]
    **Status:** [value]

    **ISSUE DETAILS:**
    **Internal/External:** [value]
    **Description of Issue:**
      • [bullet point 1, max 100 chars]
      • [bullet point 2, max 100 chars]
    **Root Cause:** [value]
    **Period Impacted:** [specific quarters/years]

    **FINANCIAL IMPACT:**
    **$ Impacts:**
      • [Primary: amount and type]
      • [Secondary: amount and type if applicable]
    **Impact Type:** [value]
    **Materiality:** P&L: $[amount] ([%] of $120mm threshold), BS: $[amount] ([%] of $1.2bn threshold)

    **COMPLIANCE:**
    **SOX Impact:** [value]
    **ICFR Assessment:** [value]
    **EUDA Related:** [Y/N]

    **RESOLUTION:**
    **Remediation Status:** [value]
    **Target Resolution:** [date or "Not specified"]
    **Summary Analysis:** [analysis text]

    **CONTACTS:**
    **Business Contact:** [value or "Not specified"]
    **SOX Office Allocation:** [value or "Not specified"]
    ```

    FORMATTING RULES:
    - Use exactly "Not specified" for missing information (never "TBD", "N/A", or other variants)
    - Keep bullet points concise
    - Use standard currency format
    - Include percentages for materiality thresholds

PHASE 4 - USAGE FIELD GENERATION:
4.  Generate the `usage` field as a single integrated paragraph optimized for both embedding similarity and LLM selection:

    "SAB99 analysis for [Segment] operations in [Region] addressing [specific issue type]. Root cause identified as [root cause] resulting in $[amount] [impact type] adjustment affecting [specific period]. SOX impact: [Y/N with SII details if Y]. ICFR assessment: [assessment]. Document covers [segment] [specific process] issues, [region] regulatory compliance, [root cause type] remediation, [accounting standards mentioned], [systems/applications affected], materiality assessments [if over thresholds], EUDA controls [if Y], and addresses queries about [specific technical keywords and concepts from the document]."

    Integration rules:
    - Create natural language flow (avoid lists)
    - Include specific technical terms from document
    - Mention accounting standards if referenced (ASC, IFRS, etc.)
    - Include system/application names if mentioned
    - Add business process keywords naturally

PHASE 5 - EDGE CASE HANDLING:
5.  **EDGE CASE RULES:**
    - Multiple segments/regions: List all, separated by commas (e.g., "CM, WM")
    - Conflicting information: Note as "Conflicting: [brief explanation]"
    - Unclear amounts: Use "Amount unclear: [available context]"
    - Multiple root causes: List primary first, then "Multiple factors: [additional causes]"
    - Missing critical fields: Always use "Not specified" consistently
    - Historical references: Include prior period comparisons if mentioned

PHASE 6 - VALIDATION:
6.  **QUALITY CHECKS:**
    - Verify all extracted information exists in source document
    - Ensure no external knowledge was added
    - Confirm natural language summary aligns with structured data
    - Validate formatting matches requirements exactly

7.  Format your response strictly as specified in <RESPONSE_FORMAT>. Do not include any preamble or explanations.
</INSTRUCTIONS>
</TASK>

<RESPONSE_FORMAT>
You MUST call the `generate_catalog_fields` tool. Provide the generated `usage` and `description` strings as arguments within a JSON object.

Example JSON for the tool call arguments:
{{
  "usage": "SAB99 analysis for segment operations in region addressing specific issue...",
  "description": "Natural language summary followed by structured extract with all fields..."
}}
</RESPONSE_FORMAT>
"""

# --- Tool Definition for GPT (Updated) ---
# Renamed tool and updated descriptions/keys
GPT_TOOL_DEFINITION = {
    "type": "function",
    "function": {
        "name": "generate_catalog_fields",
        "description": "Generates structured SAB99 catalog fields for document retrieval and analysis.",
        "parameters": {
            "type": "object",
            "properties": {
                "usage": {
                    "type": "string",
                    "description": "An integrated paragraph optimized for both embedding similarity and LLM selection, combining SAB99 facts with contextual keywords for retrieval."
                },
                "description": {
                    "type": "string",
                    "description": "A natural language summary (2-3 sentences) followed by a detailed structured extract with all SAB99 fields for comprehensive multi-document analysis."
                }
            },
            "required": ["usage", "description"]
        }
    }
}

# --- Input/Output Filenames ---
STAGE1_METADATA_FILENAME = '1C_nas_files_to_process.json'
STAGE2_OUTPUT_SUBFOLDER = '2A_processed_files'
STAGE3_CATALOG_OUTPUT_FILENAME = '3A_catalog_entries.json' # Renamed for clarity
STAGE3_CONTENT_OUTPUT_FILENAME = '3B_content_entries.json' # New output file
STAGE3_ANONYMIZATION_REPORT_FILENAME = '3C_anonymization_report.json' # New report file (standard JSON)
CA_BUNDLE_FILENAME = 'rbc-ca-bundle.cer' # Added CA bundle filename

# ==============================================================================
# --- Helper Functions ---
# ==============================================================================

def create_nas_connection():
    """Creates and returns an authenticated SMBConnection object."""
    try:
        conn = SMBConnection(
            NAS_PARAMS["user"],
            NAS_PARAMS["password"],
            CLIENT_HOSTNAME, # Local machine name
            NAS_PARAMS["ip"], # Remote server name (can be IP)
            use_ntlm_v2=True,
            is_direct_tcp=(NAS_PARAMS["port"] == 445) # Use direct TCP if port 445
        )
        connected = conn.connect(NAS_PARAMS["ip"], NAS_PARAMS["port"], timeout=60) # Increased timeout
        if not connected:
            print("   [ERROR] Failed to connect to NAS.")
            return None
        return conn
    except Exception as e:
        print(f"   [ERROR] Exception creating NAS connection: {e}")
        return None

def ensure_nas_dir_exists(conn, share_name, dir_path_relative):
    """Ensures a directory exists on the NAS, creating it if necessary."""
    if not conn:
        print("   [ERROR] Cannot ensure NAS directory: No connection.")
        return False
    
    # pysmb needs paths relative to the share, using '/' as separator
    path_parts = dir_path_relative.strip('/').split('/')
    current_path = ''
    try:
        for part in path_parts:
            if not part: continue
            current_path = os.path.join(current_path, part).replace('\\', '/')
            try:
                # Check if it exists by trying to list it
                conn.listPath(share_name, current_path)
            except Exception: # If listPath fails, assume it doesn't exist
                print(f"      Creating directory on NAS: {share_name}/{current_path}")
                conn.createDirectory(share_name, current_path)
        return True
    except Exception as e:
        print(f"   [ERROR] Failed to ensure/create NAS directory '{share_name}/{dir_path_relative}': {e}")
        return False

def write_to_nas(share_name, nas_path_relative, content_bytes):
    """Writes bytes to a file path on the NAS using pysmb."""
    conn = None
    print(f"   Attempting to write to NAS path: {share_name}/{nas_path_relative}")
    try:
        conn = create_nas_connection()
        if not conn:
            return False

        # Ensure the directory exists before writing the file
        dir_path = os.path.dirname(nas_path_relative).replace('\\', '/')
        if dir_path and not ensure_nas_dir_exists(conn, share_name, dir_path):
             print(f"   [ERROR] Failed to ensure output directory exists: {dir_path}")
             return False

        # Use BytesIO for pysmb storeFile
        file_obj = io.BytesIO(content_bytes)

        # Store the file
        bytes_written = conn.storeFile(share_name, nas_path_relative, file_obj)
        print(f"   Successfully wrote {bytes_written} bytes to: {share_name}/{nas_path_relative}")
        return True
    except Exception as e:
        print(f"   [ERROR] Unexpected error writing to NAS '{share_name}/{nas_path_relative}': {e}")
        return False
    finally:
        if conn:
            conn.close()

def read_from_nas(share_name, nas_path_relative):
    """Reads content (as bytes) from a file path on the NAS using pysmb."""
    conn = None
    print(f"   Attempting to read from NAS path: {share_name}/{nas_path_relative}")
    try:
        conn = create_nas_connection()
        if not conn:
            return None

        file_obj = io.BytesIO()
        file_attributes, filesize = conn.retrieveFile(share_name, nas_path_relative, file_obj)
        file_obj.seek(0)
        content_bytes = file_obj.read()
        print(f"   Successfully read {filesize} bytes from: {share_name}/{nas_path_relative}")
        return content_bytes
    except Exception as e:
        print(f"   [ERROR] Unexpected error reading from NAS '{share_name}/{nas_path_relative}': {e}")
        return None
    finally:
        if conn:
            conn.close()

def check_nas_path_exists(share_name, nas_path_relative):
    """Checks if a file or directory exists on the NAS using pysmb."""
    conn = None
    try:
        conn = create_nas_connection()
        if not conn:
            return False # Cannot check if connection failed

        # Use getAttributes to check existence - works for files and dirs
        conn.getAttributes(share_name, nas_path_relative)
        return True # Path exists if no exception was raised
    except Exception as e:
        # Check if the error message indicates "No such file" or similar
        err_str = str(e).lower()
        if "no such file" in err_str or "object_name_not_found" in err_str or "0xc0000034" in err_str:
            return False # Expected outcome if the file/path doesn't exist
        else:
            print(f"   [WARNING] Unexpected error checking existence of NAS path '{share_name}/{nas_path_relative}': {type(e).__name__} - {e}")
            return False # Assume not found on other errors
    finally:
        if conn:
            conn.close()

def read_json_from_nas(nas_path_relative):
    """Reads and parses JSON data from a file path on the NAS."""
    print(f"   Attempting to read JSON from NAS path: {NAS_PARAMS['share']}/{nas_path_relative}")
    try:
        if not check_nas_path_exists(NAS_PARAMS["share"], nas_path_relative):
            print(f"   JSON file not found at: {NAS_PARAMS['share']}/{nas_path_relative}. Returning empty list.")
            # Return empty list for results files (catalog or content)
            if STAGE3_CATALOG_OUTPUT_FILENAME in nas_path_relative or STAGE3_CONTENT_OUTPUT_FILENAME in nas_path_relative:
                return [] # Assume results file
            # Handle metadata file case (though it should usually exist)
            elif STAGE1_METADATA_FILENAME in nas_path_relative:
                 print(f"   [WARNING] Metadata file {STAGE1_METADATA_FILENAME} not found. Returning empty list.")
                 return [] # Return list as metadata is expected to be a list of dicts
            else:
                 print(f"   [WARNING] Unrecognized file type for not found handling: {nas_path_relative}. Returning empty list.")
                 return []

        json_bytes = read_from_nas(NAS_PARAMS["share"], nas_path_relative)
        if json_bytes is None:
            return None
        
        data = json.loads(json_bytes.decode('utf-8'))
        print(f"   Successfully read and parsed JSON from: {NAS_PARAMS['share']}/{nas_path_relative}")
        return data
    except json.JSONDecodeError as e:
        print(f"   [ERROR] Failed to parse JSON from '{nas_path_relative}': {e}")
        return None # Indicate failure
    except Exception as e:
        print(f"   [ERROR] Unexpected error reading JSON from NAS '{nas_path_relative}': {e}")
        return None # Indicate failure

def write_json_to_nas(nas_path_relative, data, max_retries=3, retry_delay=2):
    """Writes Python data (list/dict) as JSON to a file path on the NAS with retry logic."""
    print(f"   Attempting to write JSON to NAS path: {NAS_PARAMS['share']}/{nas_path_relative}")
    
    for attempt in range(max_retries):
        try:
            # Convert Python object to JSON string with specific formatting
            json_string = json.dumps(data, indent=4, default=str)
            json_bytes = json_string.encode('utf-8')
            
            # Log size for debugging large files
            file_size_mb = len(json_bytes) / (1024 * 1024)
            if file_size_mb > 5:  # Warn if file is over 5MB
                print(f"   [WARNING] Large JSON file: {file_size_mb:.1f}MB")
            
            success = write_to_nas(NAS_PARAMS["share"], nas_path_relative, json_bytes)
            if success:
                return True
            else:
                print(f"   [Attempt {attempt + 1}/{max_retries}] Failed to write to NAS")
                if attempt < max_retries - 1:
                    print(f"   Waiting {retry_delay} seconds before retry...")
                    time.sleep(retry_delay)
                    
        except TypeError as e:
            print(f"   [ERROR] Failed to serialize data to JSON: {e}")
            return False  # Don't retry serialization errors
        except Exception as e:
            print(f"   [Attempt {attempt + 1}/{max_retries}] Unexpected error preparing JSON for NAS write '{nas_path_relative}': {e}")
            if attempt < max_retries - 1:
                print(f"   Waiting {retry_delay} seconds before retry...")
                time.sleep(retry_delay)
    
    print(f"   [ERROR] Failed to write JSON after {max_retries} attempts")
    return False

def read_text_from_nas(nas_path_relative):
    """Reads text content from a file path on the NAS."""
    print(f"   Attempting to read text from NAS path: {NAS_PARAMS['share']}/{nas_path_relative}")
    try:
        if not check_nas_path_exists(NAS_PARAMS["share"], nas_path_relative):
            print(f"   [ERROR] Text file not found at: {NAS_PARAMS['share']}/{nas_path_relative}")
            return None

        text_bytes = read_from_nas(NAS_PARAMS["share"], nas_path_relative)
        if text_bytes is None:
            return None
        
        content = text_bytes.decode('utf-8')
        return content
    except Exception as e:
        print(f"   [ERROR] Unexpected error reading text from NAS '{nas_path_relative}': {e}")
        return None

def find_structured_json_files(nas_dir_relative):
    """Recursively finds all structured .json files within a given NAS directory."""
    json_files_list = []
    print(f" -> Searching for structured .json files in: {NAS_PARAMS['share']}/{nas_dir_relative}")
    conn = None
    try:
        conn = create_nas_connection()
        if not conn:
            print(f"   [ERROR] Failed to create connection for directory search")
            return []

        if not check_nas_path_exists(NAS_PARAMS["share"], nas_dir_relative):
             print(f"   [ERROR] Base search path does not exist: {NAS_PARAMS['share']}/{nas_dir_relative}")
             return []

        # Use pysmb to walk the directory tree
        def walk_directory(path_relative):
            local_json_files = []
            try:
                entries = conn.listPath(NAS_PARAMS["share"], path_relative)
                for entry in entries:
                    if entry.filename in ['.', '..']:
                        continue
                    
                    entry_path = os.path.join(path_relative, entry.filename).replace('\\', '/')
                    
                    if entry.isDirectory:
                        # Recursively search subdirectories
                        subdirectory_files = walk_directory(entry_path)
                        if subdirectory_files:  # Only extend if not None
                            local_json_files.extend(subdirectory_files)
                    elif entry.filename.lower().endswith('.json'):
                        local_json_files.append(entry_path)
            except Exception as e:
                print(f"   [WARNING] Error walking directory {path_relative}: {e}")
            return local_json_files

        json_files_list = walk_directory(nas_dir_relative)
        print(f" <- Found {len(json_files_list)} .json files.")
        return json_files_list

    except Exception as e:
        print(f"   [ERROR] Unexpected error walking NAS directory '{nas_dir_relative}': {e}")
        return [] # Return empty list on error
    finally:
        if conn:
            conn.close()

def find_md_files(nas_dir_relative):
    """Legacy function - kept for backwards compatibility."""
    # This function is now deprecated in favor of find_structured_json_files
    print(f"   [WARNING] find_md_files is deprecated. Use find_structured_json_files instead.")
    return []

def get_oauth_token():
    """Retrieves an OAuth access token using client credentials flow."""
    print("   Requesting OAuth access token...")
    token_url = OAUTH_CONFIG['token_url']
    payload = {
        'client_id': OAUTH_CONFIG['client_id'],
        'client_secret': OAUTH_CONFIG['client_secret'],
        'grant_type': 'client_credentials'
    }
    # Add scope if defined and not empty
    if OAUTH_CONFIG.get('scope'):
        payload['scope'] = OAUTH_CONFIG['scope']

    try:
        # Note: requests library automatically uses REQUESTS_CA_BUNDLE if set
        response = requests.post(token_url, data=payload)
        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
        token_data = response.json()
        access_token = token_data.get('access_token')
        if not access_token:
            print("   [ERROR] 'access_token' not found in OAuth response.")
            return None
        print("   OAuth token obtained successfully.")
        return access_token
    except requests.exceptions.RequestException as e:
        print(f"   [ERROR] Failed to get OAuth token: {e}")
        # Log more details if possible, e.g., response content for debugging
        if hasattr(e, 'response') and e.response is not None:
             print(f"   Response status: {e.response.status_code}")
             print(f"   Response body: {e.response.text}")
        return None
    except Exception as e:
        print(f"   [ERROR] Unexpected error during OAuth token request: {e}")
        return None

def call_gpt_summarizer(api_client, markdown_content, detail_level, document_source='unknown', filename=''):
    """
    Calls the custom GPT model to generate summaries using tool calling.

    Args:
        api_client: The initialized OpenAI client.
        markdown_content: The text content of the document to summarize.
        detail_level (str): The desired level of detail ('concise', 'standard', 'detailed').
        document_source (str): The source identifier for context in the prompt.
        filename (str): The original filename for reference (may contain errors).

    Returns:
        tuple: (description, usage, analyzer_results_data) strings/dict, or (None, None, None) on failure.
    """
    print(f"   Calling GPT model for summarization (Detail Level: {detail_level})...")
    try:
        # Format the system prompt with dynamic content
        system_prompt = SYSTEM_PROMPT_TEMPLATE.format(
            markdown_content=markdown_content,
            detail_level=detail_level,
            document_source=document_source, # Pass document source for context
            filename=filename # Pass filename for reference
        )

        messages = [
            # System prompt now contains all instructions, context, and content
            {"role": "system", "content": system_prompt}
            # No separate user message needed as content is embedded in system prompt
        ]

        # Note: The openai client might need explicit SSL context/CA bundle handling
        # depending on its version and underlying HTTP library (e.g., httpx).
        # If REQUESTS_CA_BUNDLE/SSL_CERT_FILE doesn't work automatically for openai,
        # you might need to configure the client like this:
        # import httpx
        # cert_path = os.environ.get('REQUESTS_CA_BUNDLE') or os.environ.get('SSL_CERT_FILE')
        # ssl_context = httpx.create_ssl_context(verify=cert_path)
        # client = OpenAI(..., http_client=httpx.Client(verify=ssl_context))
        # For now, assume env vars might be sufficient or openai uses requests internally.

        response = api_client.chat.completions.create(
            model=GPT_CONFIG['model_name'],
            messages=messages,
            tools=[GPT_TOOL_DEFINITION],
            tool_choice={"type": "function", "function": {"name": GPT_TOOL_DEFINITION['function']['name']}}, # Force tool use with updated name
            max_tokens=2048, # Add max_tokens
            temperature=0.2,  # Add temperature
            extra_query={"is_stateful_dlp": True} # Enable entity detection/anonymization
            # Consider adjusting max_tokens based on detail_level if necessary (2048 should be sufficient for now)
        )

        # --- Process analyzer_results if they exist ---
        analyzer_results_data = {
            'found_anonymized': False,
            'entities': []
        }
        if hasattr(response, 'analyzer_results') and isinstance(response.analyzer_results, list):
            print("   Processing analyzer_results...")
            for result in response.analyzer_results:
                if isinstance(result, dict) and result.get('anonymized', False): # Check type and value
                    analyzer_results_data['found_anonymized'] = True
                    analyzer_results_data['entities'].append({
                        'entity_type': result.get('entity_type'),
                        'raw_string': result.get('raw_string')
                    })
            print(f"   Anonymized Entities Found: {analyzer_results_data['found_anonymized']}")
        else:
            print("   No analyzer_results found or not in expected format.")
        # --- End of analyzer_results processing ---

        # --- Process Response ---
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        if tool_calls:
            # Expecting only one tool call in this setup
            tool_call = tool_calls[0]
            # Check against the updated tool name
            if tool_call.function.name == GPT_TOOL_DEFINITION['function']['name']:
                try:
                    arguments = json.loads(tool_call.function.arguments)
                    # Extract using updated keys
                    usage = arguments.get('usage')
                    description = arguments.get('description')

                    if description is not None and usage is not None: # Check for presence, even if empty string is valid
                        print("   GPT successfully returned summaries via tool call.")
                        # Return in the order: description, usage, and anonymization data
                        return description, usage, analyzer_results_data
                    else:
                        # Use updated keys in error message
                        print(f"   [ERROR] GPT tool call arguments missing 'usage' or 'description'. Arguments: {arguments}")
                        return None, None, None
                except json.JSONDecodeError as e:
                    print(f"   [ERROR] Failed to parse GPT tool call arguments JSON: {e}. Arguments: {tool_call.function.arguments}")
                    return None, None, None
                except Exception as e:
                    print(f"   [ERROR] Unexpected error processing GPT tool call arguments: {e}")
                    return None, None, None
            else:
                # Use updated tool name in error message
                print(f"   [ERROR] GPT called unexpected tool: {tool_call.function.name}")
                return None, None, None
        else:
            # Handle case where the model didn't use the tool (e.g., replied directly)
            print(f"   [ERROR] GPT did not use the required tool. Response content: {response_message.content}")
            # Potentially try to extract from content if fallback is desired, but tool use was requested.
            return None, None, None

    except Exception as e:
        # Catch potential API errors (authentication, connection, rate limits, etc.)
        print(f"   [ERROR] Failed to call GPT model: {type(e).__name__} - {e}")
        # Consider more specific error handling based on openai library exceptions if needed
        return None, None, None

# Legacy page splitting functions removed - now using structured JSON data directly
# Section splitting functions removed - now storing full page content only

def split_page_into_sections(page_content, min_sections=2):
    """
    Splits a page's content into sections based on natural boundaries.
    Falls back to force-splitting if needed to ensure minimum sections.
    
    Args:
        page_content: The text content of a single page
        min_sections: Minimum number of sections required (default: 2)
        
    Returns:
        list: List of tuples (section_id, section_content) where section_id starts at 1
    """
    if not page_content or not page_content.strip():
        return [(1, "")]
    
    # First, try to split by natural boundaries
    sections = detect_natural_boundaries(page_content)
    
    # If we have enough sections, return them
    if len(sections) >= min_sections:
        return [(i+1, section) for i, section in enumerate(sections)]
    
    # Otherwise, force split to meet minimum requirement
    if len(sections) == 1:
        # We have one big section, split it
        return force_split_content(sections[0], min_sections)
    else:
        # We have some sections but not enough
        # Keep existing sections and split the largest one
        result = []
        largest_idx = 0
        largest_len = 0
        
        # Find the largest section
        for i, section in enumerate(sections):
            if len(section) > largest_len:
                largest_len = len(section)
                largest_idx = i
        
        # Process all sections
        section_counter = 1
        for i, section in enumerate(sections):
            if i == largest_idx and len(sections) < min_sections:
                # Split the largest section into enough parts
                splits_needed = min_sections - len(sections) + 1
                split_sections = force_split_content(section, splits_needed)
                for _, split_content in split_sections:
                    result.append((section_counter, split_content))
                    section_counter += 1
            else:
                result.append((section_counter, section))
                section_counter += 1
        
        return result


def detect_natural_boundaries(content):
    """
    Detects natural paragraph boundaries in the content.
    
    Returns a list of non-empty sections based on:
    - Double newlines (paragraphs)
    - Markdown headers
    - List items
    - Significant indentation changes
    """
    if not content:
        return []
    
    # Split by double newlines first (most common paragraph delimiter)
    paragraphs = re.split(r'\n\s*\n', content)
    
    sections = []
    current_section = []
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
            
        # Check if this starts a new section (headers, lists, etc.)
        lines = para.split('\n')
        first_line = lines[0].strip() if lines else ""
        
        # Detect markdown headers
        if re.match(r'^#+\s+', first_line):
            # Save previous section if exists
            if current_section:
                sections.append('\n'.join(current_section))
                current_section = []
            # Start new section with header
            current_section.append(para)
        # Detect list starts
        elif re.match(r'^(\d+\.|\-|\*|\•)\s+', first_line):
            # If we already have content, this starts a new section
            if current_section and not re.match(r'^(\d+\.|\-|\*|\•)\s+', current_section[-1].split('\n')[0]):
                sections.append('\n'.join(current_section))
                current_section = []
            current_section.append(para)
        # Tables (markdown tables often start with |)
        elif first_line.startswith('|') and len(lines) > 1 and lines[1].strip().startswith('|'):
            if current_section:
                sections.append('\n'.join(current_section))
                current_section = []
            current_section.append(para)
        else:
            # Regular paragraph - check if it's substantially different
            if current_section and len(para) > 200 and len('\n'.join(current_section)) > 300:
                # Start new section if current is getting large
                sections.append('\n'.join(current_section))
                current_section = [para]
            else:
                current_section.append(para)
    
    # Don't forget the last section
    if current_section:
        sections.append('\n'.join(current_section))
    
    # Filter out empty sections and clean up
    sections = [s.strip() for s in sections if s.strip()]
    
    # If no sections were created, return the whole content as one section
    if not sections:
        return [content.strip()]
    
    return sections


def force_split_content(content, num_sections):
    """
    Force splits content into a specific number of sections.
    Tries to split at sentence boundaries when possible.
    
    Args:
        content: The text to split
        num_sections: Number of sections to create
        
    Returns:
        list: List of tuples (section_id, section_content)
    """
    if not content or num_sections <= 1:
        return [(1, content)]
    
    # Try to split by sentences first
    sentences = re.split(r'(?<=[.!?])\s+', content)
    
    if len(sentences) >= num_sections:
        # We have enough sentences, distribute them evenly
        sentences_per_section = len(sentences) // num_sections
        extra_sentences = len(sentences) % num_sections
        
        sections = []
        current_idx = 0
        
        for i in range(num_sections):
            # Calculate how many sentences for this section
            section_sentence_count = sentences_per_section
            if i < extra_sentences:
                section_sentence_count += 1
            
            # Extract sentences for this section
            section_sentences = sentences[current_idx:current_idx + section_sentence_count]
            section_content = ' '.join(section_sentences).strip()
            
            if section_content:
                sections.append((i + 1, section_content))
            
            current_idx += section_sentence_count
    else:
        # Not enough sentences, split by character count
        content_length = len(content)
        section_length = content_length // num_sections
        
        sections = []
        for i in range(num_sections):
            start_idx = i * section_length
            if i == num_sections - 1:
                # Last section gets any remainder
                end_idx = content_length
            else:
                end_idx = (i + 1) * section_length
                # Try to find a good break point (space, newline)
                search_start = max(start_idx, end_idx - 50)
                for j in range(end_idx, search_start, -1):
                    if content[j] in ' \n':
                        end_idx = j
                        break
            
            section_content = content[start_idx:end_idx].strip()
            if section_content:
                sections.append((i + 1, section_content))
    
    return sections


def generate_embeddings(texts, client, max_batch_size=50):
    """
    Generates embeddings for a list of texts using OpenAI API.
    
    Args:
        texts: List of strings to embed
        client: OpenAI client instance
        max_batch_size: Maximum number of texts to process in one API call
        
    Returns:
        list: List of embedding vectors (lists of floats)
    """
    print(f"   Generating embeddings for {len(texts)} texts...")
    
    all_embeddings = []
    
    # Process in batches
    for i in range(0, len(texts), max_batch_size):
        batch = texts[i:i + max_batch_size]
        print(f"   Processing batch {i//max_batch_size + 1}/{(len(texts) + max_batch_size - 1)//max_batch_size}")
        
        try:
            # Handle empty texts
            processed_batch = [text if text.strip() else " " for text in batch]
            
            response = client.embeddings.create(
                input=processed_batch,
                model=EMBEDDING_CONFIG["model_name"],
                dimensions=EMBEDDING_CONFIG["dimensions"]
            )
            
            # Extract embeddings from response
            batch_embeddings = [data.embedding for data in response.data]
            all_embeddings.extend(batch_embeddings)
            
        except Exception as e:
            print(f"   [ERROR] Failed to generate embeddings for batch: {e}")
            # Return None embeddings for failed batch
            all_embeddings.extend([None] * len(batch))
    
    print(f"   Successfully generated {len([e for e in all_embeddings if e is not None])} embeddings")
    return all_embeddings

# ==============================================================================
# --- Main Processing Function ---
# ==============================================================================

def main_processing_stage3(stage1_metadata_relative_path, stage2_md_dir_relative_path,
                           stage3_catalog_output_relative_path, stage3_content_output_relative_path,
                           stage3_anonymization_report_relative_path, # Added report path
                           ca_bundle_relative_path, refresh_flag_relative_path, detail_level, document_source):
    """Handles the core logic for Stage 3: CA bundle, loading data, processing MD files."""
    print(f"--- Starting Main Processing for Stage 3 ---")
    temp_cert_file_path = None # Store path instead of file object
    original_requests_ca_bundle = os.environ.get('REQUESTS_CA_BUNDLE') # Store original env var value
    original_ssl_cert_file = os.environ.get('SSL_CERT_FILE') # Store original env var value
    is_full_refresh = False # Flag to track refresh mode

    try:
        # --- Download and Set Custom CA Bundle ---
        print("[3] Setting up Custom CA Bundle...")
        try: # Inner try/except for CA bundle download/setup
            if check_nas_path_exists(NAS_PARAMS["share"], ca_bundle_relative_path):
                # Read CA bundle content from NAS
                ca_bundle_bytes = read_from_nas(NAS_PARAMS["share"], ca_bundle_relative_path)
                
                if ca_bundle_bytes:
                    # Create a temporary file to store the certificate
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".cer", mode='wb') as temp_cert_file:
                        temp_cert_file.write(ca_bundle_bytes)
                        temp_cert_file_path = temp_cert_file.name # Store the path for cleanup
                        print(f"   Downloaded CA bundle to temporary file: {temp_cert_file_path}")

                    # Set the environment variables
                    os.environ['REQUESTS_CA_BUNDLE'] = temp_cert_file_path
                    os.environ['SSL_CERT_FILE'] = temp_cert_file_path
                    print(f"   Set REQUESTS_CA_BUNDLE environment variable.")
                    print(f"   Set SSL_CERT_FILE environment variable.")
                else:
                    print(f"   [WARNING] Failed to read CA bundle content. Proceeding without custom CA bundle.")
            else:
                print(f"   [WARNING] CA Bundle file not found at {NAS_PARAMS['share']}/{ca_bundle_relative_path}. Proceeding without custom CA bundle.")
        except Exception as e:
            print(f"   [ERROR] Unexpected error during CA bundle handling '{ca_bundle_relative_path}': {e}. Proceeding without custom CA bundle.")
            # Cleanup potentially created temp file if error occurred after creation
            if temp_cert_file_path and os.path.exists(temp_cert_file_path):
                try:
                    os.remove(temp_cert_file_path)
                    print(f"   Cleaned up partially created temp CA file: {temp_cert_file_path}")
                    temp_cert_file_path = None
                except OSError: pass # Ignore cleanup error

        # --- Check for Full Refresh Flag ---
        print(f"[4] Checking for Full Refresh flag: {os.path.basename(refresh_flag_relative_path)}...")
        try:
            if check_nas_path_exists(NAS_PARAMS["share"], refresh_flag_relative_path):
                print("   *** FULL REFRESH MODE DETECTED ***")
                is_full_refresh = True
                # Delete existing Stage 3 output files
                print("   Deleting existing Stage 3 output files (if they exist)...")
                for file_path in [stage3_catalog_output_relative_path, stage3_content_output_relative_path, stage3_anonymization_report_relative_path]: # Added report path
                    conn = None
                    try:
                        conn = create_nas_connection()
                        if conn and check_nas_path_exists(NAS_PARAMS["share"], file_path):
                            conn.deleteFiles(NAS_PARAMS["share"], file_path)
                            print(f"      Deleted: {os.path.basename(file_path)}")
                        else:
                            print(f"      File not found (already deleted or never existed): {os.path.basename(file_path)}")
                    except Exception as rm_err:
                        print(f"      [WARNING] Error deleting file {os.path.basename(file_path)}: {rm_err}")
                    finally:
                        if conn:
                            conn.close()
            else:
                print("   Full Refresh flag not found. Running in incremental mode.")
        except Exception as e:
            print(f"   [WARNING] Unexpected error checking for refresh flag file '{refresh_flag_relative_path}': {e}. Assuming incremental mode.")
        print("-" * 60)

        # --- Load Stage 1 Metadata ---
        print(f"[5] Loading Stage 1 Metadata from: {os.path.basename(stage1_metadata_relative_path)}...")
        stage1_metadata_list = read_json_from_nas(stage1_metadata_relative_path)
        if stage1_metadata_list is None:
            print("[CRITICAL ERROR] Failed to load Stage 1 metadata. Exiting.")
            sys.exit(1)
        if not isinstance(stage1_metadata_list, list):
            print(f"[CRITICAL ERROR] Stage 1 metadata is not a list. Found type: {type(stage1_metadata_list)}. Exiting.")
            sys.exit(1)

        metadata_lookup = {}
        for item in stage1_metadata_list:
            if 'file_name' in item:
                # Use original file name from metadata as the key base
                original_file_name = item['file_name']
                base_name = os.path.splitext(original_file_name)[0]
                metadata_lookup[base_name] = item
            else:
                print(f"   [WARNING] Skipping metadata item due to missing 'file_name': {item}")
        print(f"   Loaded metadata for {len(metadata_lookup)} files.")
        print("-" * 60)

        # --- Load Existing Stage 3 Results (Checkpointing - only if not full refresh) ---
        print(f"[7] Loading existing Stage 3 results (if incremental mode)...") # Renumbered
        catalog_entries = []
        content_entries = []
        report_entries = [] # Initialize list for report entries
        processed_json_files = set()

        if not is_full_refresh:
            # Load Catalog Entries
            print(f"   Loading catalog entries from: {os.path.basename(stage3_catalog_output_relative_path)}...")
            catalog_entries = read_json_from_nas(stage3_catalog_output_relative_path)
            if catalog_entries is None:
                print("[CRITICAL ERROR] Failed to load or initialize existing catalog entries. Exiting.")
                sys.exit(1)
            if not isinstance(catalog_entries, list):
                print(f"[CRITICAL ERROR] Existing catalog entries file is not a list. Found type: {type(catalog_entries)}. Exiting.")
                sys.exit(1)
            print(f"   Found {len(catalog_entries)} existing catalog entries.")

            # Load Content Entries
            print(f"   Loading content entries from: {os.path.basename(stage3_content_output_relative_path)}...")
            content_entries = read_json_from_nas(stage3_content_output_relative_path)
            if content_entries is None:
                print("[CRITICAL ERROR] Failed to load or initialize existing content entries. Exiting.")
                sys.exit(1)
            if not isinstance(content_entries, list):
                print(f"[CRITICAL ERROR] Existing content entries file is not a list. Found type: {type(content_entries)}. Exiting.")
                sys.exit(1)
            print(f"   Found {len(content_entries)} existing content entries.")

            # Load Anonymization Report Entries
            print(f"   Loading anonymization report entries from: {os.path.basename(stage3_anonymization_report_relative_path)}...")
            report_entries = read_json_from_nas(stage3_anonymization_report_relative_path)
            if report_entries is None:
                print("[CRITICAL ERROR] Failed to load or initialize existing anonymization report entries. Exiting.")
                sys.exit(1)
            if not isinstance(report_entries, list):
                print(f"[CRITICAL ERROR] Existing anonymization report file is not a list. Found type: {type(report_entries)}. Exiting.")
                sys.exit(1)
            print(f"   Found {len(report_entries)} existing anonymization report entries.")

            # Determine processed files based on catalog entries (now using JSON paths)
            processed_json_files = set(entry.get('processed_json_path') for entry in catalog_entries if 'processed_json_path' in entry)
            # Also check for legacy MD paths for backwards compatibility
            legacy_md_files = set(entry.get('processed_md_path') for entry in catalog_entries if 'processed_md_path' in entry)
            print(f"   Identified {len(processed_json_files)} already processed JSON files and {len(legacy_md_files)} legacy MD files (based on catalog).")
        else:
            print("   Full refresh mode: Initializing empty results lists and skipping checkpoint loading.")
        print("-" * 60)

        # --- Find Structured JSON Files from Stage 2 ---
        print(f"[8] Searching for Stage 2 structured JSON files in: {os.path.basename(stage2_md_dir_relative_path)}...") # Renumbered
        json_files_to_process = find_structured_json_files(stage2_md_dir_relative_path)
        if not json_files_to_process:
            print("   No structured JSON files found to process.")
            print("\n" + "="*60)
            print(f"--- Stage 3 Completed (No new structured JSON files found) ---")
            print("="*60 + "\n")
            # No sys.exit(0) here, allow finally block to run
        else:
            print("-" * 60)

            # --- Process Each Structured JSON File ---
            print(f"[9] Processing {len(json_files_to_process)} structured JSON files...") # Renumbered
            new_entries_count = 0
            skipped_count = 0
            error_count = 0
            current_token = None
            token_expiry_time = time.time() # Initialize to ensure first token fetch

            for i, json_relative_path in enumerate(json_files_to_process):
                start_time = time.time()
                print(f"\n--- Processing file {i+1}/{len(json_files_to_process)} ---")
                print(f"   JSON File Path (Relative): {json_relative_path}")

                # Skip if already processed (only in incremental mode)
                if not is_full_refresh and json_relative_path in processed_json_files:
                    print("   File already processed (found in existing results). Skipping.")
                    skipped_count += 1
                    continue

                # Check token expiry
                if time.time() >= token_expiry_time:
                    print("   OAuth token expired or not yet fetched. Requesting new token...")
                    current_token = get_oauth_token()
                    if not current_token:
                        print("   [ERROR] Failed to obtain OAuth token. Skipping file.")
                        error_count += 1
                        continue
                    token_expiry_time = time.time() + (50 * 60) # Assume 50 min validity

                try:
                    client = OpenAI(
                        base_url=GPT_CONFIG['base_url'],
                        api_key=current_token,
                    )
                except Exception as e:
                    print(f"   [ERROR] Failed to initialize OpenAI client: {e}. Skipping file.")
                    error_count += 1
                    continue

                # --- Load Structured JSON Data ---
                structured_data = read_json_from_nas(json_relative_path)
                if not structured_data:
                    print(f"   [ERROR] Failed to read structured JSON from {json_relative_path}. Skipping file.")
                    error_count += 1
                    continue
                
                # Validate JSON structure
                if not isinstance(structured_data, dict) or 'document_name' not in structured_data or 'pages' not in structured_data:
                    print(f"   [ERROR] Invalid structured JSON format in {json_relative_path}. Skipping file.")
                    error_count += 1
                    continue
                
                document_name = structured_data['document_name']
                pages_data = structured_data['pages']
                total_pages = structured_data.get('total_pages', len(pages_data))
                
                print(f"   Document: {document_name} ({total_pages} pages)")
                
                # Combine all page content for GPT summarization
                combined_markdown = ""
                for page_data in pages_data:
                    if page_data.get('markdown_content'):
                        combined_markdown += page_data['markdown_content'] + "\n\n"
                
                if not combined_markdown.strip():
                    print(f"   [ERROR] No valid markdown content found in {json_relative_path}. Skipping file.")
                    error_count += 1
                    continue

                # --- Call GPT Summarizer (with detail level, source, and filename) ---
                # Use configured detail level
                print(f"   DEBUG: Using configured detail_level: {detail_level}")
                # Extract filename from JSON path for reference
                original_filename = os.path.basename(json_relative_path).replace('.json', '')
                description, usage, anonymization_data = call_gpt_summarizer(client, combined_markdown, detail_level, document_source, original_filename)

                # Check if None was returned (indicates an error in call_gpt_summarizer)
                if description is None or usage is None or anonymization_data is None:
                    print(f"   [ERROR] Failed to get summaries or anonymization data from GPT for {json_relative_path}. Skipping file.")
                    error_count += 1
                    continue # Skip if summarization fails

                # --- Prepare Data for Output ---
                json_filename = os.path.basename(json_relative_path)
                original_base_name = os.path.splitext(json_filename)[0]
                
                original_metadata = metadata_lookup.get(original_base_name)
                if not original_metadata:
                    print(f"   [WARNING] Could not find original metadata for base name '{original_base_name}' derived from {json_filename}. Skipping file.")
                    error_count += 1
                    continue

                # Extract key fields (ensure they exist in metadata)
                doc_name = original_metadata.get('file_name', document_name) # Fallback to document_name if needed
                doc_base_name = os.path.splitext(doc_name)[0] # Get filename without extension
                doc_source = document_source # Use configured source
                doc_type = DOCUMENT_TYPE   # Use configured type

                # --- Process Pages Directly from Structured Data ---
                # No need for complex JSON loading or span parsing - we have clean page data
                page_contents = []
                for page_data in pages_data:
                    page_num = page_data.get('page_number', 1)
                    page_content = page_data.get('markdown_content', '')
                    if page_content and page_content.strip():
                        page_contents.append((page_num, page_content))
                
                print(f"   Successfully extracted {len(page_contents)} pages with content.")
                
                # --- Generate embeddings for document_usage and document_description ---
                print("   Generating embeddings for catalog entry...")
                catalog_embeddings = generate_embeddings([usage, description], client)
                
                if len(catalog_embeddings) >= 2:
                    usage_embedding = catalog_embeddings[0]
                    description_embedding = catalog_embeddings[1]
                else:
                    print("   [WARNING] Failed to generate catalog embeddings. Using None.")
                    usage_embedding = None
                    description_embedding = None
                
                # Create Catalog Entry (with embeddings)
                catalog_entry = {
                    "document_source": doc_source,
                    "document_type": doc_type,
                    "document_name": doc_name,
                    "document_description": description, # Use the returned description
                    "document_usage": usage, # Use the returned usage
                    "document_usage_embedding": usage_embedding,  # Add embedding
                    "document_description_embedding": description_embedding,  # Add embedding
                    # Use date_created from Stage 1 metadata (original creation or fallback to modified)
                    "date_created": original_metadata.get('date_created'),
                    "date_last_modified": original_metadata.get('date_last_modified'),
                    "file_name": original_metadata.get('file_name'), # Redundant but matches original structure
                    "file_type": os.path.splitext(original_metadata.get('file_name', ''))[1],
                    "file_size": original_metadata.get('file_size'),
                    "file_path": original_metadata.get('file_path'),
                    "file_link": f"//{NAS_PARAMS['ip']}/{NAS_PARAMS['share']}/{original_metadata.get('file_path', '')}",
                    "processed_json_path": json_relative_path # For checkpointing (updated to JSON)
                }

                # Create Content Entries (one per page, no section splitting)
                new_content_entries = []
                print(f"   Creating content entries for {len(page_contents)} pages...")
                for page_num, page_content in page_contents:
                    content_entry = {
                        "document_source": doc_source,
                        "document_type": doc_type,
                        "document_name": doc_name,
                        "page_number": page_num,  # Page number from Azure DI
                        "section_id": 1,  # Always 1 since no sections
                        "section_name": f"{doc_base_name}_page_{page_num}",  # Simplified naming
                        "section_summary": f"Page {page_num} of {doc_name}",  # Simplified summary
                        "section_content": page_content,  # Full page content
                        "date_created": datetime.now(timezone.utc).isoformat()  # Add creation date
                    }
                    new_content_entries.append(content_entry)
                    print(f"      Page {page_num}: Created content entry")
                
                # --- Append and Save Both Entries (Atomic-like operation for checkpointing) ---
                print(f"   Appending new catalog entry for: {doc_name}")
                catalog_entries.append(catalog_entry)
                print(f"   Appending {len(new_content_entries)} content entries (sections) for: {doc_name}")
                content_entries.extend(new_content_entries)

                # Save Catalog Entries
                catalog_save_success = write_json_to_nas(stage3_catalog_output_relative_path, catalog_entries)
                # Save Content Entries
                content_save_success = write_json_to_nas(stage3_content_output_relative_path, content_entries)

                if catalog_save_success and content_save_success:
                    print(f"   Successfully saved updated catalog ({len(catalog_entries)}) and content ({len(content_entries)}) entries to NAS.")

                    # Prepare Anonymization Report Entry (Copy catalog entry and add DLP fields)
                    report_entry = catalog_entry.copy() # Create a copy
                    report_entry['dlp_anonymized_or_flagged'] = anonymization_data.get('found_anonymized', False)
                    report_entry['dlp_findings'] = []
                    if report_entry['dlp_anonymized_or_flagged']:
                        for entity in anonymization_data.get('entities', []):
                            entity_type = entity.get('entity_type', 'UNKNOWN_TYPE')
                            raw_string = entity.get('raw_string', 'UNKNOWN_VALUE')
                            report_entry['dlp_findings'].append(f"ANONYMIZED: {entity_type} - {raw_string}")
                    # TODO: Add processing for other potential flags (detection_results, content_filter_results) here if needed in the future

                    # Append and Save Anonymization Report Entry (Standard JSON list)
                    print(f"   Appending new report entry for: {doc_name}")
                    report_entries.append(report_entry)
                    report_save_success = write_json_to_nas(stage3_anonymization_report_relative_path, report_entries)

                    if report_save_success:
                        print(f"   Successfully saved updated anonymization report ({len(report_entries)}) entries to NAS.")
                    else:
                        # Log warning but don't necessarily stop the whole process? Or make it critical?
                        # For now, log warning and rollback the append for consistency.
                        print(f"   [WARNING] Failed to save anonymization report file to NAS after processing {json_filename}. Report may be out of sync.")
                        report_entries.pop() # Rollback append

                    # Only increment count and mark processed if all saves were successful
                    # (or if report save failure is considered non-critical)
                    # Assuming report save failure is non-critical for now:
                    new_entries_count += 1
                    processed_json_files.add(json_relative_path) # Mark as processed only if all saves succeed (or primary ones + report append attempted)
                else:
                    print(f"   [CRITICAL ERROR] Failed to save one or both primary output files (catalog/content) to NAS after processing {json_filename}. Stopping.")
                    error_count += 1
                    # Rollback the appends for consistency before exiting
                    catalog_entries.pop()
                    content_entries.pop()
                    print(f"   Rolled back entries for {doc_name}.")
                    # Attempt to save the rolled-back state (optional, might fail again)
                    # write_json_to_nas(stage3_catalog_output_smb_path, catalog_entries)
                    # write_json_to_nas(stage3_content_output_smb_path, content_entries)
                    sys.exit(1) # Exit on critical save failure

                end_time = time.time()
                print(f"--- Finished file {i+1} (Success) ---")
                print(f"--- Time taken: {end_time - start_time:.2f} seconds ---")

            # --- Final Summary ---
            print("\n" + "="*60)
            print(f"--- Stage 3 Processing Summary ---")
            print(f"   Total JSON files found: {len(json_files_to_process)}")
            print(f"   Files skipped (already processed): {skipped_count}")
            print(f"   New entry pairs (catalog/content) added: {new_entries_count}")
            print(f"   Errors encountered: {error_count}")
            print(f"   Total entries in '{STAGE3_CATALOG_OUTPUT_FILENAME}': {len(catalog_entries)}")
            print(f"   Total entries in '{STAGE3_CONTENT_OUTPUT_FILENAME}': {len(content_entries)}")
            print(f"   Total entries in '{STAGE3_ANONYMIZATION_REPORT_FILENAME}': {len(report_entries)}") # Updated report info
            print("="*60 + "\n")

            if error_count > 0:
                print(f"[WARNING] {error_count} files encountered errors during processing. Check logs above.")

        print(f"--- Stage 3 Completed ---")
        print(f"--- End of Main Processing for Stage 3 ---")

    # --- Cleanup (Executes regardless of success/failure in the try block) ---
    finally:
        print("\n--- Cleaning up ---")
        # Clean up the temporary certificate file
        if temp_cert_file_path and os.path.exists(temp_cert_file_path):
            try:
                os.remove(temp_cert_file_path)
                print(f"   Removed temporary CA bundle file: {temp_cert_file_path}")
            except OSError as e:
                 print(f"   [WARNING] Failed to remove temporary CA bundle file {temp_cert_file_path}: {e}")

        # Restore original environment variables
        # Restore REQUESTS_CA_BUNDLE
        current_requests_bundle = os.environ.get('REQUESTS_CA_BUNDLE')
        if original_requests_ca_bundle is None:
            # If it didn't exist originally, remove it if we set it
            if current_requests_bundle == temp_cert_file_path:
                 print("   Unsetting REQUESTS_CA_BUNDLE environment variable.")
                 # Check if key exists before deleting
                 if 'REQUESTS_CA_BUNDLE' in os.environ:
                     del os.environ['REQUESTS_CA_BUNDLE']
        else:
            # If it existed originally, restore its value if it changed
            if current_requests_bundle != original_requests_ca_bundle:
                 print(f"   Restoring original REQUESTS_CA_BUNDLE environment variable.")
                 os.environ['REQUESTS_CA_BUNDLE'] = original_requests_ca_bundle

        # Restore SSL_CERT_FILE
        current_ssl_cert = os.environ.get('SSL_CERT_FILE')
        if original_ssl_cert_file is None:
            # If it didn't exist originally, remove it if we set it
            if current_ssl_cert == temp_cert_file_path:
                 print("   Unsetting SSL_CERT_FILE environment variable.")
                 # Check if key exists before deleting
                 if 'SSL_CERT_FILE' in os.environ:
                     del os.environ['SSL_CERT_FILE']
        else:
            # If it existed originally, restore its value if it changed
            if current_ssl_cert != original_ssl_cert_file:
                 print(f"   Restoring original SSL_CERT_FILE environment variable.")
                 os.environ['SSL_CERT_FILE'] = original_ssl_cert_file

# ==============================================================================
# --- Script Entry Point ---
# ==============================================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print(f"--- Running Stage 3: Generate Document Summaries ---")
    print("="*60 + "\n")
    
    # Get document sources
    sources = load_document_sources()
    print(f"[0] Processing {len(sources)} document sources:")
    for source in sources:
        print(f"   - {source['name']} (detail level: {source['detail_level']})")
    print("-" * 60)
    
    # Track overall processing results
    all_sources_processed = []
    sources_with_summaries = []
    
    # Process each document source
    for source_config in sources:
        DOCUMENT_SOURCE = source_config['name']
        DETAIL_LEVEL = source_config['detail_level']
        
        print(f"\n{'='*60}")
        print(f"Processing Document Source: {DOCUMENT_SOURCE}")
        print(f"Detail Level: {DETAIL_LEVEL}")
        print(f"{'='*60}\n")

        # --- Define Paths (Relative to Share) ---
        print("[1] Defining NAS Paths (Relative)...")
        share_name = NAS_PARAMS["share"]
        source_base_dir_relative = os.path.join(NAS_OUTPUT_FOLDER_PATH, DOCUMENT_SOURCE).replace('\\', '/')
        stage1_metadata_relative_path = os.path.join(source_base_dir_relative, STAGE1_METADATA_FILENAME).replace('\\', '/')
        stage2_md_dir_relative_path = os.path.join(source_base_dir_relative, STAGE2_OUTPUT_SUBFOLDER).replace('\\', '/')
        stage3_catalog_output_relative_path = os.path.join(source_base_dir_relative, STAGE3_CATALOG_OUTPUT_FILENAME).replace('\\', '/')
        stage3_content_output_relative_path = os.path.join(source_base_dir_relative, STAGE3_CONTENT_OUTPUT_FILENAME).replace('\\', '/')
        stage3_anonymization_report_relative_path = os.path.join(source_base_dir_relative, STAGE3_ANONYMIZATION_REPORT_FILENAME).replace('\\', '/')
        ca_bundle_relative_path = os.path.join(NAS_OUTPUT_FOLDER_PATH, CA_BUNDLE_FILENAME).replace('\\', '/')

        print(f"   Source Base Dir (Relative): {share_name}/{source_base_dir_relative}")
        print(f"   Stage 1 Metadata File (Relative): {share_name}/{stage1_metadata_relative_path}")
        print(f"   Stage 2 MD Files Dir (Relative): {share_name}/{stage2_md_dir_relative_path}")
        print(f"   Stage 3 Catalog Output File (Relative): {share_name}/{stage3_catalog_output_relative_path}")
        print(f"   Stage 3 Content Output File (Relative): {share_name}/{stage3_content_output_relative_path}")
        print(f"   Stage 3 Anonymization Report (Relative): {share_name}/{stage3_anonymization_report_relative_path}")
        print(f"   CA Bundle File (Relative): {share_name}/{ca_bundle_relative_path}")
        # Add Refresh Flag Path
        refresh_flag_file_name = '_FULL_REFRESH.flag'
        refresh_flag_relative_path = os.path.join(source_base_dir_relative, refresh_flag_file_name).replace('\\', '/')
        print(f"   Refresh Flag File (Relative): {share_name}/{refresh_flag_relative_path}")
        print("-" * 60)

        # --- Check for Skip Flag from Stage 1 ---
        print("[2] Checking for skip flag from Stage 1...")
        skip_flag_file_name = '_SKIP_SUBSEQUENT_STAGES.flag'
        skip_flag_relative_path = os.path.join(source_base_dir_relative, skip_flag_file_name).replace('\\', '/')
        print(f"   Checking for flag file: {share_name}/{skip_flag_relative_path}")
        should_skip = False
        try:
            if check_nas_path_exists(share_name, skip_flag_relative_path):
                print(f"   Skip flag file found. Stage 1 indicated no files to process.")
                should_skip = True
            else:
                print(f"   Skip flag file not found. Proceeding with Stage 3.")
        except Exception as e:
            print(f"   [WARNING] Unexpected error checking for skip flag file '{skip_flag_relative_path}': {e}")
            print(f"   Proceeding with Stage 3.")
            # Continue execution
        print("-" * 60)

        # --- Execute Main Processing if Not Skipped ---
        if should_skip:
            print(f"   Stage 3 Skipped for source '{DOCUMENT_SOURCE}' (No files to process from Stage 1)")
        else:
            # Call the main processing function only if not skipping
            try:
                main_processing_stage3(stage1_metadata_relative_path, stage2_md_dir_relative_path,
                                       stage3_catalog_output_relative_path, stage3_content_output_relative_path,
                                       stage3_anonymization_report_relative_path, # Pass new report path
                                       ca_bundle_relative_path, refresh_flag_relative_path, DETAIL_LEVEL, DOCUMENT_SOURCE)
                sources_with_summaries.append(DOCUMENT_SOURCE)
            except Exception as e:
                print(f"   [ERROR] Summary generation failed for source '{DOCUMENT_SOURCE}': {e}")
                continue
        
        # Track this source as processed
        all_sources_processed.append(DOCUMENT_SOURCE)
        print(f"   Source '{DOCUMENT_SOURCE}' processing completed.")
        print("-" * 60)

    # Final summary
    print("\n" + "="*60)
    print(f"--- Stage 3 Completed Successfully ---")
    print(f"--- Processed {len(all_sources_processed)} sources ---")
    print(f"--- Sources with summaries generated: {len(sources_with_summaries)} ---")
    if sources_with_summaries:
        print(f"--- Sources with summaries: {', '.join(sources_with_summaries)} ---")
    print("="*60 + "\n")
