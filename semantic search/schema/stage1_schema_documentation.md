# Stage 1: Chapter Processing - Input/Output Schema Documentation

## Overview
Stage 1 takes page-level records with chapter assignments and enriches them with chapter-level summaries generated by GPT-4.

## Stage 1 Input Schema

The input is a JSON array where each record represents a single page from the document. Records should be pre-processed with chapter assignments.

### Input Fields (per record):
```json
{
  "document_id": "EY_GUIDE_2024",           // Document identifier
  "filename": "01_Chapter_Name.pdf",        // Chapter-specific PDF filename
  "filepath": "/path/to/chapters/01_Chapter_Name.pdf",  // Full path to chapter PDF
  "page_number": 1,                         // Sequential page number within chapter PDF (1, 2, 3...)
  "page_reference": "2-15",                 // Original page reference from source document
  "content": "Markdown content...",         // Page content in markdown format
  "source_filename": "ey_original.pdf",     // Original PDF filename before splitting
  "source_page_number": 45,                 // Original page number in source PDF
  "chapter_number": 1,                      // Chapter number (can be 0 for front matter, null for unassigned)
  "chapter_name": "Introduction to IFRS"    // Chapter name/title
}
```

### Input Requirements:
- All pages with the same `chapter_number` should have consistent `chapter_name` values
- Pages are expected to be sorted by `source_page_number` to maintain document order
- `chapter_number` can be `null` for unassigned pages (these won't get summaries)

## Stage 1 Output Schema

The output maintains all input fields and adds chapter-level enrichments.

### Output Fields (per record):
```json
{
  "document_id": "EY_GUIDE_2024",           // [Preserved] Document identifier
  "filename": "01_Chapter_Name.pdf",        // [Preserved] Chapter-specific PDF filename
  "filepath": "/path/to/chapters/01_Chapter_Name.pdf",  // [Preserved] Full path to chapter PDF
  "source_filename": "ey_original.pdf",     // [Preserved] Original PDF filename
  "chapter_number": 1,                      // [Preserved] Chapter number
  "chapter_name": "Introduction to IFRS",   // [Preserved] Chapter name
  "chapter_summary": "This chapter provides foundational...",  // [NEW] GPT-generated chapter summary (2-3 sentences)
  "chapter_page_count": 25,                 // [NEW] Total pages in this chapter
  "page_number": 1,                         // [Preserved] Page number within chapter PDF
  "page_reference": "2-15",                 // [Preserved] Original page reference
  "source_page_number": 45,                 // [Preserved] Original document page number
  "content": "Markdown content..."          // [Preserved] Page content
}
```

### New Fields Added by Stage 1:

1. **`chapter_summary`** (string or null)
   - Condensed summary of the entire chapter (2-3 sentences)
   - Generated by GPT-4 for all pages in a chapter
   - Captures in a concise format:
     - The primary purpose and scope of the chapter
     - Main accounting standards and technical topics covered (e.g., IFRS 16, ASC 842)
     - Key applicability, outcomes, or decision points
   - All pages in the same chapter receive the same summary
   - `null` for unassigned pages

2. **`chapter_page_count`** (integer or null)
   - Total number of pages in the chapter
   - All pages in the same chapter receive the same count
   - `null` for unassigned pages

### Processing Logic:

1. **Grouping**: Pages are grouped by `chapter_number`
2. **Concatenation**: All page content within a chapter is concatenated
3. **Segmentation**: If chapter content exceeds token limits (~80k tokens), it's processed in segments
4. **Summary Generation**: GPT-4 generates a comprehensive summary for the entire chapter
5. **Distribution**: The summary and page count are applied to all pages in that chapter
6. **Ordering**: Output maintains original order sorted by `source_page_number`

## Key Differences from Input:

- **Added**: `chapter_summary` - Critical for downstream processing in Stage 2+
- **Added**: `chapter_page_count` - Metadata about chapter size
- **Removed**: No fields are removed; all input fields are preserved
- **Modified**: No existing fields are modified

## Usage Notes for Stage 2:

- The `chapter_summary` field is the primary value-add from Stage 1
- Stage 2 can use the summary to understand chapter context when processing sections
- Pages with `chapter_summary: null` are unassigned and may need special handling
- The `chapter_page_count` can be used to understand the relative size of chapters

## Edge Case Handling

Stage 1 includes robust validation and error handling for various edge cases:

### Empty/Whitespace Content Validation
- **Check**: Validates that chapter text exists and contains non-whitespace content
- **Location**: Lines 673-683 in processing logic  
- **Behavior**: If chapter text is empty or contains only whitespace characters, returns `None` and logs a warning
- **Impact**: Prevents GPT API calls on empty content, avoiding unnecessary costs and API errors

### Zero Token Validation  
- **Check**: Verifies that chapter text contains tokenizable content
- **Location**: Lines 680-683 in processing logic
- **Behavior**: Even if character count > 0, if token count = 0, returns `None` and logs warning with character/token counts
- **Purpose**: Catches edge cases where content exists but cannot be processed by tokenizer (special characters, encoding issues, etc.)

### Division by Zero Protection
- **Check**: Ensures positive token availability for segmentation calculations
- **Location**: Lines 692-697 in processing logic  
- **Behavior**: If `available_for_content <= 0`, logs error and returns `None`
- **Context**: Prevents division by zero when calculating number of segments needed for large chapters
- **Fallback**: Uses `max(1000, GPT_INPUT_TOKEN_LIMIT - TOKEN_BUFFER)` to ensure minimum token availability

These validations ensure robust processing and prevent downstream failures while providing clear logging for debugging.

## Example Processing Flow:

```
Input: 100 pages across 5 chapters + 10 unassigned pages
       ↓
Stage 1 Processing:
  - Groups pages by chapter_number
  - Validates chapter content (empty/whitespace/zero-token checks)
  - Generates 5 chapter summaries via GPT-4 (with segmentation if needed)
  - Applies summaries to all pages in respective chapters
       ↓  
Output: 100 pages with chapter summaries + 10 pages with null summaries
```