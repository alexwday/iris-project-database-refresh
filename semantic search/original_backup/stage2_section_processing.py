# -*- coding: utf-8 -*-
"""
Stage 2: Section Identification & Enrichment

Purpose:
Processes chapter data generated by Stage 1. It identifies logical sections
within each chapter based on Markdown headings (H1-H6). For each section, it
calculates token count, determines start/end page numbers, and uses an LLM
to generate a concise summary, tags, applicable standard(s), standard codes,
an importance score, and references.

Input: JSON file from Stage 1 (e.g., 'pipeline_output/stage1/stage1_chapter_data.json').
Output: A JSON file in OUTPUT_DIR containing a list of section dictionaries.
        (e.g., 'pipeline_output/stage2/stage2_section_data.json')
"""

import os
import json
import traceback
import re
import time
import logging
import requests
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional, Union
from collections import defaultdict

# --- Dependencies Check ---
try:
    import tiktoken
except ImportError:
    tiktoken = None
    print("WARNING: tiktoken not installed. Token counts will be estimated (chars/4). `pip install tiktoken`")

try:
    import natsort
except ImportError:
    natsort = None
    print("INFO: natsort not installed. Chapters/Sections might not sort naturally. `pip install natsort`")

try:
    from openai import OpenAI, APIError
except ImportError:
    OpenAI = None
    APIError = None
    print("ERROR: openai library not installed. GPT features unavailable. `pip install openai`")

try:
    from tqdm import tqdm
except ImportError:
    tqdm = lambda x, **kwargs: x # Make tqdm optional
    print("INFO: tqdm not installed. Progress bars disabled. `pip install tqdm`")

# ==============================================================================
# Configuration
# ==============================================================================

# --- Directory Paths ---
# TODO: Adjust these paths as needed
STAGE1_OUTPUT_DIR = "pipeline_output/stage1"
STAGE1_FILENAME = "stage1_chapter_data.json"
OUTPUT_DIR = "pipeline_output/stage2"
OUTPUT_FILENAME = "stage2_section_data.json"
LOG_DIR = "pipeline_output/logs"

# --- API Configuration ---
# TODO: Load securely or replace placeholders (Ensure consistency with Stage 1)
BASE_URL = os.environ.get("OPENAI_API_BASE", "https://api.example.com/v1")
MODEL_NAME_CHAT = os.environ.get("OPENAI_MODEL_CHAT", "gpt-4-turbo-nonp")
OAUTH_URL = os.environ.get("OAUTH_URL", "https://api.example.com/oauth/token")
CLIENT_ID = os.environ.get("OAUTH_CLIENT_ID", "your_client_id")
CLIENT_SECRET = os.environ.get("OAUTH_CLIENT_SECRET", "your_client_secret")
SSL_SOURCE_PATH = os.environ.get("SSL_SOURCE_PATH", "/path/to/your/rbc-ca-bundle.cer")
SSL_LOCAL_PATH = "/tmp/rbc-ca-bundle.cer"

# --- API Parameters ---
MAX_COMPLETION_TOKENS_SECTION = 2000 # Max tokens for section details response
TEMPERATURE = 0.3
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 5 # seconds
MAX_RECENT_SUMMARIES_CONTEXT = 5 # Number of previous section summaries to include in context

# --- Token Cost (Optional) ---
PROMPT_TOKEN_COST = 0.01
COMPLETION_TOKEN_COST = 0.03

# --- Section Merging Thresholds (from script 2) ---
# OPTIMAL_TOKENS = 500 # Target token count (currently informational).
MAX_TOKENS = 750  # Maximum tokens allowed in a merged section.
MIN_TOKENS = 250  # Sections below this count trigger merging logic (Pass 1).
ULTRA_SMALL_THRESHOLD = 25  # Sections below this trigger more aggressive merging (Pass 2).


# --- Logging Setup ---
Path(LOG_DIR).mkdir(parents=True, exist_ok=True)
# Use a different log file for this stage
log_file = Path(LOG_DIR) / 'stage2_section_processing.log'
# Remove existing handlers if configuring multiple times in a notebook
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)

# Silence overly verbose HTTPX and OpenAI logging
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

# ==============================================================================
# Utility Functions (Self-Contained, adapted from Stage 1 and original scripts)
# ==============================================================================

# --- Tokenizer ---
_TOKENIZER = None
if tiktoken:
    try:
        _TOKENIZER = tiktoken.get_encoding("cl100k_base")
        logging.info("Using 'cl100k_base' tokenizer via tiktoken.")
    except Exception as e:
        logging.warning(f"Failed to initialize tiktoken tokenizer: {e}. Falling back to estimate.")
        _TOKENIZER = None

def count_tokens(text: str) -> int:
    """Counts tokens using tiktoken if available, otherwise estimates (chars/4)."""
    if not text: return 0
    if _TOKENIZER:
        try: return len(_TOKENIZER.encode(text))
        except Exception: return len(text) // 4
    else: return len(text) // 4

# --- API Client ---
_SSL_CONFIGURED = False
_OPENAI_CLIENT = None

def _setup_ssl(source_path=SSL_SOURCE_PATH, local_path=SSL_LOCAL_PATH) -> bool:
    """Copies SSL cert locally and sets environment variables."""
    global _SSL_CONFIGURED
    if _SSL_CONFIGURED: return True
    if not Path(source_path).is_file():
         logging.warning(f"SSL source certificate not found at {source_path}. API calls may fail.")
         _SSL_CONFIGURED = True
         return True
    logging.info("Setting up SSL certificate...")
    try:
        source = Path(source_path); local = Path(local_path)
        local.parent.mkdir(parents=True, exist_ok=True)
        with open(source, "rb") as sf, open(local, "wb") as df: df.write(sf.read())
        os.environ["SSL_CERT_FILE"] = str(local)
        os.environ["REQUESTS_CA_BUNDLE"] = str(local)
        logging.info(f"SSL certificate configured successfully at: {local}")
        _SSL_CONFIGURED = True
        return True
    except Exception as e:
        logging.error(f"Error setting up SSL certificate: {e}", exc_info=True)
        return False

def _get_oauth_token(oauth_url=OAUTH_URL, client_id=CLIENT_ID, client_secret=CLIENT_SECRET, ssl_verify_path=SSL_LOCAL_PATH) -> Optional[str]:
    """Retrieves OAuth token."""
    verify_path = ssl_verify_path if Path(ssl_verify_path).exists() else True
    logging.info("Attempting to get OAuth token...")
    payload = {'grant_type': 'client_credentials', 'client_id': client_id, 'client_secret': client_secret}
    try:
        response = requests.post(oauth_url, data=payload, timeout=30, verify=verify_path)
        response.raise_for_status()
        token_data = response.json(); oauth_token = token_data.get('access_token')
        if not oauth_token: logging.error("Error: 'access_token' not found."); return None
        logging.info("OAuth token obtained successfully.")
        return oauth_token
    except requests.exceptions.RequestException as e:
        logging.error(f"Error getting OAuth token: {e}", exc_info=True); return None

def get_openai_client(base_url=BASE_URL) -> Optional[OpenAI]:
    """Initializes and returns the OpenAI client."""
    global _OPENAI_CLIENT
    if _OPENAI_CLIENT: return _OPENAI_CLIENT
    if not OpenAI: logging.error("OpenAI library not available."); return None
    if not _setup_ssl(): logging.warning("Proceeding without explicit SSL setup.")
    api_key = _get_oauth_token()
    if not api_key: logging.error("Aborting client creation due to OAuth token failure."); return None
    try:
        _OPENAI_CLIENT = OpenAI(api_key=api_key, base_url=base_url)
        logging.info("OpenAI client created successfully.")
        return _OPENAI_CLIENT
    except Exception as e:
        logging.error(f"Error creating OpenAI client: {e}", exc_info=True); return None

# --- API Call (Single Attempt) ---
# Renamed from _call_gpt_with_retry to reflect single attempt nature
def _call_gpt_single_attempt(client, model, messages, max_tokens, temperature, tools=None, tool_choice=None):
    """Makes a single API call attempt."""
    logging.debug("Making single API call attempt...")
    completion_kwargs = {"model": model, "messages": messages, "max_tokens": max_tokens, "temperature": temperature, "stream": False}
    if tools and tool_choice:
        completion_kwargs["tools"] = tools; completion_kwargs["tool_choice"] = tool_choice
        logging.debug("Making API call with tool choice...")
    else:
        # This function now expects tool use based on how get_section_details_from_gpt uses it
        if not tools or not tool_choice:
             logging.warning("API call initiated without explicit tool choice - this function expects tool use.")
        completion_kwargs["tools"] = tools # Still send tools if provided
        # completion_kwargs["response_format"] = {"type": "json_object"} # Use if not using tools

    # No try/except here, let exceptions propagate up to the caller (get_section_details_from_gpt)
    response = client.chat.completions.create(**completion_kwargs)
    logging.debug("API call successful.")
    response_message = response.choices[0].message; usage_info = response.usage

    if response_message.tool_calls:
        tool_call = response_message.tool_calls[0]
        # Basic validation can still happen here or be deferred to parser
        if tool_choice and isinstance(tool_choice, dict):
            expected_tool_name = tool_choice.get("function", {}).get("name")
            if expected_tool_name and tool_call.function.name != expected_tool_name:
                raise ValueError(f"Expected tool '{expected_tool_name}' but received '{tool_call.function.name}'")
        return tool_call.function.arguments, usage_info # Return JSON string from tool arguments
    elif response_message.content:
        # If content is returned instead of tool_calls, return it for the parser to handle
        logging.warning("API response contained content instead of expected tool_calls.")
        return response_message.content, usage_info
    else:
        raise ValueError("API response missing both tool calls and content.")

def parse_gpt_json_response(response_content_str: str, expected_keys: List[str]) -> Optional[Dict]:
    """Parses JSON response string from GPT and validates expected keys. Raises exceptions on failure."""
    # Removed the unnecessary try block
    if response_content_str.strip().startswith("```json"): response_content_str = response_content_str.strip()[7:-3].strip()
    elif response_content_str.strip().startswith("```"): response_content_str = response_content_str.strip()[3:-3].strip()
    data = json.loads(response_content_str)
    # Corrected indentation for the following lines
    if not isinstance(data, dict): raise ValueError("Response is not a JSON object.")
    missing_keys = [key for key in expected_keys if key not in data]
    if missing_keys: raise ValueError(f"Missing expected keys: {', '.join(missing_keys)}")
    logging.debug("GPT JSON response parsed successfully.")
    return data
    # Let exceptions propagate up to the caller (get_section_details_from_gpt)
    # except json.JSONDecodeError as e:
    #     logging.error(f"Error decoding GPT JSON: {e}\nRaw response: {response_content_str[:500]}..."); return None
    # except ValueError as e:
    #     logging.error(f"Error validating GPT JSON: {e}\nRaw response: {response_content_str[:500]}..."); return None

# --- File/Path Utils ---
def create_directory(directory: str):
    """Creates the specified directory if it does not already exist."""
    Path(directory).mkdir(parents=True, exist_ok=True)

# --- Page Tag Extraction (Identical to Stage 1) ---
PAGE_NUMBER_TAG_PATTERN = re.compile(r'<!--\s*PageNumber="(\d+)"\s*-->')
AZURE_TAG_PATTERN = re.compile(r'<!--\s*Page(Footer|Number|Break|Header)=?(".*?"|\d+)?\s*-->\s*\n?')

def clean_azure_tags(text: str) -> str:
    """Removes Azure Document Intelligence specific HTML comment tags."""
    return AZURE_TAG_PATTERN.sub("", text)

def extract_page_mapping(content: str) -> list[tuple[int, int]]:
    """Extracts (character_position, page_number) tuples from tags."""
    mapping = []; raw_matches = []
    for match in PAGE_NUMBER_TAG_PATTERN.finditer(content):
        raw_matches.append((match.start(), int(match.group(1))))
    if not raw_matches: return []
    raw_matches.sort(key=lambda x: (x[0], -x[1]))
    if raw_matches:
        mapping.append(raw_matches[0])
        for i in range(1, len(raw_matches)):
            if raw_matches[i][0] > mapping[-1][0]: mapping.append(raw_matches[i])
    if mapping and mapping[-1][0] < len(content):
        mapping.append((len(content), mapping[-1][1]))
    return mapping

def get_section_page_range(raw_section_slice: str, chapter_start_page: int) -> Tuple[int, int]:
    """Determines start/end page for a section slice."""
    section_mapping = extract_page_mapping(raw_section_slice)
    if not section_mapping:
        # If no tags within the section, assume it's on the chapter's start page
        # This might be inaccurate if sections span pages without internal tags.
        logging.debug("No page tags found within section slice. Using chapter start page.")
        return chapter_start_page, chapter_start_page
    else:
        # Use the first and last page numbers found *within the slice*
        start_page = section_mapping[0][1]
        end_page = section_mapping[-1][1]
        end_page = max(start_page, end_page) # Ensure end >= start
        logging.debug(f"Section page range derived from tags: {start_page}-{end_page}")
        return start_page, end_page

# --- Section Identification ---
def find_headings(raw_content: str) -> list[dict]:
    """Finds Markdown headings (levels 1-6) in raw text."""
    heading_pattern = re.compile(r"^(#{1,6})\s+(.+)$", re.MULTILINE)
    headings = []
    for match in heading_pattern.finditer(raw_content):
        headings.append({
            "level": len(match.group(1)),
            "text": match.group(2).strip(),
            "position": match.start()
        })
    # Add virtual end marker
    headings.append({"level": 0, "text": "DOCUMENT_END", "position": len(raw_content)})
    headings.sort(key=lambda h: h["position"])
    return headings

def split_chapter_into_sections(chapter_data: dict) -> list[dict]:
    """Splits raw chapter content into initial sections based on headings."""
    raw_content = chapter_data["raw_content"]
    headings = find_headings(raw_content)
    initial_sections = []
    section_index_in_chapter = 0
    current_heading_context = {f"level_{i}": None for i in range(1, 7)}
    # Initialize L1 with chapter name for context
    current_heading_context["level_1"] = chapter_data.get("chapter_name")
    # Get chapter-level fields once for potential use
    document_id = chapter_data.get("document_id")
    chapter_number = chapter_data.get("chapter_number")
    chapter_name = chapter_data.get("chapter_name")
    chapter_token_count = chapter_data.get("chapter_token_count")
    chapter_page_start = chapter_data.get("chapter_page_start")
    chapter_page_end = chapter_data.get("chapter_page_end")

    # Handle content before the first heading
    first_heading_pos = headings[0]['position'] if headings and headings[0]['level'] > 0 else len(raw_content)
    if first_heading_pos > 0:
        intro_slice = raw_content[:first_heading_pos].strip()
        if intro_slice:
            section_index_in_chapter += 1
            # Ensure intro section includes chapter pass-through fields
            intro_section_data = {
                # Pass-through fields
                "document_id": document_id,
                "chapter_number": chapter_number,
                "chapter_name": chapter_name,
                "chapter_token_count": chapter_token_count,
                "chapter_page_start": chapter_page_start,
                "chapter_page_end": chapter_page_end,
                # Section specific fields
                "raw_section_slice": intro_slice,
                "level": 1, # Assign level 1 conceptually
                "section_title": chapter_name or "Introduction", # Use chapter name as title
                "start_pos": 0,
                "end_pos": first_heading_pos,
                "section_number": section_index_in_chapter,
                "level_1": chapter_name, # Add level_1 context
            }
            initial_sections.append(intro_section_data)

    # Process sections defined by headings
    for i in range(len(headings) - 1):
        current_heading = headings[i]
        next_heading = headings[i + 1]

        # Skip if level is 0 (e.g., the intro section we might have handled)
        if current_heading["level"] == 0: continue

        section_start_pos = current_heading["position"]
        section_end_pos = next_heading["position"]
        raw_section_slice = raw_content[section_start_pos:section_end_pos].strip()

        if raw_section_slice: # Only create section if content exists
            section_index_in_chapter += 1
            current_level = current_heading["level"]
            current_title = current_heading["text"]

            # Update heading context
            current_heading_context[f"level_{current_level}"] = current_title
            for lower_level in range(current_level + 1, 7):
                current_heading_context[f"level_{lower_level}"] = None

            # Create section data dictionary, including pass-through fields
            section_data = {
                # Pass-through fields
                "document_id": document_id,
                "chapter_number": chapter_number,
                "chapter_name": chapter_name,
                "chapter_token_count": chapter_token_count,
                "chapter_page_start": chapter_page_start,
                "chapter_page_end": chapter_page_end,
                # Section specific fields
                "raw_section_slice": raw_section_slice,
                "level": current_level,
                "section_title": current_title,
                "start_pos": section_start_pos, # Position relative to raw_content start
                "end_pos": section_end_pos,     # Position relative to raw_content start
                "section_number": section_index_in_chapter,
            }
            # Add current hierarchy context
            for level_num in range(1, 7):
                level_key = f"level_{level_num}"
                if current_heading_context.get(level_key):
                    section_data[level_key] = current_heading_context[level_key]

            initial_sections.append(section_data)

    return initial_sections

def generate_hierarchy_string(section_data: dict) -> str:
    """Generates a breadcrumb-style hierarchy string."""
    parts = []
    max_level_to_check = section_data.get("level", 6)
    for i in range(1, max_level_to_check + 1):
        level_key = f"level_{i}"
        heading_text = section_data.get(level_key)
        if heading_text: parts.append(heading_text)
        else: break # Stop if a level is missing
    return " > ".join(parts)

# --- Section Merging Logic (Copied from 1_chunk_creation/2_identify_sections_and_merge.py) ---

def merge_small_sections(
    sections: list[dict], min_tokens: int, max_tokens: int, ultra_small_threshold: int
) -> list[dict]:
    """
    Merges sections smaller than `min_tokens` or `ultra_small_threshold`
    with adjacent sections, respecting hierarchy and `max_tokens` limit.

    Operates in two passes:
    1. Merges sections < `min_tokens` based on level and proximity.
    2. Merges sections < `ultra_small_threshold` more aggressively.

    Args:
        sections: List of cleaned section dictionaries (must include 'content',
                  'section_token_count', 'start_pos', 'end_pos', 'level', 'chapter_number',
                  and other pass-through fields).
        min_tokens: Threshold for the first merging pass.
        max_tokens: Maximum allowed tokens for a merged section.
        ultra_small_threshold: Threshold for the second, more aggressive pass.

    Returns:
        A list of sections after merging potentially small ones.
    """
    if not sections:
        return []

    # Ensure sections are sorted by their original position (section_number) before merging
    sections_to_process = sorted(sections, key=lambda s: s["section_number"])

    # --- Pass 1: Merge sections smaller than `min_tokens` ---
    pass1_merged = []
    i = 0
    while i < len(sections_to_process):
        current = sections_to_process[i]
        # Use section_token_count for merging decisions
        current_tokens = current.get("section_token_count", 0)

        # Keep sections that are already large enough
        if current_tokens >= min_tokens:
            pass1_merged.append(current)
            i += 1
            continue

        # Attempt to merge small sections
        merged_pass1 = False

        # Strategy 1: Merge forward with next section if compatible levels and combined size <= max_tokens
        if i + 1 < len(sections_to_process):
            next_s = sections_to_process[i + 1]
            next_tokens = next_s.get("section_token_count", 0)
            # Ensure chapter_number exists before comparing
            if (
                current.get("chapter_number") == next_s.get("chapter_number")
                and current.get("level") == next_s.get("level") # Require same level for forward merge
                and current_tokens + next_tokens <= max_tokens
            ):
                # Create merged section data, keeping metadata from the *first* section (current)
                merged_data = current.copy() # Start with current's metadata
                merged_data["content"] = f"{current.get('content', '')}\n\n{next_s.get('content', '')}"
                # Recalculate token count for the merged content
                merged_data["section_token_count"] = count_tokens(merged_data["content"])
                # Keep word count for reference, though less critical now (removed word_count field later)
                # merged_data["word_count"] = current.get("word_count", 0) + next_s.get("word_count", 0)
                merged_data["end_pos"] = next_s["end_pos"]
                # All other fields (hierarchy, pass-through) are inherited from 'current'

                pass1_merged.append(merged_data)
                i += 2  # Skip the next section as it's now merged
                merged_pass1 = True

        # Strategy 2: Merge backward with the previously added section if compatible
        if not merged_pass1 and pass1_merged:
            prev_s = pass1_merged[-1]
            prev_tokens = prev_s.get("section_token_count", 0)
            # Check chapter, token limits, and compatible levels (current is same or deeper level)
            # Ensure chapter_number exists before comparing
            if (
                current.get("chapter_number") == prev_s.get("chapter_number")
                and prev_tokens + current_tokens <= max_tokens
                and current.get("level", 1) >= prev_s.get("level", 1) # Allow merging deeper level back
            ):
                # Merge current's content into the previous section
                prev_s["content"] = f"{prev_s.get('content', '')}\n\n{current.get('content', '')}"
                # Recalculate token count for the merged content
                prev_s["section_token_count"] = count_tokens(prev_s["content"])
                # prev_s["word_count"] = prev_s.get("word_count", 0) + current.get("word_count", 0)
                prev_s["end_pos"] = current["end_pos"]
                # Metadata (hierarchy, pass-through) remains from prev_s

                i += 1 # Move to the next section to process
                merged_pass1 = True

        # If no merge occurred, keep the current section as is
        if not merged_pass1:
            pass1_merged.append(current)
            i += 1

    # --- Pass 2: Merge "ultra-small" sections (< ultra_small_threshold) ---
    if not pass1_merged:  # Skip if Pass 1 resulted in nothing
        return []

    final_merged = []
    i = 0
    while i < len(pass1_merged):
        current = pass1_merged[i]
        # Use section_token_count for decisions here too
        current_tokens = current.get("section_token_count", 0)

        # Keep sections that meet the ultra-small threshold
        if current_tokens >= ultra_small_threshold:
            final_merged.append(current)
            i += 1
            continue

        # Determine if the section content looks like just a heading
        is_heading_only = (
            re.match(r"^\s*#+\s+[^#]", current.get("content", "").strip()) is not None
        )
        merged_pass2 = False

        # --- Preferred Merge Direction ---
        if is_heading_only:
            # Headings prefer merging FORWARD (merge *current* heading into the *next* section's content)
            if i + 1 < len(pass1_merged):
                next_s = pass1_merged[i + 1]
                next_tokens = next_s.get("section_token_count", 0)
                # Ensure chapter_number exists before comparing
                if (
                    current.get("chapter_number") == next_s.get("chapter_number")
                    and current_tokens + next_tokens <= max_tokens
                ):
                    # Create merged section, taking metadata from NEXT section but content from both
                    merged_data = next_s.copy() # Start with next section's metadata
                    merged_data["content"] = f"{current.get('content', '')}\n\n{next_s.get('content', '')}"
                    merged_data["section_token_count"] = count_tokens(merged_data["content"])
                    # merged_data["word_count"] = current.get("word_count", 0) + next_s.get("word_count", 0)
                    merged_data["start_pos"] = current["start_pos"] # Start pos from current
                    # Hierarchy etc. comes from next_s

                    final_merged.append(merged_data)
                    i += 2 # Skip current and next
                    merged_pass2 = True
        else:
            # Content sections prefer merging BACKWARD (merge *current* content into the *previous* section)
            if final_merged:
                prev_s = final_merged[-1]
                prev_tokens = prev_s.get("section_token_count", 0)
                # Ensure chapter_number exists before comparing
                if (
                    current.get("chapter_number") == prev_s.get("chapter_number")
                    and prev_tokens + current_tokens <= max_tokens
                ):
                    # Merge current's content into previous
                    prev_s["content"] = f"{prev_s.get('content', '')}\n\n{current.get('content', '')}"
                    prev_s["section_token_count"] = count_tokens(prev_s["content"])
                    # prev_s["word_count"] = prev_s.get("word_count", 0) + current.get("word_count", 0)
                    prev_s["end_pos"] = current["end_pos"]
                    # Metadata remains from prev_s
                    i += 1 # Move to next item in pass1_merged
                    merged_pass2 = True

        # --- Fallback Merge Direction (if preferred failed) ---
        if not merged_pass2:
            if is_heading_only:
                # Fallback for heading: Merge BACKWARD (merge *current* heading into *previous*)
                if final_merged:
                    prev_s = final_merged[-1]
                    prev_tokens = prev_s.get("section_token_count", 0)
                    # Ensure chapter_number exists before comparing
                    if (
                        current.get("chapter_number") == prev_s.get("chapter_number")
                        and prev_tokens + current_tokens <= max_tokens
                    ):
                        # Merge current's content into previous
                        prev_s["content"] = f"{prev_s.get('content', '')}\n\n{current.get('content', '')}"
                        prev_s["section_token_count"] = count_tokens(prev_s["content"])
                        # prev_s["word_count"] = prev_s.get("word_count", 0) + current.get("word_count", 0)
                        prev_s["end_pos"] = current["end_pos"]
                        # Metadata remains from prev_s
                        i += 1 # Move to next item in pass1_merged
                        merged_pass2 = True
            else:
                # Fallback for content: Merge FORWARD (merge *current* content into *next*)
                if i + 1 < len(pass1_merged):
                    next_s = pass1_merged[i + 1]
                    next_tokens = next_s.get("section_token_count", 0)
                    # Ensure chapter_number exists before comparing
                    if (
                        current.get("chapter_number") == next_s.get("chapter_number")
                        and current_tokens + next_tokens <= max_tokens
                    ):
                        # Create merged section, taking metadata from NEXT section
                        merged_data = next_s.copy() # Start with next section's metadata
                        merged_data["content"] = f"{current.get('content', '')}\n\n{next_s.get('content', '')}"
                        merged_data["section_token_count"] = count_tokens(merged_data["content"])
                        # merged_data["word_count"] = current.get("word_count", 0) + next_s.get("word_count", 0)
                        merged_data["start_pos"] = current["start_pos"] # Start pos from current
                        # Hierarchy etc. comes from next_s

                        final_merged.append(merged_data)
                        i += 2 # Skip current and next
                        merged_pass2 = True

        # If no merge happened in Pass 2 either, keep the ultra-small section
        if not merged_pass2:
            final_merged.append(current)
            i += 1

    # Ensure token count field is consistently named 'section_token_count'
    for section in final_merged:
        # Remove word_count if it exists, as it's not used later
        section.pop("word_count", None)
        
        # Ensure we're using section_token_count as the field name (not chunk_token_count)
        # This maintains consistency with our database schema
        if "chunk_token_count" in section and "section_token_count" not in section:
            section["section_token_count"] = section.pop("chunk_token_count")

    return final_merged


# --- GPT Prompting for Section Details ---
SECTION_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "extract_section_details",
        "description": "Extracts detailed information about a specific document section based on its content and the overall chapter context.",
        "parameters": {
            "type": "object",
            "properties": {
                "section_summary": {
                    "type": "string",
                    "description": "A concise summary (1-3 sentences) capturing the core topic or purpose of this section, suitable for reranking search results."
                },
                "section_tags": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "A list of meaningful keywords or tags specific to this section's content, scaled appropriately to the section's length and complexity. These tags will be used as metadata for search reranking."
                },
                "section_standard": {
                    "type": "string",
                    "description": "The primary accounting or reporting standard applicable to this section (e.g., 'IFRS', 'US GAAP', 'N/A')."
                },
                "section_standard_codes": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "A list of specific standard codes explicitly mentioned or directly relevant in the section (e.g., ['IFRS 16', 'IAS 17']). The number of codes should reflect the section's content. These codes will be used as metadata for search reranking."
                },
                "section_importance_score": { # Matches field name in script 8's schema
                    "type": "number",
                    "description": "A score between 0.0 (low importance) and 1.0 (high importance) indicating how crucial this section is to understanding the overall chapter's topic. A score of 0.5 indicates average or unknown importance. This float value will be used for search reranking.",
                    "minimum": 0.0,
                    "maximum": 1.0
                },
                "section_references": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "A list of explicit references to other sections, chapters, or standard codes found within this section's text (e.g., ['See Section 4.5', 'Refer to Chapter 3', 'IAS 36.12']). Provide an empty list [] if none are found. These references provide context."
                }
            },
            # Keep required field name as 'section_importance_score' to match script 8's schema
            "required": ["section_summary", "section_tags", "section_standard", "section_standard_codes", "section_importance_score", "section_references"]
        }
    }
}

def _build_section_prompt(section_text, chapter_summary, chapter_tags, previous_section_summaries=None):
    """Builds the messages list for the section processing call."""
    if previous_section_summaries is None: previous_section_summaries = []

    # System prompt from script 8
    system_prompt = """<role>You are an expert financial reporting specialist.</role>
<source_material>You are analyzing a specific section within a chapter from an EY technical accounting guidance manual. You are provided with the overall chapter summary/tags and summaries of recently processed sections from the same chapter.</source_material>
<task>Your primary task is to generate a **concise summary (1-3 sentences)** for the current section, suitable for use in reranking search results. Additionally, extract relevant tags, the primary applicable accounting standard, and specific standard codes mentioned. Use the 'extract_section_details' tool for your response.</task>
<guardrails>Base your analysis strictly on the provided section text and context. Focus on capturing the core topic/purpose concisely for the summary. Ensure tags and standard codes are precise and derived from the section text.</guardrails>"""

    user_prompt_elements = ["<prompt>"]
    # User prompt elements from script 8
    user_prompt_elements.append("<style>Concise, factual, keyword-focused for summary; technical and precise for other fields.</style>")
    user_prompt_elements.append("<tone>Professional, objective, expert.</tone>")
    user_prompt_elements.append("<audience>Accounting professionals needing specific guidance on this section.</audience>")
    user_prompt_elements.append('<response_format>Use the "extract_section_details" tool.</response_format>')

    user_prompt_elements.append("<overall_chapter_context>")
    user_prompt_elements.append(f"<chapter_summary>{chapter_summary}</chapter_summary>")
    user_prompt_elements.append(f"<chapter_tags>{json.dumps(chapter_tags)}</chapter_tags>")
    user_prompt_elements.append("</overall_chapter_context>")

    if previous_section_summaries:
        user_prompt_elements.append("<recent_section_context>")
        for i, summary in enumerate(previous_section_summaries):
            user_prompt_elements.append(f"<previous_section_{i+1}_summary>{summary}</previous_section_{i+1}_summary>")
        user_prompt_elements.append("</recent_section_context>")

    user_prompt_elements.append(f"<current_section_text>{section_text}</current_section_text>")

    # Instructions block from script 8 (includes 'section_importance' in point 5)
    user_prompt_elements.append("<instructions>")
    user_prompt_elements.append("""
    **Analysis Objective:** Analyze the provided <current_section_text> considering the <overall_chapter_context> and <recent_section_context> (if provided).
    **Action:** Generate the following details for the **current section** using the 'extract_section_details' tool:
    1.  **section_summary:** A **very concise summary (1-3 sentences)** capturing the core topic or purpose of this section. This summary will be used to help rerank search results, so it should be distinct and informative at a glance.
    2.  **section_tags:** Generate a list of meaningful, granular tags specific to THIS SECTION's content. The number of tags should be dynamic and reflect the section's complexity and key topics. These tags are crucial metadata for search reranking.
    3.  **section_standard:** Identify the single, primary accounting standard framework most relevant to THIS SECTION (e.g., 'IFRS', 'US GAAP', 'N/A').
    4.  **section_standard_codes:** List specific standard codes (e.g., 'IFRS 16', 'IAS 36.12', 'ASC 842-10-15') explicitly mentioned or directly and significantly relevant within THIS SECTION's text. The number of codes should be dynamic, reflecting the section's content. Provide an empty list [] if none are applicable. These codes are crucial metadata for search reranking.
    5.  **section_importance_score:** Assign a score between 0.0 (low importance) and 1.0 (high importance) representing how crucial this section's content is for understanding the overall topic of the chapter provided in the <overall_chapter_context>. Consider the section's scope and depth relative to the chapter summary. A score of 0.5 indicates average or unknown importance. Provide a float value (e.g., 0.7). This score will directly influence search result ranking.
    6.  **section_references:** List any explicit textual references made within the <current_section_text> to other sections, chapters, paragraphs, or specific standard codes (e.g., "See Section 4.5", "Refer to Chapter 3", "IAS 36.12"). Provide an empty list [] if no explicit references are found.
    """)
    user_prompt_elements.append("</instructions>")
    user_prompt_elements.append("</prompt>")
    user_prompt = "\n".join(user_prompt_elements)
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    return messages

def get_section_details_from_gpt(section_text: str, chapter_details: Dict, previous_summaries: List[str], client: OpenAI) -> Optional[Dict]:
    """Calls GPT to get structured details for a section."""
    messages = _build_section_prompt(
        section_text=section_text,
        chapter_summary=chapter_details.get('chapter_summary', 'N/A'),
        chapter_tags=chapter_details.get('chapter_tags', []),
        previous_section_summaries=previous_summaries
    )
    prompt_tokens_est = sum(count_tokens(msg["content"]) for msg in messages)
    logging.debug(f"Estimated prompt tokens for section enrichment: {prompt_tokens_est}")

    last_exception = None
    for attempt in range(API_RETRY_ATTEMPTS):
        try:
            logging.debug(f"Attempt {attempt + 1}/{API_RETRY_ATTEMPTS} to get and parse details for section...")
            # 1. Call API (single attempt)
            response_content_json_str, usage_info = _call_gpt_single_attempt(
                client, MODEL_NAME_CHAT, messages, MAX_COMPLETION_TOKENS_SECTION, TEMPERATURE,
                tools=[SECTION_TOOL_SCHEMA],
                tool_choice={"type": "function", "function": {"name": "extract_section_details"}}
            )
            if not response_content_json_str:
                raise ValueError("API call returned empty response content.")

            # 2. Parse Response (raises exception on failure)
            parsed_data = parse_gpt_json_response(
                response_content_json_str,
                expected_keys=["section_summary", "section_tags", "section_standard", "section_standard_codes", "section_importance_score", "section_references"]
            )

            # 3. Log usage and return data if successful
            if usage_info:
                 prompt_tokens = usage_info.prompt_tokens; completion_tokens = usage_info.completion_tokens
                 total_tokens = usage_info.total_tokens
                 total_cost = (prompt_tokens / 1000 * PROMPT_TOKEN_COST) + (completion_tokens / 1000 * COMPLETION_TOKEN_COST)
                 # Changed to DEBUG to reduce console noise
                 logging.debug(f"API Usage (Section Details) - Prompt: {prompt_tokens}, Completion: {completion_tokens}, Total: {total_tokens}, Cost: ${total_cost:.4f}")
            else:
                 logging.debug("Usage information not available.")

            return parsed_data # Success! Exit loop and return.

        except (APIError, requests.exceptions.RequestException) as e:
            logging.warning(f"API communication error on attempt {attempt + 1}: {e}")
            last_exception = e
            time.sleep(API_RETRY_DELAY * (attempt + 1)) # Exponential backoff for API errors
        except (json.JSONDecodeError, ValueError) as e:
             logging.warning(f"Parsing/Validation error on attempt {attempt + 1}: {e}")
             # Log raw response if possible (might be large)
             if 'response_content_json_str' in locals() and response_content_json_str:
                 logging.warning(f"Raw response snippet: {response_content_json_str[:500]}...")
             last_exception = e
             time.sleep(API_RETRY_DELAY) # Simple delay for parsing errors
        except Exception as e:
            logging.error(f"Unexpected error on attempt {attempt + 1}: {e}", exc_info=True)
            last_exception = e
            time.sleep(API_RETRY_DELAY) # Simple delay for unexpected errors

    # If loop finishes without returning, all attempts failed
    logging.error(f"Failed to get valid section details after {API_RETRY_ATTEMPTS} attempts.")
    if last_exception:
        logging.error(f"Last error encountered: {last_exception}")
    return None # Indicate failure

import sys # Ensure sys is imported

# ==============================================================================
# Main Stage 2 Logic
# ==============================================================================

# Helper function to create a unique ID for a section
def _create_section_id(section_data: Dict) -> Optional[str]:
    """Creates a unique identifier string for a section."""
    doc_id = section_data.get("document_id")
    chap_num = section_data.get("chapter_number")
    
    # Always use the renumbered section_number for consistent IDs
    sec_num = section_data.get("section_number")
    
    if doc_id is not None and chap_num is not None and sec_num is not None:
        return f"{doc_id}::{chap_num}::{sec_num}"
    logging.warning(f"Could not create section ID from data: {section_data.get('section_title', 'Unknown Section')}")
    return None # Return None if essential parts are missing

def process_chapter_for_sections(
    chapter_data: Dict,
    client: Optional[OpenAI],
    existing_section_details: Dict[str, Dict] # Changed: Pass dict of existing details
    ) -> List[Dict]:
    """
    Identifies, enriches, and assembles section data for a single chapter.
    Retries sections if they exist in existing_section_details but lack enrichment.
    Returns a list of all section data dictionaries for this chapter (new, retried, or existing valid).
    """
    chapter_number = chapter_data.get("chapter_number", "UNKNOWN")
    document_id = chapter_data.get("document_id", "UNKNOWN_DOC") # Get doc_id for ID creation
    logging.info(f"Processing sections for Chapter {chapter_number}...")
    processed_chapter_sections = [] # Store all sections for this chapter
    recent_section_summaries = [] # Context for GPT within this chapter run

    # 1. Initial split based on headings
    initial_sections = split_chapter_into_sections(chapter_data)
    logging.info(f"  Identified {len(initial_sections)} initial sections.")

    # 2. Clean content and calculate initial metrics
    cleaned_sections = []
    for section_raw in initial_sections:
        # Clean Azure tags from the raw slice
        cleaned_content = clean_azure_tags(section_raw["raw_section_slice"])
        # Only keep sections with non-whitespace content after cleaning
        if cleaned_content.strip():
            section_clean = section_raw.copy() # Keep all fields from initial split
            section_clean["content"] = cleaned_content # Store cleaned content
            # Calculate and store the *initial* token count needed for merging
            section_clean["section_token_count"] = count_tokens(cleaned_content)
            # Remove raw slice now that we have cleaned content
            section_clean.pop("raw_section_slice", None)
            cleaned_sections.append(section_clean)
    logging.info(f"  Sections after cleaning & filtering empty: {len(cleaned_sections)}")

    # 3. Merge small sections
    merged_sections = merge_small_sections(
        cleaned_sections, MIN_TOKENS, MAX_TOKENS, ULTRA_SMALL_THRESHOLD
    )
    logging.info(f"  Sections after merging small ones: {len(merged_sections)}")
    
    # 3a. Renumber section_number sequentially while preserving order
    for i, section in enumerate(merged_sections):
        section["original_section_number"] = section["section_number"]  # Preserve original for reference
        section["section_number"] = i + 1  # Renumber sequentially starting from 1
    logging.info(f"  Renumbered {len(merged_sections)} merged sections sequentially.")
    
    # 3b. Extract and assign page numbers from Azure tags
    chapter_start_page = chapter_data.get("chapter_page_start")
    chapter_end_page = chapter_data.get("chapter_page_end")
    
    # Ensure first and last sections have the chapter's start/end pages
    if merged_sections and chapter_start_page is not None:
        merged_sections[0]["section_start_page"] = chapter_start_page
    if merged_sections and chapter_end_page is not None:
        merged_sections[-1]["section_end_page"] = chapter_end_page
        
    # Find page tags in each section's content
    page_numbers_by_section = []
    for section in merged_sections:
        content = section.get("content", "")
        page_tags = list(PAGE_NUMBER_TAG_PATTERN.finditer(content))
        
        if page_tags:
            # Extract page numbers from content
            page_numbers = [int(match.group(1)) for match in page_tags]
            page_numbers_by_section.append((section["section_number"], min(page_numbers), max(page_numbers)))
            
            # Set section's page range based on its own tags
            section["section_start_page"] = min(page_numbers)
            section["section_end_page"] = max(page_numbers)
            logging.debug(f"  Section {section['section_number']} has page tags: range {min(page_numbers)}-{max(page_numbers)}")
        else:
            page_numbers_by_section.append((section["section_number"], None, None))
            logging.debug(f"  Section {section['section_number']} has no page tags")
    
    # Fill in missing page numbers by inferring from neighboring sections
    for i, section in enumerate(merged_sections):
        if "section_start_page" not in section or section["section_start_page"] is None:
            # Find previous section with a defined end page
            prev_end_page = None
            for j in range(i-1, -1, -1):
                if merged_sections[j].get("section_end_page") is not None:
                    prev_end_page = merged_sections[j]["section_end_page"]
                    break
            
            # If found, use previous section's end page for current start page
            if prev_end_page is not None:
                section["section_start_page"] = prev_end_page
                logging.debug(f"  Inferred section {section['section_number']} start page = {prev_end_page} (from previous section)")
            else:
                # If not found, use chapter start page
                section["section_start_page"] = chapter_start_page
                logging.debug(f"  Section {section['section_number']} start page defaulting to chapter start: {chapter_start_page}")
        
        if "section_end_page" not in section or section["section_end_page"] is None:
            # Find next section with a defined start page
            next_start_page = None
            for j in range(i+1, len(merged_sections)):
                if merged_sections[j].get("section_start_page") is not None:
                    next_start_page = merged_sections[j]["section_start_page"]
                    break
            
            # If found, use next section's start page for current end page
            if next_start_page is not None:
                section["section_end_page"] = next_start_page
                logging.debug(f"  Inferred section {section['section_number']} end page = {next_start_page} (from next section)")
            else:
                # If not found, use chapter end page
                section["section_end_page"] = chapter_end_page
                logging.debug(f"  Section {section['section_number']} end page defaulting to chapter end: {chapter_end_page}")
    
    # Final validation pass - ensure end page >= start page for each section
    for section in merged_sections:
        if section.get("section_start_page") is not None and section.get("section_end_page") is not None:
            if section["section_end_page"] < section["section_start_page"]:
                logging.warning(f"  Fixing invalid page range for section {section['section_number']}: {section['section_start_page']}-{section['section_end_page']}")
                section["section_end_page"] = section["section_start_page"]  # Ensure end page is at least the start page
    
    logging.info(f"  Applied page number extraction and inference to all sections")


    # 4. Process merged sections (enrichment, resumability check)
    skipped_fully_processed_count = 0
    retried_count = 0
    newly_processed_count = 0
    failed_processing_count = 0

    # Iterate through the *merged* sections now
    for section_data in tqdm(merged_sections, desc=f"Chapter {chapter_number} Sections"):
        # section_data now contains 'content', 'section_token_count', 'level',
        # 'section_title', 'start_pos', 'end_pos', 'section_number', level_X fields, etc.

        section_number = section_data["section_number"] # Use the number from the (potentially merged) section
        section_title = section_data.get('section_title', 'Unknown Title')

        # Create ID for checking/storing using potentially merged section data
        temp_id_data = {"document_id": document_id, "chapter_number": chapter_number, "section_number": section_number}
        section_id = _create_section_id(temp_id_data)
        if not section_id:
            logging.warning(f"Could not generate ID for section {section_number} ('{section_title[:30]}...'). Skipping.")
            failed_processing_count += 1
            continue

        # --- Check existing data ---
        existing_record = existing_section_details.get(section_id)
        needs_enrichment = True
        if existing_record:
            # Check if enrichment exists (using section_summary as a proxy)
            if existing_record.get("section_summary") is not None:
                logging.debug(f"  Section {section_id} already processed with enrichment. Using existing.")
                # Use the existing enriched record directly
                final_section_data = existing_record
                processed_chapter_sections.append(final_section_data)
                # Update context window even if skipped
                if final_section_data.get("section_summary"):
                    recent_section_summaries.append(final_section_data["section_summary"])
                skipped_fully_processed_count += 1
                needs_enrichment = False
            else:
                # Found existing record but it lacks enrichment, needs processing
                logging.debug(f"  Section {section_id} found with missing enrichment. Retrying enrichment.")
                retried_count += 1
        else:
             # Section not found in existing data, needs processing
             logging.debug(f"  Section {section_id} not found in existing data. Processing as new.")
             newly_processed_count += 1

        if not needs_enrichment:
            continue # Go to next section in merged_sections

        # --- Enrich Section (New or Retry) ---
        logging.debug(f"  Enriching Section {section_id} ('{section_title[:30]}...')")

        # Content for enrichment comes from the potentially merged section_data
        content_for_gpt = section_data.get("content", "")
        if not content_for_gpt:
             logging.warning(f"  Skipping enrichment for section {section_id} due to empty content after merge.")
             # Still need to assemble basic data for this section
             gpt_details = None
        elif client:
            # Use only the last N summaries for context
            context_summaries = recent_section_summaries[-MAX_RECENT_SUMMARIES_CONTEXT:]
            # Pass potentially merged content to GPT
            gpt_details = get_section_details_from_gpt(content_for_gpt, chapter_data, context_summaries, client)
        else:
            logging.debug("  Skipping GPT enrichment (no client).")
            gpt_details = None

        # Assemble final section data using the merged section_data as base
        # and adding GPT details if available
        final_section_data = section_data.copy() # Start with merged section data

        # Add/update fields based on GPT enrichment or defaults
        final_section_data["section_summary"] = gpt_details.get("section_summary", None) if gpt_details else None
        final_section_data["section_tags"] = gpt_details.get("section_tags", []) if gpt_details else []
        final_section_data["section_standard"] = gpt_details.get("section_standard", "N/A") if gpt_details else "N/A"
        final_section_data["section_standard_codes"] = gpt_details.get("section_standard_codes", []) if gpt_details else []
        final_section_data["section_importance_score"] = gpt_details.get("section_importance_score", 0.5) if gpt_details else 0.5
        final_section_data["section_references"] = gpt_details.get("section_references", []) if gpt_details else []

        # Ensure required fields from merge/split are present
        final_section_data["document_id"] = chapter_data.get("document_id")
        final_section_data["chapter_number"] = chapter_number
        final_section_data["chapter_name"] = chapter_data.get("chapter_name")
        final_section_data["chapter_tags"] = chapter_data.get("chapter_tags") # From stage 1
        final_section_data["chapter_summary"] = chapter_data.get("chapter_summary") # From stage 1
        final_section_data["chapter_token_count"] = chapter_data.get("chapter_token_count") # From stage 1

        # Rename 'content' to 'cleaned_section_content' for final output consistency
        if "content" in final_section_data:
            final_section_data["cleaned_section_content"] = final_section_data.pop("content")

        # Ensure hierarchy string is present (it should be from merge_small_sections inheriting it)
        if "section_hierarchy" not in final_section_data:
             final_section_data["section_hierarchy"] = generate_hierarchy_string(final_section_data)

        # Page numbers have already been extracted and filled in before this step
        # Just ensure they're copied to the final section data
        final_section_data["section_start_page"] = section_data.get("section_start_page")
        final_section_data["section_end_page"] = section_data.get("section_end_page")

        # Ensure token count field name is consistent ('section_token_count' is expected after merge func)
        if "section_token_count" not in final_section_data:
             # If somehow missing after merge (shouldn't happen), calculate it
             logging.warning(f"Recalculating missing section_token_count for section {section_id}")
             final_section_data["section_token_count"] = count_tokens(final_section_data.get("cleaned_section_content", ""))


        processed_chapter_sections.append(final_section_data) # Add the processed data

        # Update recent summaries list for context (use summary from final_section_data)
        current_summary = final_section_data.get("section_summary")
        if current_summary: # Only add if enrichment was successful (not None)
            recent_section_summaries.append(current_summary)
            # Keep only the last N summaries
            if len(recent_section_summaries) > MAX_RECENT_SUMMARIES_CONTEXT:
                recent_section_summaries.pop(0)
        elif gpt_details is None: # Log if enrichment failed
             failed_processing_count += 1
             logging.warning(f"  Failed to get enrichment for section {section_id} after retries.")


    # Log chapter summary
    logging.info(f"  Chapter {chapter_number} Summary: Skipped={skipped_fully_processed_count}, Retried={retried_count}, New={newly_processed_count}, Failed={failed_processing_count}")
    logging.info(f"  Total sections returned after renumbering and processing: {len(processed_chapter_sections)}")
    return processed_chapter_sections # Return all sections processed/retrieved for this chapter


def run_stage2():
    """Main function to execute Stage 2 processing with resumability."""
    logging.info("--- Starting Stage 2: Section Identification & Enrichment ---")
    create_directory(OUTPUT_DIR)
    output_filepath = Path(OUTPUT_DIR) / OUTPUT_FILENAME

    # --- Load Existing Stage 2 Data (for Resumability) ---
    existing_section_details = {} # Store as dict: {section_id: section_data}
    if output_filepath.exists():
        try:
            with open(output_filepath, "r", encoding="utf-8") as f:
                existing_data_list = json.load(f)
            if not isinstance(existing_data_list, list):
                 logging.warning(f"Existing output file {output_filepath} does not contain a valid list. Starting fresh.")
            else:
                 # Populate the dictionary, creating IDs
                 count = 0
                 for sec_data in existing_data_list:
                     sec_id = _create_section_id(sec_data)
                     if sec_id:
                         existing_section_details[sec_id] = sec_data
                         count += 1
                 logging.info(f"Loaded {count} existing section records into map from {output_filepath}.")
        except json.JSONDecodeError:
            logging.error(f"Error decoding JSON from {output_filepath}. Starting fresh.", exc_info=True)
            existing_section_details = {}
        except Exception as e:
            logging.error(f"Error loading existing data from {output_filepath}: {e}. Starting fresh.", exc_info=True)
            existing_section_details = {}

    # --- Load Stage 1 Data ---
    stage1_output_file = Path(STAGE1_OUTPUT_DIR) / STAGE1_FILENAME
    if not stage1_output_file.exists():
        logging.error(f"Stage 1 output file not found: {stage1_output_file}. Cannot proceed.")
        # Exit if stage 1 data is missing
        sys.exit(f"Error: Stage 1 output file '{stage1_output_file}' not found.")
    try:
        with open(stage1_output_file, "r", encoding="utf-8") as f:
            all_chapter_data = json.load(f)
        logging.info(f"Loaded {len(all_chapter_data)} chapters from {stage1_output_file}")
    except Exception as e:
        logging.error(f"Error loading Stage 1 data from {stage1_output_file}: {e}", exc_info=True)
        # Exit if stage 1 data is unloadable
        sys.exit(f"Error: Failed to load Stage 1 data from '{stage1_output_file}'.")

    if not all_chapter_data:
        logging.warning("Stage 1 data is empty. No sections to process.")
        # Save the potentially loaded (but empty) existing data back
        try:
            with open(output_filepath, "w", encoding="utf-8") as f:
                # Save values from the map as a list
                json.dump(list(existing_section_details.values()), f, indent=2, ensure_ascii=False)
            logging.info(f"No chapters to process. Saved {len(existing_section_details)} existing section records back to {output_filepath}")
        except Exception as e:
            logging.error(f"Error saving output JSON to {output_filepath}: {e}", exc_info=True)
        return list(existing_section_details.values()) # Return list

    # --- Initialize OpenAI Client ---
    client = get_openai_client()
    if not client:
        logging.warning("OpenAI client initialization failed. Section enrichment will be skipped.")

    # --- Process Chapters for Sections ---
    # existing_section_details dictionary holds the state, including loaded data
    # We will update this dictionary and save it incrementally

    for chapter_data in tqdm(all_chapter_data, desc="Processing Chapters for Sections"):
        chapter_number = chapter_data.get("chapter_number", "UNKNOWN")
        # Pass the dictionary of existing details for checking/retrying
        # process_chapter_for_sections returns the list of sections for *this* chapter
        chapter_results = process_chapter_for_sections(
            chapter_data, client, existing_section_details
        )

        # Update the main dictionary with the results from this chapter
        # This overwrites existing entries for retried sections with the new data
        # and adds newly processed sections. Skipped sections are already in the dict.
        updated_count = 0
        for section_data in chapter_results:
            sec_id = _create_section_id(section_data)
            if sec_id:
                # Check if this section was actually processed/retried in this run
                # or just retrieved from existing_section_details
                # We only need to update the dictionary if it's new/retried data
                # Note: process_chapter_for_sections returns *all* sections for the chapter,
                # including those skipped because they were already valid. We need to update
                # the dictionary only with sections that were actually processed or retried.
                # A simpler approach: always update the dictionary with the returned data.
                # If it was skipped, it's the same data; if processed/retried, it's the new data.
                existing_section_details[sec_id] = section_data
                updated_count += 1
            else:
                 logging.warning(f"Could not generate ID for a section in chapter {chapter_number} results, cannot update map.")

        logging.debug(f"Updated in-memory map with {updated_count} sections from chapter {chapter_number}.")

        # --- Incremental Save (after each chapter) ---
        try:
            # Convert current state of the dictionary values to a list
            current_data_list = list(existing_section_details.values())
            # Sort the list before saving
            temp_sorted_data = current_data_list
            if natsort:
                try:
                    # Sort by chapter then section number
                    temp_sorted_data = sorted(current_data_list, key=lambda x: (
                        x.get('chapter_number', float('inf')),
                        x.get('section_number', float('inf'))
                    ))
                except Exception as sort_e:
                    logging.warning(f"Could not sort data before incremental save for chapter {chapter_number}: {sort_e}. Saving in current order.")
                    # Keep temp_sorted_data as the unsorted list

            # Write the sorted list to the output file, overwriting previous content
            with open(output_filepath, "w", encoding="utf-8") as f:
                json.dump(temp_sorted_data, f, indent=2, ensure_ascii=False)
            logging.info(f"Incrementally saved {len(temp_sorted_data)} total section records after processing Chapter {chapter_number}")

        except Exception as e:
            logging.error(f"Error during incremental save to {output_filepath} after processing Chapter {chapter_number}: {e}", exc_info=True)
            # Continue processing other chapters, but progress might be lost if script fails later

    # --- Final Summary ---
    # No final save needed here as it's done incrementally
    final_record_count = len(existing_section_details) # Count items in the final dictionary
    logging.info("--- Stage 2 Summary ---")
    logging.info(f"Total chapters from Stage 1: {len(all_chapter_data)}")
    logging.info(f"Total sections in final file : {final_record_count}")
    # Add more detailed counts here if implemented
    logging.info(f"Output JSON file             : {output_filepath}")
    logging.info("--- Stage 2 Finished ---")

    return list(existing_section_details.values()) # Return the final data list

# ==============================================================================
# Main Execution Block
# ==============================================================================

if __name__ == "__main__":
    run_stage2()

Now that you have the latest state of the file, try the operation again with fewer, more precise SEARCH blocks. For large files especially, it may be prudent to try to limit yourself to <5 SEARCH/REPLACE blocks at a time, then wait for the user to respond with the result of the operation before following up with another replace_in_file call to make additional edits.
(If you run into this error 3 times in a row, you may use the write_to_file tool as a fallback.)
</error><environment_details>
# VSCode Visible Files
stage2_section_processing.py

# VSCode Open Tabs
1_chunk_creation/1_extract_chapters_to_json.py
1_chunk_creation/2_identify_sections_and_merge.py
1_chunk_creation/4_split_large_sections.py
1_chunk_creation/5_merge_small_chunks.py
2_gpt_enhancement/7_generate_chapter_details.py
2_gpt_enhancement/8_generate_section_details.py
2_gpt_enhancement/9_assemble_database_records.py
stage3_chunking_assembly.py
stage4_database_population.py
stage1_chapter_processing.py
temp_count_stage2_sections.py
stage2_section_processing.py
9_assemble_database_records.py
10_generate_embeddings.py
2_gpt_enhancement/10_generate_embeddings.py
2_gpt_enhancement/11_populate_textbook_chunks.py
2_gpt_enhancement/12_verify_database_insertion.py
2_gpt_enhancement/13_hybrid_search.py
2_gpt_enhancement/14_hybrid_search_assessment.py

# Current Time
4/16/2025, 8:22:55 AM (America/Halifax, UTC-3:00)

# Context Window Usage
274,345 / 1,048.576K tokens used (26%)

# Current Mode
ACT MODE
</environment_details>
