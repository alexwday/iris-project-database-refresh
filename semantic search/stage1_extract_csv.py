# -*- coding: utf-8 -*-
"""
Stage 1: Extract & Compare - CSV Textbook vs. NAS Filesystem (using pysmb)

This script performs the first stage of a semantic search data synchronization process.
It reads from a master CSV file (master_textbook.csv) to retrieve a catalog of textbook
content for specific document IDs. It then connects to a Network Attached Storage (NAS) 
device via SMB (using pysmb) to list the actual files present in the corresponding 
source directories.

Auto-initialization: If the master CSV file doesn't exist, it is created with 
proper schema and headers based on iris_textbook_database table.

The process assumes complete refresh for each document_id (folder):
1. All existing CSV records for the document_id are marked for deletion.
2. All files currently in the NAS folder are marked as new to be processed.
3. No "updated" files - only add/remove operations.

The results of the comparison (files to process and files to delete from CSV)
are saved as JSON files to a specified output directory on the NAS.

Configuration for NAS and processing parameters should be set in the 
'Configuration' section below or externalized to a config file.
"""

import pandas as pd
import sys
import os
from smb.SMBConnection import SMBConnection
from smb import smb_structs
import io
from datetime import datetime, timezone
import socket
import json

# ==============================================================================
# --- Configuration ---
# ==============================================================================

# --- CSV Configuration ---
# Path to master CSV file on NAS (relative to share)
MASTER_CSV_FOLDER_PATH = "path/to/master_csv_folder"
MASTER_TEXTBOOK_CSV = "master_textbook.csv"

# --- NAS Configuration ---
# Network attached storage connection parameters
NAS_PARAMS = {
        "ip": "your_nas_ip",
        "share": "your_share_name",
        "user": "your_nas_user",
        "password": "your_nas_password",
        "port": 445
}
# Base path on the NAS share containing the root folders for different document IDs
NAS_BASE_INPUT_PATH = "path/to/your/base_input_folder"
# Base path on the NAS share where output JSON files will be stored
NAS_OUTPUT_FOLDER_PATH = "path/to/your/output_folder"

# --- Processing Configuration ---
# Document IDs configuration - each line contains document_id and detail level
DOCUMENT_IDS = """
external_ey,detailed
external_ias,standard
# external_ifrs,detailed
external_ifric,standard
external_sic,concise
# external_pwc,detailed
"""

def load_document_ids():
        """Parse document IDs configuration - works for all stages"""
        document_ids = []
        for line in DOCUMENT_IDS.strip().split('\n'):
            line = line.strip()
            if line and not line.startswith('#'):
                parts = line.split(',')
                if len(parts) == 2:
                    document_id = parts[0].strip()
                    detail_level = parts[1].strip()
                    document_ids.append({
                        'document_id': document_id,
                        'detail_level': detail_level
                    })
                else:
                    print(f"Warning: Invalid config line ignored: {line}")
        return document_ids

# --- pysmb Configuration ---
smb_structs.SUPPORT_SMB2 = True
smb_structs.MAX_PAYLOAD_SIZE = 65536
CLIENT_HOSTNAME = socket.gethostname()

# ==============================================================================
# --- CSV Schema Definition ---
# ==============================================================================

# Master textbook CSV schema - based on iris_textbook_database table
TEXTBOOK_SCHEMA = {
        'id': 'int64',
        'created_at': 'datetime64[ns, UTC]',
        'document_id': 'object',
        'chapter_number': 'int64',
        'section_number': 'int64',
        'part_number': 'int64',
        'sequence_number': 'int64',
        'chapter_name': 'object',
        'chapter_tags': 'object',  # Will store JSON string representation of array
        'chapter_summary': 'object',
        'chapter_token_count': 'int64',
        'section_start_page': 'int64',
        'section_end_page': 'int64',
        'section_importance_score': 'float64',
        'section_token_count': 'int64',
        'section_hierarchy': 'object',
        'section_title': 'object',
        'section_standard': 'object',
        'section_standard_codes': 'object',  # Will store JSON string representation of array
        'section_references': 'object',  # Will store JSON string representation of array
        'content': 'object',
        'embedding': 'object',  # Will store JSON string representation of vector
        'text_search_vector': 'object',  # Will be generated by database
        # File metadata fields for tracking
        'file_name': 'object',
        'file_type': 'object',
        'file_size': 'int64',
        'file_path': 'object',
        'date_created': 'datetime64[ns, UTC]',
        'date_last_modified': 'datetime64[ns, UTC]'
}

# ==============================================================================
# --- Helper Functions ---
# ==============================================================================

def create_nas_connection():
        """Creates and returns an authenticated SMBConnection object."""
        try:
            conn = SMBConnection(
                NAS_PARAMS["user"],
                NAS_PARAMS["password"],
                CLIENT_HOSTNAME,
                NAS_PARAMS["ip"],
                use_ntlm_v2=True,
                is_direct_tcp=(NAS_PARAMS["port"] == 445)
            )
            connected = conn.connect(NAS_PARAMS["ip"], NAS_PARAMS["port"], timeout=60)
            if not connected:
                print("   [ERROR] Failed to connect to NAS.")
                return None
            print(f"   Successfully connected to NAS: {NAS_PARAMS['ip']}:{NAS_PARAMS['port']} on share '{NAS_PARAMS['share']}'")
            return conn
        except Exception as e:
            print(f"   [ERROR] Exception creating NAS connection: {e}")
            return None

def ensure_nas_dir_exists(conn, share_name, dir_path):
        """Ensures a directory exists on the NAS, creating it if necessary."""
        if not conn:
            print("   [ERROR] Cannot ensure NAS directory: No connection.")
            return False
        
        path_parts = dir_path.strip('/').split('/')
        current_path = ''
        try:
            for part in path_parts:
                if not part: continue
                current_path = os.path.join(current_path, part).replace('\\', '/')
                try:
                    conn.listPath(share_name, current_path)
                except Exception:
                    print(f"      Creating directory on NAS: {current_path}")
                    conn.createDirectory(share_name, current_path)
            return True
        except Exception as e:
            print(f"   [ERROR] Failed to ensure/create NAS directory '{dir_path}': {e}")
            return False

def write_json_to_nas(share_name, nas_path_relative, data_string):
        """Writes a string (expected to be JSON) to a specified file path on the NAS using pysmb."""
        conn = None
        print(f"   Attempting to write to NAS path: {share_name}/{nas_path_relative}")
        try:
            conn = create_nas_connection()
            if not conn:
                return False

            dir_path = os.path.dirname(nas_path_relative).replace('\\', '/')
            if dir_path and not ensure_nas_dir_exists(conn, share_name, dir_path):
                 print(f"   [ERROR] Failed to ensure output directory exists: {dir_path}")
                 return False

            data_bytes = data_string.encode('utf-8')
            file_obj = io.BytesIO(data_bytes)

            bytes_written = conn.storeFile(share_name, nas_path_relative, file_obj)
            print(f"   Successfully wrote {bytes_written} bytes to: {share_name}/{nas_path_relative}")
            return True
        except Exception as e:
            print(f"   [ERROR] Unexpected error writing to NAS '{share_name}/{nas_path_relative}': {e}")
            return False
        finally:
            if conn:
                conn.close()

def get_nas_files(share_name, base_folder_path):
        """Lists files recursively from a specific directory on an SMB share using pysmb."""
        files_list = []
        conn = None
        print(f" -> Attempting to list files from NAS path: {share_name}/{base_folder_path}")

        try:
            conn = create_nas_connection()
            if not conn:
                return None

            try:
                 conn.listPath(share_name, base_folder_path)
            except Exception as e:
                 print(f"   [ERROR] Base NAS path does not exist or is inaccessible: {share_name}/{base_folder_path} - {e}")
                 return None

            print(f" -> Walking directory tree: {share_name}/{base_folder_path} ...")
            
            def walk_nas_path(current_path_relative):
                try:
                    items = conn.listPath(share_name, current_path_relative)
                    for item in items:
                        if item.filename == '.' or item.filename == '..':
                            continue

                        if item.filename == '.DS_Store' or item.filename.startswith('~$') or item.filename.startswith('.'):
                            print(f"      Skipping system/temporary file: {item.filename}")
                            continue

                        full_path_relative = os.path.join(current_path_relative, item.filename).replace('\\', '/')

                        if item.isDirectory:
                            walk_nas_path(full_path_relative)
                        else:
                            try:
                                last_modified_dt = datetime.fromtimestamp(item.last_write_time, tz=timezone.utc)
                                created_dt = datetime.fromtimestamp(item.create_time if item.create_time else item.last_write_time, tz=timezone.utc)

                                files_list.append({
                                    'file_name': item.filename,
                                    'file_path': full_path_relative,
                                    'file_size': item.file_size,
                                    'date_last_modified': last_modified_dt,
                                    'date_created': created_dt
                                })
                            except Exception as file_err:
                                print(f"      [WARNING] Could not process file '{full_path_relative}': {file_err}. Skipping file.")

                except Exception as list_err:
                     print(f"      [WARNING] Error listing path '{current_path_relative}': {list_err}. Skipping directory.")

            walk_nas_path(base_folder_path)

            print(f" <- Successfully listed {len(files_list)} files from NAS.")
            return files_list

        except Exception as e:
            print(f"   [ERROR] Unexpected error listing NAS files from '{share_name}/{base_folder_path}': {e}")
            return None
        finally:
            if conn:
                conn.close()

def check_nas_file_exists(share_name, file_path):
        """Check if a file exists on NAS."""
        conn = None
        try:
            conn = create_nas_connection()
            if not conn:
                return False
            conn.getAttributes(share_name, file_path)
            return True
        except:
            return False
        finally:
            if conn:
                conn.close()

def write_csv_to_nas(share_name, nas_path_relative, df):
        """Write a DataFrame as CSV to NAS."""
        conn = None
        try:
            conn = create_nas_connection()
            if not conn:
                return False

            # Ensure directory exists
            dir_path = os.path.dirname(nas_path_relative).replace('\\', '/')
            if dir_path and not ensure_nas_dir_exists(conn, share_name, dir_path):
                print(f"   [ERROR] Failed to ensure CSV directory exists: {dir_path}")
                return False

            # Convert DataFrame to CSV string
            csv_content = df.to_csv(index=False)
            csv_bytes = csv_content.encode('utf-8')
            file_obj = io.BytesIO(csv_bytes)

            # Write to NAS
            bytes_written = conn.storeFile(share_name, nas_path_relative, file_obj)
            print(f"   Successfully wrote {bytes_written} bytes to: {share_name}/{nas_path_relative}")
            return True
        except Exception as e:
            print(f"   [ERROR] Failed to write CSV to NAS '{share_name}/{nas_path_relative}': {e}")
            return False
        finally:
            if conn:
                conn.close()

def read_csv_from_nas(share_name, nas_path_relative):
        """Read a CSV file from NAS into a DataFrame."""
        conn = None
        try:
            if not check_nas_file_exists(share_name, nas_path_relative):
                return pd.DataFrame()

            conn = create_nas_connection()
            if not conn:
                return None

            file_obj = io.BytesIO()
            file_attributes, filesize = conn.retrieveFile(share_name, nas_path_relative, file_obj)
            file_obj.seek(0)
            csv_content = file_obj.read().decode('utf-8')
            
            # Read CSV from string
            from io import StringIO
            df = pd.read_csv(StringIO(csv_content))
            print(f"   Successfully read CSV from: {share_name}/{nas_path_relative} ({len(df)} records)")
            return df
        except Exception as e:
            print(f"   [ERROR] Failed to read CSV from NAS '{share_name}/{nas_path_relative}': {e}")
            return None
        finally:
            if conn:
                conn.close()

def initialize_master_textbook_csv():
        """Initialize master textbook CSV file on NAS with proper schema if it doesn't exist."""
        textbook_path = os.path.join(MASTER_CSV_FOLDER_PATH, MASTER_TEXTBOOK_CSV).replace('\\', '/')
        
        # Initialize master_textbook.csv on NAS
        if not check_nas_file_exists(NAS_PARAMS["share"], textbook_path):
            print(f"   Creating master textbook CSV on NAS: {NAS_PARAMS['share']}/{textbook_path}")
            textbook_df = pd.DataFrame(columns=list(TEXTBOOK_SCHEMA.keys()))
            textbook_df = textbook_df.astype(TEXTBOOK_SCHEMA)
            if write_csv_to_nas(NAS_PARAMS["share"], textbook_path, textbook_df):
                print(f"   Created empty master_textbook.csv with {len(TEXTBOOK_SCHEMA)} columns")
            else:
                print(f"   [ERROR] Failed to create master_textbook.csv on NAS")
                return False
        else:
            print(f"   Master textbook CSV already exists on NAS: {NAS_PARAMS['share']}/{textbook_path}")
        
        return True

def load_master_textbook(document_id):
        """Load the master textbook CSV from NAS and filter by document_id."""
        textbook_path = os.path.join(MASTER_CSV_FOLDER_PATH, MASTER_TEXTBOOK_CSV).replace('\\', '/')
        
        try:
            textbook_df = read_csv_from_nas(NAS_PARAMS["share"], textbook_path)
            
            if textbook_df is None:
                print(f"   [ERROR] Failed to read master textbook from NAS")
                return pd.DataFrame(columns=list(TEXTBOOK_SCHEMA.keys()))
            
            if textbook_df.empty:
                print(f"   Master textbook CSV is empty")
                return textbook_df
            
            # Filter by document_id
            if 'document_id' in textbook_df.columns:
                textbook_df = textbook_df[textbook_df['document_id'] == document_id]
            
            # Handle timestamp columns
            if not textbook_df.empty:
                for col in ['created_at', 'date_created', 'date_last_modified']:
                    if col in textbook_df.columns:
                        textbook_df[col] = pd.to_datetime(textbook_df[col], errors='coerce', utc=True)
            
            print(f"   Loaded {len(textbook_df)} records from master textbook for document_id '{document_id}'")
            return textbook_df
                
        except Exception as e:
            print(f"   [ERROR] Failed to load master textbook from NAS: {e}")
            return pd.DataFrame(columns=list(TEXTBOOK_SCHEMA.keys()))

# ==============================================================================
# --- Main Execution Logic ---
# ==============================================================================

if __name__ == "__main__":

    print("\n" + "="*60)
    print(f"--- Running Stage 1: Extract & Compare (Textbook CSV-based) ---")
    print("="*60 + "\n")
    
    # Get document IDs
    document_configs = load_document_ids()
    print(f"[0] Processing {len(document_configs)} document IDs:")
    for config in document_configs:
        print(f"   - {config['document_id']} (detail level: {config['detail_level']})")
    print("-" * 60)

    # --- Initialize Master Textbook CSV on NAS ---
    print("\n[1] Initializing Master Textbook CSV File on NAS...")
    if not initialize_master_textbook_csv():
        print("[CRITICAL ERROR] Failed to initialize master textbook CSV file on NAS. Exiting.")
        sys.exit(1)
    print("-" * 60)
    
    # Track overall processing results
    all_document_ids_processed = []
    document_ids_with_files = []
    
    # Process each document ID
    for document_config in document_configs:
            DOCUMENT_ID = document_config['document_id']
            
            print(f"\n{'='*60}")
            print(f"Processing Document ID: {DOCUMENT_ID}")
            print(f"{'='*60}\n")

            # --- Construct NAS Paths ---
            print("[2] Constructing NAS Input and Output Paths...")
            nas_input_path_relative = os.path.join(NAS_BASE_INPUT_PATH, DOCUMENT_ID).replace('\\', '/')
            print(f"   NAS Input Path: {NAS_PARAMS['share']}/{nas_input_path_relative}")

            nas_output_dir_relative = os.path.join(NAS_OUTPUT_FOLDER_PATH, DOCUMENT_ID).replace('\\', '/')
            print(f"   NAS Output Directory: {NAS_PARAMS['share']}/{nas_output_dir_relative}")

            # Define output file paths
            csv_output_relative_file = os.path.join(nas_output_dir_relative, '1A_textbook_in_csv.json').replace('\\', '/')
            nas_output_relative_file = os.path.join(nas_output_dir_relative, '1B_files_in_nas.json').replace('\\', '/')
            process_output_relative_file = os.path.join(nas_output_dir_relative, '1C_nas_files_to_process.json').replace('\\', '/')
            delete_output_relative_file = os.path.join(nas_output_dir_relative, '1D_csv_files_to_delete.json').replace('\\', '/')
            print("-" * 60)

            # --- Ensure NAS Output Directory Exists ---
            print("[3] Ensuring NAS Output Directory Exists...")
            conn_check = None
            try:
                conn_check = create_nas_connection()
                if not conn_check:
                     print("   [ERROR] Failed to connect to NAS to check/create output directory.")
                     continue  # Skip this document_id
                if not ensure_nas_dir_exists(conn_check, NAS_PARAMS["share"], nas_output_dir_relative):
                    print(f"   [ERROR] Failed to create/access NAS output directory '{nas_output_dir_relative}'.")
                    continue  # Skip this document_id
                else:
                     print(f"   NAS output directory ensured: '{NAS_PARAMS['share']}/{nas_output_dir_relative}'")
            except Exception as e:
                print(f"   [ERROR] Unexpected error creating/accessing NAS directory '{nas_output_dir_relative}': {e}")
                continue  # Skip this document_id
            finally:
                if conn_check:
                    conn_check.close()
            print("-" * 60)

            # --- Load Data from Master Textbook CSV ---
            print(f"[4] Loading Data from Master Textbook CSV...")
            csv_df = load_master_textbook(DOCUMENT_ID)

            # Save CSV textbook data to NAS
            print(f"\n   Saving CSV textbook data to NAS file: '{os.path.basename(csv_output_relative_file)}'...")
            csv_json_string = csv_df.to_json(orient='records', indent=4, date_format='iso')
            if not write_json_to_nas(NAS_PARAMS["share"], csv_output_relative_file, csv_json_string):
                 print("   [ERROR] Failed to write CSV textbook JSON to NAS. Skipping this document_id.")
                 continue
            print("-" * 60)

            # --- Get File List from NAS ---
            print(f"[5] Listing Files from NAS Source Path: '{NAS_PARAMS['share']}/{nas_input_path_relative}'...")
            nas_files_list = get_nas_files(NAS_PARAMS["share"], nas_input_path_relative)

            if nas_files_list is None:
                print("   [ERROR] Failed to retrieve file list from NAS. Skipping this document_id.")
                continue

            nas_df = pd.DataFrame(nas_files_list)

            # Handle NAS timestamps
            if not nas_df.empty and 'date_last_modified' in nas_df.columns:
                 print("   Processing NAS timestamps...")
                 nas_df['date_last_modified'] = pd.to_datetime(nas_df['date_last_modified'], errors='coerce')
                 valid_nas_dates = nas_df['date_last_modified'].notna()
                 if not nas_df.loc[valid_nas_dates].empty:
                     if nas_df.loc[valid_nas_dates, 'date_last_modified'].dt.tz is None:
                         print("   Localizing naive NAS timestamps to UTC...")
                         nas_df.loc[valid_nas_dates, 'date_last_modified'] = nas_df.loc[valid_nas_dates, 'date_last_modified'].dt.tz_localize('UTC')
                     else:
                         print("   Converting timezone-aware NAS timestamps to UTC...")
                         nas_df.loc[valid_nas_dates, 'date_last_modified'] = nas_df.loc[valid_nas_dates, 'date_last_modified'].dt.tz_convert('UTC')
                 print("   NAS timestamps processed.")

            # Save NAS file list to NAS
            print(f"\n   Saving NAS file list data to NAS file: '{os.path.basename(nas_output_relative_file)}'...")
            nas_json_string = nas_df.to_json(orient='records', indent=4, date_format='iso')
            if not write_json_to_nas(NAS_PARAMS["share"], nas_output_relative_file, nas_json_string):
                print("   [ERROR] Failed to write NAS file list JSON to NAS. Skipping this document_id.")
                continue
            print("-" * 60)

            # --- Complete Refresh Logic ---
            print("[6] Performing Complete Refresh for Document ID...")
            print(f"   CSV records: {len(csv_df)}")
            print(f"   NAS files found: {len(nas_df)}")

            # Initialize result DataFrames
            files_to_process = pd.DataFrame(columns=['file_name', 'file_path', 'file_size', 'date_last_modified', 'date_created', 'reason'])
            files_to_delete = pd.DataFrame(columns=['id', 'document_id', 'file_name', 'file_path'])

            print("\n   *** COMPLETE REFRESH MODE ***")
            print(f"   All existing CSV records for document_id '{DOCUMENT_ID}' will be marked for deletion.")
            print(f"   All NAS files in folder '{DOCUMENT_ID}' will be marked for processing.")

            # Mark all NAS files for processing
            if nas_df.empty:
                print("   NAS directory is empty. No files to process.")
                files_to_process = pd.DataFrame(columns=['file_name', 'file_path', 'file_size', 'date_last_modified', 'date_created', 'reason'])
            else:
                print(f"   Marking all {len(nas_df)} NAS files for processing.")
                process_cols = ['file_name', 'file_path', 'file_size', 'date_last_modified', 'date_created']
                existing_process_cols = [col for col in process_cols if col in nas_df.columns]
                files_to_process = nas_df[existing_process_cols].copy()
                files_to_process['reason'] = 'complete_refresh'

            # Mark all existing CSV records for deletion
            if csv_df.empty:
                print(f"   CSV textbook is empty for document_id '{DOCUMENT_ID}'. No existing records to mark for deletion.")
                files_to_delete = pd.DataFrame(columns=['id', 'document_id', 'file_name', 'file_path'])
            else:
                print(f"   Marking all {len(csv_df)} existing CSV records for deletion.")
                delete_cols = ['id', 'document_id', 'file_name', 'file_path']
                existing_delete_cols = [col for col in delete_cols if col in csv_df.columns]
                files_to_delete = csv_df[existing_delete_cols].copy()

            # --- Final Summary ---
            print(f"\n   Complete Refresh Summary:")
            print(f"      - NAS Files to Process (All): {len(files_to_process)}")
            print(f"      - Existing CSV Records to Delete (All for document_id): {len(files_to_delete)}")
            print("-" * 60)

            # --- Save Comparison Results to NAS ---
            print("[7] Saving Comparison Results to NAS...")

            # Save files to process
            print(f"   Saving 'files to process' list to: '{os.path.basename(process_output_relative_file)}'...")
            process_json_string = files_to_process.to_json(orient='records', indent=4, date_format='iso')
            if not write_json_to_nas(NAS_PARAMS["share"], process_output_relative_file, process_json_string):
                print("   [CRITICAL ERROR] Failed to write 'files to process' JSON to NAS. Exiting.")
                sys.exit(1)

            # Save files to delete
            print(f"   Saving 'files to delete' list to: '{os.path.basename(delete_output_relative_file)}'...")
            if 'id' in files_to_delete.columns and not files_to_delete['id'].isnull().all():
                 files_to_delete['id'] = files_to_delete['id'].astype('Int64')
            delete_json_string = files_to_delete.to_json(orient='records', indent=4)
            if not write_json_to_nas(NAS_PARAMS["share"], delete_output_relative_file, delete_json_string):
                print("   [CRITICAL ERROR] Failed to write 'files to delete' JSON to NAS. Exiting.")
                sys.exit(1)
            print("-" * 60)

            # --- Create Skip Flag if No Files to Process ---
            print("[8] Managing Flag Files...")
            skip_flag_file_name = '_SKIP_SUBSEQUENT_STAGES.flag'
            complete_refresh_flag_file_name = '_COMPLETE_REFRESH.flag'
            skip_flag_relative_path = os.path.join(nas_output_dir_relative, skip_flag_file_name).replace('\\', '/')
            complete_refresh_flag_relative_path = os.path.join(nas_output_dir_relative, complete_refresh_flag_file_name).replace('\\', '/')
            conn_flag = None

            try:
                conn_flag = create_nas_connection()
                if not conn_flag:
                    print("   [WARNING] Failed to connect to NAS to manage flag files. Skipping flag operations.")
                else:
                    # Skip flag logic - only skip if no files to process AND no files to delete
                    if files_to_process.empty and files_to_delete.empty:
                        print(f"   No files to process and no files to delete found. Creating skip flag file: '{skip_flag_file_name}'")
                        try:
                            conn_flag.storeFile(NAS_PARAMS["share"], skip_flag_relative_path, io.BytesIO(b''))
                            print(f"   Successfully created skip flag file: {skip_flag_relative_path}")
                        except Exception as e:
                            print(f"   [WARNING] Error creating skip flag file '{skip_flag_relative_path}': {e}")
                    else:
                        action_reason = []
                        if not files_to_process.empty:
                            action_reason.append(f"{len(files_to_process)} files to process")
                        if not files_to_delete.empty:
                            action_reason.append(f"{len(files_to_delete)} files to delete")
                        
                        print(f"   Work found: {', '.join(action_reason)}. Ensuring skip flag does not exist.")
                        try:
                            conn_flag.deleteFiles(NAS_PARAMS["share"], skip_flag_relative_path)
                            print(f"   Removed potentially existing skip flag file: {skip_flag_relative_path}")
                        except Exception as e:
                            if "OBJECT_NAME_NOT_FOUND" not in str(e) and "STATUS_NO_SUCH_FILE" not in str(e):
                                 print(f"   [INFO] Error removing skip flag file (may not exist): {e}")
                            else:
                                 print(f"   Skip flag file did not exist.")

                    # Complete refresh flag logic - always create since we always do complete refresh
                    print(f"   Complete refresh mode enabled. Creating complete refresh flag file: '{complete_refresh_flag_file_name}'")
                    try:
                        conn_flag.storeFile(NAS_PARAMS["share"], complete_refresh_flag_relative_path, io.BytesIO(b''))
                        print(f"   Successfully created complete refresh flag file: {complete_refresh_flag_relative_path}")
                    except Exception as e:
                        print(f"   [WARNING] Error creating complete refresh flag file '{complete_refresh_flag_relative_path}': {e}")

            except Exception as e:
                print(f"   [WARNING] Unexpected error during flag file management: {e}")
            finally:
                if conn_flag:
                    conn_flag.close()

                # Track this document_id as processed
                all_document_ids_processed.append(DOCUMENT_ID)
                if not files_to_process.empty:
                    document_ids_with_files.append(DOCUMENT_ID)
                
                print(f"   Document ID '{DOCUMENT_ID}' processing completed.")
                print("-" * 60)

    # Final summary
    print("\n" + "="*60)
    print(f"--- Stage 1 Completed Successfully (Textbook CSV-based) ---")
    print(f"--- Processed {len(all_document_ids_processed)} document IDs ---")
    print(f"--- Document IDs with files to process: {len(document_ids_with_files)} ---")
    if document_ids_with_files:
        print(f"--- Document IDs with files: {', '.join(document_ids_with_files)} ---")
    print("="*60 + "\n")