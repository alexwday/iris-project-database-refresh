# -*- coding: utf-8 -*-
"""
Stage 2: Section Identification & Enrichment - ROBUST VERSION
Aligned with Stage 1 Chapter Processing methods

Purpose:
Processes page-level data generated by Stage 1. It groups pages by chapter,
identifies logical sections within each chapter based on Markdown headings,
and uses an LLM to generate structured details for each section.

Input: JSON file from Stage 1 output (stage1_page_records.json)
Output: JSON file containing enriched section data (stage2_section_records.json)
"""

import os
import json
import traceback
import re
import time
import logging
import requests
import tempfile
import socket
import io
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional, Union
from collections import defaultdict
from datetime import datetime

# --- pysmb imports for NAS access ---
from smb.SMBConnection import SMBConnection
from smb import smb_structs

# --- Dependencies Check ---
try:
    from openai import OpenAI, APIError
except ImportError:
    OpenAI = None
    APIError = None
    print("ERROR: openai library not installed. GPT features unavailable. `pip install openai`")

try:
    from tqdm import tqdm
except ImportError:
    tqdm = lambda x, **kwargs: x
    print("INFO: tqdm not installed. Progress bars disabled. `pip install tqdm`")

# ==============================================================================
# Configuration (Hardcoded - update these values)
# ==============================================================================

# --- NAS Configuration ---
NAS_PARAMS = {
    "ip": "your_nas_ip",  # TODO: Replace with actual NAS IP
    "share": "your_share_name",  # TODO: Replace with actual share name
    "user": "your_nas_user",  # TODO: Replace with actual NAS username
    "password": "your_nas_password",  # TODO: Replace with actual NAS password
    "port": 445  # Default SMB port (can be 139)
}

# --- Directory Paths (Relative to NAS Share) ---
# Stage 1 output path (input for Stage 2)
NAS_INPUT_PATH = "semantic_search/pipeline_output/stage1"  # Same as Stage 1's NAS_OUTPUT_PATH
INPUT_FILENAME = "stage1_page_records.json"  # Stage 1's OUTPUT_FILENAME
# Stage 2 output paths
NAS_OUTPUT_PATH = "semantic_search/pipeline_output/stage2"
NAS_LOG_PATH = "semantic_search/pipeline_output/logs"
OUTPUT_FILENAME = "stage2_section_records.json"

# --- CA Bundle Configuration ---
NAS_SSL_CERT_PATH = "certificates/rbc-ca-bundle.cer"
SSL_LOCAL_PATH = "/tmp/rbc-ca-bundle.cer"

# --- API Configuration ---
BASE_URL = "https://api.example.com/v1"  # TODO: Replace with actual API base URL
MODEL_NAME_CHAT = "gpt-4-turbo-nonp"  # TODO: Replace with actual model name
OAUTH_URL = "https://api.example.com/oauth/token"  # TODO: Replace with actual OAuth URL
CLIENT_ID = "your_client_id"  # TODO: Replace with actual client ID
CLIENT_SECRET = "your_client_secret"  # TODO: Replace with actual client secret

# --- API Parameters ---
GPT_INPUT_TOKEN_LIMIT = 80000  # Maximum tokens for input/prompt
MAX_COMPLETION_TOKENS = 4000   # Maximum tokens for output/completion
TEMPERATURE = 0.3
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 5
TOKEN_BUFFER = 2000  # Safety buffer for prompt overhead

# Retry parameters specifically for tool response validation
TOOL_RESPONSE_RETRIES = 5  # More retries for getting proper tool responses
TOOL_RESPONSE_RETRY_DELAY = 3

# --- Token Cost ---
PROMPT_TOKEN_COST = 0.01
COMPLETION_TOKEN_COST = 0.03

# --- pysmb Configuration ---
smb_structs.SUPPORT_SMB2 = True
smb_structs.MAX_PAYLOAD_SIZE = 65536
CLIENT_HOSTNAME = socket.gethostname()

# --- Logging Level Control ---
VERBOSE_LOGGING = False

# ==============================================================================
# Stage 2 Specific Configuration
# ==============================================================================

# --- Section Merging Thresholds ---
MAX_TOKENS = 750  # Maximum tokens allowed in a merged section
MIN_TOKENS = 250  # Sections below this count trigger merging logic (Pass 1)
ULTRA_SMALL_THRESHOLD = 25  # Sections below this trigger more aggressive merging (Pass 2)

# --- Context Window for Section Processing ---
MAX_RECENT_SUMMARIES_CONTEXT = 5  # Number of previous section summaries to include in context

# ==============================================================================
# Configuration Validation
# ==============================================================================

def validate_configuration():
    """Validates that configuration values have been properly set."""
    errors = []
    
    if "your_nas_ip" in NAS_PARAMS["ip"]:
        errors.append("NAS IP address not configured")
    if "your_share_name" in NAS_PARAMS["share"]:
        errors.append("NAS share name not configured")
    if "your_nas_user" in NAS_PARAMS["user"]:
        errors.append("NAS username not configured")
    if "your_nas_password" in NAS_PARAMS["password"]:
        errors.append("NAS password not configured")
    if "api.example.com" in BASE_URL:
        errors.append("API base URL not configured")
    if "api.example.com" in OAUTH_URL:
        errors.append("OAuth URL not configured")
    if "your_client_id" in CLIENT_ID:
        errors.append("Client ID not configured")
    if "your_client_secret" in CLIENT_SECRET:
        errors.append("Client secret not configured")
    
    if errors:
        print("❌ Configuration errors detected:")
        for error in errors:
            print(f"  - {error}")
        print("\nPlease update the configuration values in the script before running.")
        return False
    return True

# ==============================================================================
# NAS Helper Functions (Aligned with Stage 1)
# ==============================================================================

def create_nas_connection():
    """Creates and returns an authenticated SMBConnection object."""
    try:
        conn = SMBConnection(
            NAS_PARAMS["user"],
            NAS_PARAMS["password"],
            CLIENT_HOSTNAME,
            NAS_PARAMS["ip"],
            use_ntlm_v2=True,
            is_direct_tcp=(NAS_PARAMS["port"] == 445)
        )
        connected = conn.connect(NAS_PARAMS["ip"], NAS_PARAMS["port"], timeout=60)
        if not connected:
            logging.error("Failed to connect to NAS")
            return None
        return conn
    except Exception as e:
        logging.error(f"Exception creating NAS connection: {e}")
        return None

def ensure_nas_dir_exists(conn, share_name, dir_path_relative):
    """Ensures a directory exists on the NAS, creating it if necessary."""
    if not conn:
        return False
    
    path_parts = dir_path_relative.strip('/').split('/')
    current_path = ''
    try:
        for part in path_parts:
            if not part: continue
            current_path = os.path.join(current_path, part).replace('\\', '/')
            try:
                conn.listPath(share_name, current_path)
            except Exception:
                conn.createDirectory(share_name, current_path)
        return True
    except Exception as e:
        logging.error(f"Failed to ensure NAS directory: {e}")
        return False

def write_to_nas(share_name, nas_path_relative, content_bytes):
    """Writes bytes to a file path on the NAS using pysmb."""
    conn = None
    try:
        conn = create_nas_connection()
        if not conn:
            return False

        dir_path = os.path.dirname(nas_path_relative).replace('\\', '/')
        if dir_path and not ensure_nas_dir_exists(conn, share_name, dir_path):
            return False

        file_obj = io.BytesIO(content_bytes)
        bytes_written = conn.storeFile(share_name, nas_path_relative, file_obj)
        
        if bytes_written == 0 and len(content_bytes) > 0:
            logging.error(f"No bytes written to {nas_path_relative}")
            return False
            
        return True
    except Exception as e:
        logging.error(f"Error writing to NAS: {e}")
        return False
    finally:
        if conn:
            conn.close()

def read_from_nas(share_name, nas_path_relative):
    """Reads content (as bytes) from a file path on the NAS using pysmb."""
    conn = None
    file_obj = None
    try:
        conn = create_nas_connection()
        if not conn:
            return None

        file_obj = io.BytesIO()
        file_attributes, filesize = conn.retrieveFile(share_name, nas_path_relative, file_obj)
        file_obj.seek(0)
        content_bytes = file_obj.read()
        return content_bytes
    except Exception as e:
        logging.error(f"Error reading from NAS: {e}")
        return None
    finally:
        if file_obj:
            try:
                file_obj.close()
            except:
                pass
        if conn:
            conn.close()

# ==============================================================================
# Logging Setup (Aligned with Stage 1)
# ==============================================================================

def setup_logging():
    """Setup logging with controlled verbosity - aligned with Stage 1."""
    temp_log = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.log')
    temp_log_path = temp_log.name
    temp_log.close()
    
    # Clear any existing handlers to prevent duplication
    logging.root.handlers = []
    
    log_level = logging.DEBUG if VERBOSE_LOGGING else logging.WARNING
    
    # Only add file handler to root logger (no console handler)
    root_file_handler = logging.FileHandler(temp_log_path)
    root_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[root_file_handler]
    )
    
    # Progress logger handles all console output
    progress_logger = logging.getLogger('progress')
    progress_logger.setLevel(logging.INFO)
    progress_logger.propagate = False
    
    # Console handler for progress messages
    progress_console_handler = logging.StreamHandler()
    progress_console_handler.setFormatter(logging.Formatter('%(message)s'))
    progress_logger.addHandler(progress_console_handler)
    
    # File handler for progress messages
    progress_file_handler = logging.FileHandler(temp_log_path)
    progress_file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
    progress_logger.addHandler(progress_file_handler)
    
    # If verbose logging is enabled, also show warnings/errors on console
    if VERBOSE_LOGGING:
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(logging.Formatter('%(levelname)s - %(message)s'))
        console_handler.setLevel(logging.WARNING)
        logging.root.addHandler(console_handler)
    
    return temp_log_path

def log_progress(message, end='\n'):
    """Log a progress message that always shows."""
    progress_logger = logging.getLogger('progress')
    
    if end == '':
        sys.stdout.write(message)
        sys.stdout.flush()
        for handler in progress_logger.handlers:
            if isinstance(handler, logging.FileHandler):
                handler.stream.write(f"{datetime.now().isoformat()} - {message}\n")
                handler.flush()
    else:
        progress_logger.info(message)

# ==============================================================================
# Token Counting (Aligned with Stage 1)
# ==============================================================================

def count_tokens(text: str) -> int:
    """Estimates token count using empirically-calibrated formula from Stage 1."""
    if not text:
        return 0
    
    char_count = len(text)
    estimated_tokens = int(char_count / 3.5)
    
    MIN_CHARS_PER_TOKEN = 2
    MAX_CHARS_PER_TOKEN = 10
    
    max_possible_tokens = char_count // MIN_CHARS_PER_TOKEN
    min_possible_tokens = char_count // MAX_CHARS_PER_TOKEN
    
    estimated_tokens = max(min_possible_tokens, min(estimated_tokens, max_possible_tokens))
    
    return estimated_tokens

# ==============================================================================
# API Client Setup (Aligned with Stage 1)
# ==============================================================================

_SSL_CONFIGURED = False
_OPENAI_CLIENT = None

def _setup_ssl_from_nas() -> bool:
    """Downloads SSL cert from NAS and sets environment variables."""
    global _SSL_CONFIGURED
    if _SSL_CONFIGURED:
        return True
    
    try:
        cert_bytes = read_from_nas(NAS_PARAMS["share"], NAS_SSL_CERT_PATH)
        if cert_bytes is None:
            logging.warning("SSL certificate not found on NAS, continuing without it")
            _SSL_CONFIGURED = True
            return True
        
        local_cert = Path(SSL_LOCAL_PATH)
        local_cert.parent.mkdir(parents=True, exist_ok=True)
        with open(local_cert, "wb") as f:
            f.write(cert_bytes)
        
        os.environ["SSL_CERT_FILE"] = str(local_cert)
        os.environ["REQUESTS_CA_BUNDLE"] = str(local_cert)
        _SSL_CONFIGURED = True
        return True
    except Exception as e:
        logging.error(f"Error setting up SSL: {e}")
        return False

def _get_oauth_token(oauth_url=OAUTH_URL, client_id=CLIENT_ID, client_secret=CLIENT_SECRET, ssl_verify_path=SSL_LOCAL_PATH) -> Optional[str]:
    """Retrieves OAuth token."""
    try:
        verify_path = ssl_verify_path if ssl_verify_path and Path(ssl_verify_path).exists() else True
    except (TypeError, OSError):
        verify_path = True

    payload = {'grant_type': 'client_credentials', 'client_id': client_id, 'client_secret': client_secret}
    try:
        response = requests.post(oauth_url, data=payload, timeout=30, verify=verify_path)
        response.raise_for_status()
        token_data = response.json()
        oauth_token = token_data.get('access_token')
        if not oauth_token:
            logging.error("No access token in OAuth response")
            return None
        return oauth_token
    except requests.exceptions.RequestException as e:
        logging.error(f"OAuth token request failed: {e}")
        return None

def get_openai_client(base_url=BASE_URL) -> Optional[OpenAI]:
    """Initializes and returns the OpenAI client."""
    global _OPENAI_CLIENT
    if _OPENAI_CLIENT:
        return _OPENAI_CLIENT
    if not OpenAI:
        return None
    if not _setup_ssl_from_nas():
        pass

    api_key = _get_oauth_token()
    if not api_key:
        return None
    try:
        _OPENAI_CLIENT = OpenAI(api_key=api_key, base_url=base_url)
        return _OPENAI_CLIENT
    except Exception as e:
        logging.error(f"Failed to create OpenAI client: {e}")
        return None

# ==============================================================================
# IMPROVED API Call with Strict Tool Enforcement (from Stage 1)
# ==============================================================================

def call_gpt_with_tool_enforcement(client, model, messages, max_tokens, temperature, tool_schema):
    """
    Makes API call with STRICT tool enforcement.
    Will retry if response doesn't use the specified tool.
    """
    tool_name = tool_schema["function"]["name"]
    
    # Validate messages list is not empty
    if not messages:
        logging.error("Messages list is empty")
        return None, None
    
    for attempt in range(TOOL_RESPONSE_RETRIES):
        try:
            if attempt > 0:
                logging.info(f"Tool response retry {attempt + 1}/{TOOL_RESPONSE_RETRIES}")
                # Add enforcement message to prompt
                enforcement_msg = {
                    "role": "system",
                    "content": f"CRITICAL: You MUST use the '{tool_name}' tool to provide your response. Do not respond with plain text."
                }
                enhanced_messages = messages[:-1] + [enforcement_msg] + messages[-1:]
            else:
                enhanced_messages = messages
            
            response = client.chat.completions.create(
                model=model,
                messages=enhanced_messages,
                max_tokens=max_tokens,
                temperature=temperature,
                tools=[tool_schema],
                tool_choice={"type": "function", "function": {"name": tool_name}},
                stream=False
            )
            
            response_message = response.choices[0].message
            usage_info = response.usage
            
            # Validate we got a tool call
            if not response_message.tool_calls:
                logging.warning(f"Attempt {attempt + 1}: No tool calls in response")
                time.sleep(TOOL_RESPONSE_RETRY_DELAY)
                continue
            
            tool_call = response_message.tool_calls[0]
            
            # Validate it's the correct tool
            if tool_call.function.name != tool_name:
                logging.warning(f"Attempt {attempt + 1}: Wrong tool used: {tool_call.function.name}")
                time.sleep(TOOL_RESPONSE_RETRY_DELAY)
                continue
            
            # Parse and validate the tool arguments
            try:
                function_args = json.loads(tool_call.function.arguments)
                
                # Validate required fields are present and non-empty
                required_fields = tool_schema["function"]["parameters"].get("required", [])
                for field in required_fields:
                    if field not in function_args:
                        logging.warning(f"Attempt {attempt + 1}: Missing required field '{field}'")
                        time.sleep(TOOL_RESPONSE_RETRY_DELAY)
                        break
                else:
                    # All required fields present - return the validated response
                    return function_args, usage_info
                    
            except json.JSONDecodeError as e:
                logging.warning(f"Attempt {attempt + 1}: Invalid JSON in tool arguments: {e}")
                logging.debug(f"Malformed JSON content: {tool_call.function.arguments[:500]}...")
                time.sleep(TOOL_RESPONSE_RETRY_DELAY)
                continue
                
        except APIError as e:
            logging.warning(f"API Error on attempt {attempt + 1}: {e}")
            time.sleep(TOOL_RESPONSE_RETRY_DELAY * (2 ** min(attempt, 3)))
        except Exception as e:
            logging.warning(f"Unexpected error on attempt {attempt + 1}: {e}")
            time.sleep(TOOL_RESPONSE_RETRY_DELAY)
    
    # All retries exhausted
    logging.error(f"Failed to get valid tool response after {TOOL_RESPONSE_RETRIES} attempts")
    return None, None

# ==============================================================================
# Page Processing and Chapter Consolidation
# ==============================================================================

def group_pages_by_chapter(page_records: List[Dict]) -> Dict[int, List[Dict]]:
    """Groups page records by chapter_number."""
    chapters = defaultdict(list)
    
    for record in page_records:
        chapter_num = record.get('chapter_number')
        if chapter_num is not None:
            chapters[chapter_num].append(record)
    
    # Sort pages within each chapter by page number (Stage 1 provides pdf_page_number)
    for chapter_num in chapters:
        chapters[chapter_num].sort(key=lambda x: x.get('pdf_page_number', x.get('page_number', 0)))
    
    return dict(chapters)

def consolidate_chapter_data(chapter_num: int, pages: List[Dict]) -> Dict:
    """Consolidates page records into chapter data for section processing."""
    if not pages:
        return None
    
    # Get chapter metadata from first page
    first_page = pages[0]
    chapter_name = first_page.get('chapter_name', f'Chapter {chapter_num}')
    chapter_summary = first_page.get('chapter_summary')
    chapter_tags = first_page.get('chapter_tags', [])
    chapter_token_count = first_page.get('chapter_token_count', 0)
    # Use pdf_filename if available (Stage 1 output), fallback to filename
    filename = first_page.get('pdf_filename', first_page.get('filename', 'unknown.pdf'))
    
    # Concatenate all page content
    content_parts = []
    for page in pages:
        page_content = page.get('content', '')
        if page_content:
            content_parts.append(page_content)
    
    raw_content = "\n\n".join(content_parts)
    
    # Determine page range - use pdf_page_number from Stage 1, fallback to page_number
    page_numbers = [p.get('pdf_page_number', p.get('page_number', 0)) for p in pages]
    chapter_page_start = min(page_numbers) if page_numbers else None
    chapter_page_end = max(page_numbers) if page_numbers else None
    
    return {
        'chapter_number': chapter_num,
        'chapter_name': chapter_name,
        'chapter_summary': chapter_summary,
        'chapter_tags': chapter_tags,
        'chapter_token_count': chapter_token_count,
        'chapter_page_start': chapter_page_start,
        'chapter_page_end': chapter_page_end,
        'document_id': filename,  # Use filename as document_id
        'raw_content': raw_content
    }

# ==============================================================================
# Section Identification
# ==============================================================================

# Page tag patterns from original Stage 2
PAGE_NUMBER_TAG_PATTERN = re.compile(r'<!--\s*PageNumber="(\d+)"\s*-->')
AZURE_TAG_PATTERN = re.compile(r'<!--\s*Page(Footer|Number|Break|Header)=?(".*?"|\d+)?\s*-->\s*\n?')

def clean_azure_tags(text: str) -> str:
    """Removes Azure Document Intelligence specific HTML comment tags."""
    return AZURE_TAG_PATTERN.sub("", text)

def find_headings(raw_content: str) -> list[dict]:
    """Finds Markdown headings (levels 1-6) in raw text."""
    heading_pattern = re.compile(r"^(#{1,6})\s+(.+)$", re.MULTILINE)
    headings = []
    for match in heading_pattern.finditer(raw_content):
        headings.append({
            "level": len(match.group(1)),
            "text": match.group(2).strip(),
            "position": match.start()
        })
    # Add virtual end marker
    headings.append({"level": 0, "text": "DOCUMENT_END", "position": len(raw_content)})
    headings.sort(key=lambda h: h["position"])
    return headings

def split_chapter_into_sections(chapter_data: dict) -> list[dict]:
    """Splits raw chapter content into initial sections based on headings."""
    raw_content = chapter_data["raw_content"]
    headings = find_headings(raw_content)
    initial_sections = []
    section_index_in_chapter = 0
    current_heading_context = {f"level_{i}": None for i in range(1, 7)}
    # Initialize L1 with chapter name for context
    current_heading_context["level_1"] = chapter_data.get("chapter_name")
    # Get chapter-level fields once for potential use
    document_id = chapter_data.get("document_id")
    chapter_number = chapter_data.get("chapter_number")
    chapter_name = chapter_data.get("chapter_name")
    chapter_token_count = chapter_data.get("chapter_token_count")
    chapter_page_start = chapter_data.get("chapter_page_start")
    chapter_page_end = chapter_data.get("chapter_page_end")

    # Handle content before the first heading
    first_heading_pos = headings[0]['position'] if headings and headings[0]['level'] > 0 else len(raw_content)
    if first_heading_pos > 0:
        intro_slice = raw_content[:first_heading_pos].strip()
        if intro_slice:
            section_index_in_chapter += 1
            intro_section_data = {
                # Pass-through fields
                "document_id": document_id,
                "chapter_number": chapter_number,
                "chapter_name": chapter_name,
                "chapter_token_count": chapter_token_count,
                "chapter_page_start": chapter_page_start,
                "chapter_page_end": chapter_page_end,
                # Section specific fields
                "raw_section_slice": intro_slice,
                "level": 1,
                "section_title": chapter_name or "Introduction",
                "start_pos": 0,
                "end_pos": first_heading_pos,
                "section_number": section_index_in_chapter,
                "level_1": chapter_name,
            }
            initial_sections.append(intro_section_data)

    # Process sections defined by headings
    for i in range(len(headings) - 1):
        current_heading = headings[i]
        next_heading = headings[i + 1]

        if current_heading["level"] == 0: continue

        section_start_pos = current_heading["position"]
        section_end_pos = next_heading["position"]
        raw_section_slice = raw_content[section_start_pos:section_end_pos].strip()

        if raw_section_slice:
            section_index_in_chapter += 1
            current_level = current_heading["level"]
            current_title = current_heading["text"]

            # Update heading context
            current_heading_context[f"level_{current_level}"] = current_title
            for lower_level in range(current_level + 1, 7):
                current_heading_context[f"level_{lower_level}"] = None

            section_data = {
                # Pass-through fields
                "document_id": document_id,
                "chapter_number": chapter_number,
                "chapter_name": chapter_name,
                "chapter_token_count": chapter_token_count,
                "chapter_page_start": chapter_page_start,
                "chapter_page_end": chapter_page_end,
                # Section specific fields
                "raw_section_slice": raw_section_slice,
                "level": current_level,
                "section_title": current_title,
                "start_pos": section_start_pos,
                "end_pos": section_end_pos,
                "section_number": section_index_in_chapter,
            }
            # Add current hierarchy context
            for level_num in range(1, 7):
                level_key = f"level_{level_num}"
                if current_heading_context.get(level_key):
                    section_data[level_key] = current_heading_context[level_key]

            initial_sections.append(section_data)

    return initial_sections

def generate_hierarchy_string(section_data: dict) -> str:
    """Generates a breadcrumb-style hierarchy string."""
    parts = []
    max_level_to_check = section_data.get("level", 6)
    for i in range(1, max_level_to_check + 1):
        level_key = f"level_{i}"
        heading_text = section_data.get(level_key)
        if heading_text: 
            parts.append(heading_text)
        else: 
            break
    return " > ".join(parts)

# ==============================================================================
# Section Merging Logic
# ==============================================================================

def merge_small_sections(
    sections: list[dict], min_tokens: int, max_tokens: int, ultra_small_threshold: int
) -> list[dict]:
    """
    Merges sections smaller than thresholds with adjacent sections,
    respecting hierarchy and max_tokens limit.
    """
    if not sections:
        return []

    # Ensure sections are sorted by their original position
    sections_to_process = sorted(sections, key=lambda s: s["section_number"])

    # --- Pass 1: Merge sections smaller than min_tokens ---
    pass1_merged = []
    i = 0
    while i < len(sections_to_process):
        current = sections_to_process[i]
        current_tokens = current.get("section_token_count", 0)

        if current_tokens >= min_tokens:
            pass1_merged.append(current)
            i += 1
            continue

        merged_pass1 = False

        # Try to merge forward with next section
        if i + 1 < len(sections_to_process):
            next_s = sections_to_process[i + 1]
            next_tokens = next_s.get("section_token_count", 0)
            if (
                current.get("chapter_number") == next_s.get("chapter_number")
                and current.get("level") == next_s.get("level")
                and current_tokens + next_tokens <= max_tokens
            ):
                merged_data = current.copy()
                merged_data["content"] = f"{current.get('content', '')}\n\n{next_s.get('content', '')}"
                merged_data["section_token_count"] = count_tokens(merged_data["content"])
                merged_data["end_pos"] = next_s["end_pos"]
                pass1_merged.append(merged_data)
                i += 2
                merged_pass1 = True

        # Try to merge backward with previous section
        if not merged_pass1 and pass1_merged:
            prev_s = pass1_merged[-1]
            prev_tokens = prev_s.get("section_token_count", 0)
            if (
                current.get("chapter_number") == prev_s.get("chapter_number")
                and prev_tokens + current_tokens <= max_tokens
                and current.get("level", 1) >= prev_s.get("level", 1)
            ):
                prev_s["content"] = f"{prev_s.get('content', '')}\n\n{current.get('content', '')}"
                prev_s["section_token_count"] = count_tokens(prev_s["content"])
                prev_s["end_pos"] = current["end_pos"]
                i += 1
                merged_pass1 = True

        if not merged_pass1:
            pass1_merged.append(current)
            i += 1

    # --- Pass 2: Merge ultra-small sections ---
    if not pass1_merged:
        return []

    final_merged = []
    i = 0
    while i < len(pass1_merged):
        current = pass1_merged[i]
        current_tokens = current.get("section_token_count", 0)

        if current_tokens >= ultra_small_threshold:
            final_merged.append(current)
            i += 1
            continue

        # Determine if content is just a heading
        is_heading_only = (
            re.match(r"^\s*#+\s+[^#]", current.get("content", "").strip()) is not None
        )
        merged_pass2 = False

        # Headings prefer merging forward
        if is_heading_only:
            if i + 1 < len(pass1_merged):
                next_s = pass1_merged[i + 1]
                next_tokens = next_s.get("section_token_count", 0)
                if (
                    current.get("chapter_number") == next_s.get("chapter_number")
                    and current_tokens + next_tokens <= max_tokens
                ):
                    merged_data = next_s.copy()
                    merged_data["content"] = f"{current.get('content', '')}\n\n{next_s.get('content', '')}"
                    merged_data["section_token_count"] = count_tokens(merged_data["content"])
                    merged_data["start_pos"] = current["start_pos"]
                    final_merged.append(merged_data)
                    i += 2
                    merged_pass2 = True
        else:
            # Content sections prefer merging backward
            if final_merged:
                prev_s = final_merged[-1]
                prev_tokens = prev_s.get("section_token_count", 0)
                if (
                    current.get("chapter_number") == prev_s.get("chapter_number")
                    and prev_tokens + current_tokens <= max_tokens
                ):
                    prev_s["content"] = f"{prev_s.get('content', '')}\n\n{current.get('content', '')}"
                    prev_s["section_token_count"] = count_tokens(prev_s["content"])
                    prev_s["end_pos"] = current["end_pos"]
                    i += 1
                    merged_pass2 = True

        # Fallback merging strategies
        if not merged_pass2:
            if is_heading_only:
                # Fallback: merge backward
                if final_merged:
                    prev_s = final_merged[-1]
                    prev_tokens = prev_s.get("section_token_count", 0)
                    if (
                        current.get("chapter_number") == prev_s.get("chapter_number")
                        and prev_tokens + current_tokens <= max_tokens
                    ):
                        prev_s["content"] = f"{prev_s.get('content', '')}\n\n{current.get('content', '')}"
                        prev_s["section_token_count"] = count_tokens(prev_s["content"])
                        prev_s["end_pos"] = current["end_pos"]
                        i += 1
                        merged_pass2 = True
            else:
                # Fallback: merge forward
                if i + 1 < len(pass1_merged):
                    next_s = pass1_merged[i + 1]
                    next_tokens = next_s.get("section_token_count", 0)
                    if (
                        current.get("chapter_number") == next_s.get("chapter_number")
                        and current_tokens + next_tokens <= max_tokens
                    ):
                        merged_data = next_s.copy()
                        merged_data["content"] = f"{current.get('content', '')}\n\n{next_s.get('content', '')}"
                        merged_data["section_token_count"] = count_tokens(merged_data["content"])
                        merged_data["start_pos"] = current["start_pos"]
                        final_merged.append(merged_data)
                        i += 2
                        merged_pass2 = True

        if not merged_pass2:
            final_merged.append(current)
            i += 1

    # Clean up field names
    for section in final_merged:
        section.pop("word_count", None)
        if "chunk_token_count" in section and "section_token_count" not in section:
            section["section_token_count"] = section.pop("chunk_token_count")

    return final_merged

# ==============================================================================
# GPT Prompting for Section Details (with Tool Enforcement)
# ==============================================================================

SECTION_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "provide_section_analysis",
        "description": "Provides structured analysis of section content including summary, tags, and metadata",
        "parameters": {
            "type": "object",
            "properties": {
                "section_summary": {
                    "type": "string",
                    "description": "A concise summary (1-3 sentences) capturing the core topic or purpose of this section"
                },
                "section_tags": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 3,
                    "maxItems": 15,
                    "description": "Meaningful keywords or tags specific to this section's content"
                },
                "section_standard": {
                    "type": "string",
                    "description": "The primary accounting standard framework (e.g., 'IFRS', 'US GAAP', 'N/A')"
                },
                "section_standard_codes": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Specific standard codes mentioned or relevant (e.g., ['IFRS 16', 'IAS 17'])"
                },
                "section_importance_score": {
                    "type": "number",
                    "minimum": 0.0,
                    "maximum": 1.0,
                    "description": "Importance score between 0.0 (low) and 1.0 (high) for this section"
                },
                "section_references": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "Explicit references to other sections, chapters, or standards found in the text"
                }
            },
            "required": ["section_summary", "section_tags", "section_standard", 
                        "section_standard_codes", "section_importance_score", "section_references"],
            "additionalProperties": False
        }
    }
}

def build_section_analysis_prompt(section_text, chapter_summary=None, chapter_tags=None, 
                                 previous_summaries=None):
    """Builds prompt for section analysis with tool enforcement."""
    if previous_summaries is None: 
        previous_summaries = []

    system_prompt = """<role>
You are an expert financial reporting specialist analyzing EY technical accounting guidance.
</role>

<context>
You are processing a section from comprehensive accounting guidance covering IFRS and US GAAP.
The content will be used to build a searchable knowledge base for accounting professionals.
</context>

<objective>
Extract and structure key information from the section to create:
1. A concise summary suitable for search result reranking
2. Granular metadata tags and standard references
3. Importance scoring for prioritization
</objective>

<style>
Technical, precise, structured, and professional
</style>

<response_format>
YOU MUST use the 'provide_section_analysis' tool to structure your response.
DO NOT provide a plain text response.
</response_format>"""

    user_prompt_parts = []
    
    # Add chapter context if available
    if chapter_summary or chapter_tags:
        user_prompt_parts.append("<chapter_context>")
        if chapter_summary:
            user_prompt_parts.append(f"<chapter_summary>\n{chapter_summary}\n</chapter_summary>")
        if chapter_tags:
            user_prompt_parts.append(f"<chapter_tags>\n{json.dumps(chapter_tags)}\n</chapter_tags>")
        user_prompt_parts.append("</chapter_context>")
    
    # Add previous section summaries for context
    if previous_summaries:
        user_prompt_parts.append("<recent_sections>")
        for i, summary in enumerate(previous_summaries):
            user_prompt_parts.append(f"<previous_section_{i+1}>\n{summary}\n</previous_section_{i+1}>")
        user_prompt_parts.append("</recent_sections>")
    
    # Add current section
    user_prompt_parts.append(f"<current_section>\n{section_text}\n</current_section>")
    
    # Add instructions
    user_prompt_parts.append("<instructions>")
    user_prompt_parts.append("""Analyze the provided section and generate:

1. **section_summary**: A concise 1-3 sentence summary capturing the core topic/purpose
2. **section_tags**: 3-15 specific, granular tags relevant to the content
3. **section_standard**: Primary accounting standard framework (IFRS, US GAAP, or N/A)
4. **section_standard_codes**: Specific standard codes mentioned (e.g., 'IFRS 16', 'ASC 842')
5. **section_importance_score**: Float between 0.0-1.0 indicating importance (0.5 = average)
6. **section_references**: Explicit references to other sections/chapters/standards

Focus on extracting information that will be useful for search and retrieval.""")
    user_prompt_parts.append("</instructions>")
    
    user_prompt_parts.append("""<critical_requirement>
YOU MUST USE THE 'provide_section_analysis' TOOL TO PROVIDE YOUR RESPONSE.
The tool must include all required fields with appropriate content.
</critical_requirement>""")
    
    user_prompt = "\n".join(user_prompt_parts)
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    return messages

def process_section_robust(section_text, client, model_name, chapter_details=None, 
                          previous_summaries=None):
    """Process a section with robust tool calling and validation."""
    messages = build_section_analysis_prompt(
        section_text,
        chapter_summary=chapter_details.get('chapter_summary') if chapter_details else None,
        chapter_tags=chapter_details.get('chapter_tags') if chapter_details else None,
        previous_summaries=previous_summaries
    )
    
    # Call API with strict tool enforcement
    result, usage_info = call_gpt_with_tool_enforcement(
        client=client,
        model=model_name,
        messages=messages,
        max_tokens=MAX_COMPLETION_TOKENS,
        temperature=TEMPERATURE,
        tool_schema=SECTION_TOOL_SCHEMA
    )
    
    if result:
        # Log usage if verbose
        if usage_info and VERBOSE_LOGGING:
            prompt_tokens = usage_info.prompt_tokens
            completion_tokens = usage_info.completion_tokens
            total_cost = (prompt_tokens / 1000) * PROMPT_TOKEN_COST + (completion_tokens / 1000) * COMPLETION_TOKEN_COST
            logging.debug(f"API Usage - Prompt: {prompt_tokens}, Completion: {completion_tokens}, Cost: ${total_cost:.4f}")
        
        return result
    
    return None

# ==============================================================================
# Section Processing
# ==============================================================================

def extract_page_numbers_from_section(section_content: str) -> Tuple[Optional[int], Optional[int]]:
    """Extract page numbers from section content using Azure tags."""
    page_tags = list(PAGE_NUMBER_TAG_PATTERN.finditer(section_content))
    
    if page_tags:
        page_numbers = [int(match.group(1)) for match in page_tags]
        return min(page_numbers), max(page_numbers)
    
    return None, None

def process_chapter_sections(chapter_data: Dict, client: Optional[OpenAI]) -> List[Dict]:
    """Process all sections in a chapter."""
    chapter_number = chapter_data.get("chapter_number", "UNKNOWN")
    chapter_name = chapter_data.get("chapter_name", "Unknown Chapter")
    
    log_progress("")
    log_progress(f"📚 Processing Chapter {chapter_number}: {chapter_name}")
    
    # 1. Split into initial sections
    initial_sections = split_chapter_into_sections(chapter_data)
    log_progress(f"  📄 Found {len(initial_sections)} initial sections")
    
    # 2. Clean and calculate tokens
    cleaned_sections = []
    for section_raw in initial_sections:
        cleaned_content = clean_azure_tags(section_raw["raw_section_slice"])
        if cleaned_content.strip():
            section_clean = section_raw.copy()
            section_clean["content"] = cleaned_content
            section_clean["section_token_count"] = count_tokens(cleaned_content)
            section_clean.pop("raw_section_slice", None)
            cleaned_sections.append(section_clean)
    
    log_progress(f"  ✂️  {len(cleaned_sections)} sections after cleaning")
    
    # 3. Merge small sections
    merged_sections = merge_small_sections(
        cleaned_sections, MIN_TOKENS, MAX_TOKENS, ULTRA_SMALL_THRESHOLD
    )
    log_progress(f"  🔗 {len(merged_sections)} sections after merging")
    
    # 4. Renumber sections sequentially
    for i, section in enumerate(merged_sections):
        section["original_section_number"] = section["section_number"]
        section["section_number"] = i + 1
    
    # 5. Extract page numbers for each section
    chapter_start_page = chapter_data.get("chapter_page_start")
    chapter_end_page = chapter_data.get("chapter_page_end")
    
    for i, section in enumerate(merged_sections):
        # Try to extract from content
        start_page, end_page = extract_page_numbers_from_section(section.get("content", ""))
        
        # If not found, infer from position
        if start_page is None:
            if i == 0:
                start_page = chapter_start_page
            else:
                # Use previous section's end page
                for j in range(i-1, -1, -1):
                    if merged_sections[j].get("section_end_page") is not None:
                        start_page = merged_sections[j]["section_end_page"]
                        break
                if start_page is None:
                    start_page = chapter_start_page
        
        if end_page is None:
            if i == len(merged_sections) - 1:
                end_page = chapter_end_page
            else:
                # Look ahead for next section's start page
                for j in range(i+1, len(merged_sections)):
                    next_start, _ = extract_page_numbers_from_section(merged_sections[j].get("content", ""))
                    if next_start is not None:
                        end_page = next_start
                        break
                if end_page is None:
                    end_page = chapter_end_page
        
        # Ensure end >= start
        if start_page is not None and end_page is not None and end_page < start_page:
            end_page = start_page
        
        section["section_start_page"] = start_page
        section["section_end_page"] = end_page
    
    # 6. Process sections with GPT
    enriched_sections = []
    recent_summaries = []
    
    for section in tqdm(merged_sections, desc=f"  Ch{chapter_number} sections"):
        section_title = section.get('section_title', 'Unknown')
        content = section.get("content", "")
        
        # Get GPT enrichment if available
        gpt_details = None
        if content and client:
            context_summaries = recent_summaries[-MAX_RECENT_SUMMARIES_CONTEXT:]
            gpt_details = process_section_robust(
                content, client, MODEL_NAME_CHAT,
                chapter_details=chapter_data,
                previous_summaries=context_summaries
            )
        
        # Assemble final section data
        final_section = section.copy()
        
        # Add GPT enrichment or defaults
        if gpt_details:
            final_section["section_summary"] = gpt_details.get("section_summary")
            final_section["section_tags"] = gpt_details.get("section_tags", [])
            final_section["section_standard"] = gpt_details.get("section_standard", "N/A")
            final_section["section_standard_codes"] = gpt_details.get("section_standard_codes", [])
            final_section["section_importance_score"] = gpt_details.get("section_importance_score", 0.5)
            final_section["section_references"] = gpt_details.get("section_references", [])
            
            # Update context
            if gpt_details.get("section_summary"):
                recent_summaries.append(gpt_details["section_summary"])
        else:
            final_section["section_summary"] = None
            final_section["section_tags"] = []
            final_section["section_standard"] = "N/A"
            final_section["section_standard_codes"] = []
            final_section["section_importance_score"] = 0.5
            final_section["section_references"] = []
        
        # Add chapter metadata
        final_section["document_id"] = chapter_data.get("document_id")
        final_section["chapter_number"] = chapter_number
        final_section["chapter_name"] = chapter_name
        final_section["chapter_tags"] = chapter_data.get("chapter_tags", [])
        final_section["chapter_summary"] = chapter_data.get("chapter_summary")
        final_section["chapter_token_count"] = chapter_data.get("chapter_token_count")
        
        # Rename content field
        if "content" in final_section:
            final_section["cleaned_section_content"] = final_section.pop("content")
        
        # Add hierarchy string
        if "section_hierarchy" not in final_section:
            final_section["section_hierarchy"] = generate_hierarchy_string(final_section)
        
        enriched_sections.append(final_section)
    
    log_progress(f"  ✅ Processed {len(enriched_sections)} sections")
    
    return enriched_sections

def cleanup_logging_handlers():
    """Safely cleanup logging handlers."""
    progress_logger = logging.getLogger('progress')
    handlers_to_remove = list(progress_logger.handlers)
    for handler in handlers_to_remove:
        try:
            handler.flush()
            handler.close()
        except:
            pass
        try:
            progress_logger.removeHandler(handler)
        except:
            pass
    
    root_handlers_to_remove = list(logging.root.handlers)
    for handler in root_handlers_to_remove:
        try:
            handler.flush()
            handler.close()
        except:
            pass
        try:
            logging.root.removeHandler(handler)
        except:
            pass
    
    progress_logger.handlers.clear()
    logging.root.handlers.clear()

# ==============================================================================
# Main Processing Function
# ==============================================================================

def run_stage2():
    """Main function to execute Stage 2 processing."""
    if not validate_configuration():
        return
    
    temp_log_path = setup_logging()
    
    log_progress("=" * 70)
    log_progress("🚀 Starting Stage 2: Section Processing (Robust Version)")
    log_progress("=" * 70)
    
    _setup_ssl_from_nas()
    
    share_name = NAS_PARAMS["share"]
    output_path_relative = os.path.join(NAS_OUTPUT_PATH, OUTPUT_FILENAME).replace('\\', '/')
    
    # Load Stage 1 output
    stage1_path_relative = os.path.join(NAS_INPUT_PATH, INPUT_FILENAME).replace('\\', '/')
    log_progress("📥 Loading Stage 1 output from NAS...")
    
    stage1_json_bytes = read_from_nas(share_name, stage1_path_relative)
    if not stage1_json_bytes:
        log_progress(f"❌ Failed to read Stage 1 output")
        return
    
    try:
        page_records = json.loads(stage1_json_bytes.decode('utf-8'))
        if not isinstance(page_records, list):
            log_progress("❌ Stage 1 output is not a list")
            return
        log_progress(f"✅ Loaded {len(page_records)} page records")
    except json.JSONDecodeError as e:
        log_progress(f"❌ Error decoding Stage 1 JSON: {e}")
        return
    
    # Initialize OpenAI client
    client = None
    if OpenAI:
        client = get_openai_client()
        if client:
            log_progress("✅ OpenAI client initialized")
        else:
            log_progress("⚠️ Failed to initialize OpenAI client")
    else:
        log_progress("⚠️ OpenAI library not installed")
    
    # Group pages by chapter
    chapters = group_pages_by_chapter(page_records)
    unassigned_pages = [r for r in page_records if r.get('chapter_number') is None]
    
    log_progress(f"📊 Found {len(chapters)} chapters and {len(unassigned_pages)} unassigned pages")
    log_progress("-" * 70)
    
    # Process each chapter
    all_sections = []
    
    for chapter_num in sorted(chapters.keys()):
        pages = chapters[chapter_num]
        
        # Consolidate pages into chapter data
        chapter_data = consolidate_chapter_data(chapter_num, pages)
        if not chapter_data:
            continue
        
        # Process sections in this chapter
        chapter_sections = process_chapter_sections(chapter_data, client)
        all_sections.extend(chapter_sections)
    
    # Sort sections by chapter and section number
    all_sections.sort(key=lambda x: (
        x.get('chapter_number', float('inf')),
        x.get('section_number', float('inf'))
    ))
    
    # Save output
    log_progress("-" * 70)
    log_progress(f"💾 Saving {len(all_sections)} section records...")
    
    try:
        output_json = json.dumps(all_sections, indent=2, ensure_ascii=False)
        output_bytes = output_json.encode('utf-8')
        
        if write_to_nas(share_name, output_path_relative, output_bytes):
            log_progress(f"✅ Successfully saved output")
        else:
            log_progress("❌ Failed to write output")
    except Exception as e:
        log_progress(f"❌ Error saving output: {e}")
    
    # Upload log file
    try:
        log_file_name = f"stage2_robust_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        log_path_relative = os.path.join(NAS_LOG_PATH, log_file_name).replace('\\', '/')
        
        cleanup_logging_handlers()
        
        with open(temp_log_path, 'rb') as f:
            log_content = f.read()
        
        if write_to_nas(share_name, log_path_relative, log_content):
            print(f"📝 Log file uploaded: {share_name}/{log_path_relative}")
        else:
            print(f"⚠️ Failed to upload log file")
        
        os.remove(temp_log_path)
    except Exception as e:
        print(f"⚠️ Error handling log file: {e}")
    
    # Final summary
    print("=" * 70)
    print("📊 Stage 2 Summary")
    print("-" * 70)
    print(f"  Input: {len(page_records)} page records")
    print(f"  Chapters processed: {len(chapters)}")
    print(f"  Output: {len(all_sections)} section records")
    if unassigned_pages:
        print(f"  Unassigned pages (skipped): {len(unassigned_pages)}")
    print(f"  Output file: {share_name}/{output_path_relative}")
    print("=" * 70)
    print("✅ Stage 2 Completed")

if __name__ == "__main__":
    run_stage2()