# -*- coding: utf-8 -*-
"""
Stage 3: Generate Document Summaries using Custom GPT

This script processes Markdown files generated by Stage 2.
For each Markdown file, it:
1.  Authenticates with a custom GPT service using OAuth 2.0 client credentials.
2.  Sends the Markdown content to the GPT model.
3.  Instructs the model (via tool calling) to generate two summaries:
    - document_description: A brief 1-2 sentence overview.
    - document_usage: A more detailed, formatted usage description for cataloging.
4.  Retrieves metadata associated with the original source file from Stage 1 outputs.
5.  Combines the generated summaries and metadata into a catalog entry.
6.  Appends the entry to a JSON file (`3A_catalog_entries.json`) on the NAS,
    saving progress after each file to allow for resuming after interruptions.
"""

import os
import sys
import json
import time
import requests # For OAuth token request
import smbclient
import pandas as pd
import tempfile # Added for temporary certificate file
from datetime import datetime, timezone
from openai import OpenAI # Assuming standard openai library v1.x+

# ==============================================================================
# --- Configuration ---
# ==============================================================================

# --- NAS Configuration (Should match Stage 1/2 or be loaded) ---
# IMPORTANT: Replace placeholder values if not loaded from a shared config.
NAS_PARAMS = {
    "ip": "your_nas_ip",          # Replace with NAS IP address
    "share": "your_share_name",   # Replace with NAS share name
    "user": "your_nas_user",      # Replace with NAS username
    "password": "your_nas_password" # Replace with NAS password
}
# Base path on the NAS share where Stage 1/2 output files were stored.
NAS_OUTPUT_FOLDER_PATH = "path/to/your/output_folder" # From Stage 1/2

# --- Processing Configuration (Should match Stage 1/2) ---
# Define the specific document source processed in previous stages.
DOCUMENT_SOURCE = 'internal_esg' # From Stage 1/2
# Define the type for this batch of documents (used in catalog entry)
DOCUMENT_TYPE = 'POLICY_DOCUMENT' # Example: Replace with appropriate type

# --- OAuth Configuration ---
# IMPORTANT: Replace placeholder values with your actual OAuth details.
OAUTH_CONFIG = {
    "token_url": "YOUR_OAUTH_TOKEN_ENDPOINT_URL", # Replace
    "client_id": "YOUR_CLIENT_ID",               # Replace
    "client_secret": "YOUR_CLIENT_SECRET",       # Replace (Consider secure loading, e.g., env vars)
    "scope": "api://YourResource/.default"       # Replace with actual scope if needed, else remove/adjust
}

# --- Custom GPT Configuration ---
# IMPORTANT: Replace placeholder values with your actual GPT service details.
GPT_CONFIG = {
    "base_url": "YOUR_CUSTOM_GPT_API_BASE_URL", # Replace (e.g., "https://your-api.azurewebsites.net/v1")
    "model_name": "your-gpt-model-deployment-name", # Replace
    "api_version": "2024-02-01" # Example Azure API version, adjust if needed
}

# --- Tool Definition for GPT ---
# This defines the function/tool the GPT model should call to return summaries.
GPT_TOOL_DEFINITION = {
    "type": "function",
    "function": {
        "name": "record_document_summary",
        "description": "Records the extracted description and usage summary of a document.",
        "parameters": {
            "type": "object",
            "properties": {
                "document_description": {
                    "type": "string",
                    "description": "A very brief (1-2 sentence) description of the document's content and purpose."
                },
                "document_usage": {
                    "type": "string",
                    "description": "A concise, formatted summary explaining how the document can be used or what insights it provides, suitable for cataloging and LLM ingestion."
                }
            },
            "required": ["document_description", "document_usage"]
        }
    }
}

# --- Input/Output Filenames ---
STAGE1_METADATA_FILENAME = '1C_nas_files_to_process.json'
STAGE2_OUTPUT_SUBFOLDER = '2A_processed_files'
STAGE3_CATALOG_OUTPUT_FILENAME = '3A_catalog_entries.json' # Renamed for clarity
STAGE3_CONTENT_OUTPUT_FILENAME = '3B_content_entries.json' # New output file
CA_BUNDLE_FILENAME = 'rbc-ca-bundle.cer' # Added CA bundle filename

# ==============================================================================
# --- Helper Functions ---
# ==============================================================================

def initialize_smb_client():
    """Sets up smbclient credentials."""
    try:
        smbclient.ClientConfig(username=NAS_PARAMS["user"], password=NAS_PARAMS["password"])
        print("SMB client configured successfully.")
        return True
    except Exception as e:
        print(f"[ERROR] Failed to configure SMB client: {e}")
        return False

def create_nas_directory(smb_dir_path):
    """Creates a directory on the NAS if it doesn't exist."""
    try:
        if not smbclient.path.exists(smb_dir_path):
            print(f"   Creating NAS directory: {smb_dir_path}")
            smbclient.makedirs(smb_dir_path, exist_ok=True)
            print(f"   Successfully created directory.")
        # else: # Optional: reduce verbosity
            # print(f"   NAS directory already exists: {smb_dir_path}")
        return True
    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error creating/accessing directory '{smb_dir_path}': {e}")
        return False
    except Exception as e:
        print(f"   [ERROR] Unexpected error creating/accessing NAS directory '{smb_dir_path}': {e}")
        return False

def read_json_from_nas(smb_path):
    """Reads and parses JSON data from a file path on the NAS."""
    print(f"   Attempting to read JSON from NAS path: {smb_path}")
    try:
        if not smbclient.path.exists(smb_path):
            print(f"   JSON file not found at: {smb_path}. Returning empty list.")
            # Return empty list for results files (catalog or content)
            # Check against both possible output filenames
            if STAGE3_CATALOG_OUTPUT_FILENAME in smb_path or STAGE3_CONTENT_OUTPUT_FILENAME in smb_path:
                return [] # Assume results file
            # Handle metadata file case (though it should usually exist)
            elif STAGE1_METADATA_FILENAME in smb_path:
                 print(f"   [WARNING] Metadata file {STAGE1_METADATA_FILENAME} not found. Returning empty list.")
                 return [] # Return list as metadata is expected to be a list of dicts
            else:
                 # Fallback for unexpected files, maybe return None or raise error?
                 print(f"   [WARNING] Unrecognized file type for not found handling: {smb_path}. Returning empty list.")
                 return []

        with smbclient.open_file(smb_path, mode='r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"   Successfully read and parsed JSON from: {smb_path}")
        return data
    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error reading JSON from '{smb_path}': {e}")
        return None # Indicate failure
    except json.JSONDecodeError as e:
        print(f"   [ERROR] Failed to parse JSON from '{smb_path}': {e}")
        return None # Indicate failure
    except Exception as e:
        print(f"   [ERROR] Unexpected error reading JSON from NAS '{smb_path}': {e}")
        return None # Indicate failure

def write_json_to_nas(smb_path, data):
    """Writes Python data (list/dict) as JSON to a file path on the NAS."""
    print(f"   Attempting to write JSON to NAS path: {smb_path}")
    try:
        # Ensure the directory exists first
        dir_path = os.path.dirname(smb_path)
        if not create_nas_directory(dir_path):
             return False # Failed to create directory

        # Convert Python object to JSON string with specific formatting
        # Use default=str for potential datetime objects or other non-serializables
        json_string = json.dumps(data, indent=4, default=str)

        with smbclient.open_file(smb_path, mode='w', encoding='utf-8') as f:
            f.write(json_string)
        print(f"   Successfully wrote JSON to: {smb_path}")
        return True
    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error writing JSON to '{smb_path}': {e}")
        return False
    except TypeError as e:
        print(f"   [ERROR] Failed to serialize data to JSON: {e}")
        return False
    except Exception as e:
        print(f"   [ERROR] Unexpected error writing JSON to NAS '{smb_path}': {e}")
        return False

def read_text_from_nas(smb_path):
    """Reads text content from a file path on the NAS."""
    print(f"   Attempting to read text from NAS path: {smb_path}")
    try:
        if not smbclient.path.exists(smb_path):
            print(f"   [ERROR] Text file not found at: {smb_path}")
            return None

        with smbclient.open_file(smb_path, mode='r', encoding='utf-8') as f:
            content = f.read()
        # print(f"   Successfully read text from: {smb_path}") # Reduce verbosity
        return content
    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error reading text from '{smb_path}': {e}")
        return None
    except Exception as e:
        print(f"   [ERROR] Unexpected error reading text from NAS '{smb_path}': {e}")
        return None

def find_md_files(smb_start_path):
    """Recursively finds all .md files within a given NAS directory."""
    md_files_list = []
    print(f" -> Searching for .md files in: {smb_start_path}")
    try:
        if not smbclient.path.exists(smb_start_path):
             print(f"   [ERROR] Base search path does not exist: {smb_start_path}")
             return []

        for dirpath, dirnames, filenames in smbclient.walk(smb_start_path):
            for filename in filenames:
                if filename.lower().endswith('.md'):
                    full_smb_path = os.path.join(dirpath, filename).replace('\\', '/')
                    md_files_list.append(full_smb_path)
                    # print(f"      Found: {full_smb_path}") # Optional verbosity

        print(f" <- Found {len(md_files_list)} .md files.")
        return md_files_list

    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error walking directory '{smb_start_path}': {e}")
        return [] # Return empty list on error
    except Exception as e:
        print(f"   [ERROR] Unexpected error walking NAS directory '{smb_start_path}': {e}")
        return [] # Return empty list on error

def get_oauth_token():
    """Retrieves an OAuth access token using client credentials flow."""
    print("   Requesting OAuth access token...")
    token_url = OAUTH_CONFIG['token_url']
    payload = {
        'client_id': OAUTH_CONFIG['client_id'],
        'client_secret': OAUTH_CONFIG['client_secret'],
        'grant_type': 'client_credentials'
    }
    # Add scope if defined and not empty
    if OAUTH_CONFIG.get('scope'):
        payload['scope'] = OAUTH_CONFIG['scope']

    try:
        # Note: requests library automatically uses REQUESTS_CA_BUNDLE if set
        response = requests.post(token_url, data=payload)
        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
        token_data = response.json()
        access_token = token_data.get('access_token')
        if not access_token:
            print("   [ERROR] 'access_token' not found in OAuth response.")
            return None
        print("   OAuth token obtained successfully.")
        return access_token
    except requests.exceptions.RequestException as e:
        print(f"   [ERROR] Failed to get OAuth token: {e}")
        # Log more details if possible, e.g., response content for debugging
        if hasattr(e, 'response') and e.response is not None:
             print(f"   Response status: {e.response.status_code}")
             print(f"   Response body: {e.response.text}")
        return None
    except Exception as e:
        print(f"   [ERROR] Unexpected error during OAuth token request: {e}")
        return None

def call_gpt_summarizer(api_client, markdown_content):
    """Calls the custom GPT model to generate summaries using tool calling."""
    print("   Calling GPT model for summarization...")
    try:
        messages = [
            {"role": "system", "content": "You are an expert assistant skilled at summarizing technical documents. Analyze the provided document content and use the 'record_document_summary' tool to provide a brief description and a detailed usage summary."},
            {"role": "user", "content": f"Please summarize the following document content:\n\n---\n\n{markdown_content}\n\n---"}
        ]

        # Note: The openai client might need explicit SSL context/CA bundle handling
        # depending on its version and underlying HTTP library (e.g., httpx).
        # If REQUESTS_CA_BUNDLE/SSL_CERT_FILE doesn't work automatically for openai,
        # you might need to configure the client like this:
        # import httpx
        # cert_path = os.environ.get('REQUESTS_CA_BUNDLE') or os.environ.get('SSL_CERT_FILE')
        # ssl_context = httpx.create_ssl_context(verify=cert_path)
        # client = OpenAI(..., http_client=httpx.Client(verify=ssl_context))
        # For now, assume env vars might be sufficient or openai uses requests internally.

        response = api_client.chat.completions.create(
            model=GPT_CONFIG['model_name'],
            messages=messages,
            tools=[GPT_TOOL_DEFINITION],
            tool_choice={"type": "function", "function": {"name": GPT_TOOL_DEFINITION['function']['name']}} # Force tool use
            # Add other parameters like temperature, max_tokens if needed
        )

        # --- Process Response ---
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        if tool_calls:
            # Expecting only one tool call in this setup
            tool_call = tool_calls[0]
            if tool_call.function.name == GPT_TOOL_DEFINITION['function']['name']:
                try:
                    arguments = json.loads(tool_call.function.arguments)
                    description = arguments.get('document_description')
                    usage = arguments.get('document_usage')

                    if description and usage:
                        print("   GPT successfully returned summaries via tool call.")
                        return description, usage
                    else:
                        print(f"   [ERROR] GPT tool call arguments missing 'document_description' or 'document_usage'. Arguments: {arguments}")
                        return None, None
                except json.JSONDecodeError as e:
                    print(f"   [ERROR] Failed to parse GPT tool call arguments JSON: {e}. Arguments: {tool_call.function.arguments}")
                    return None, None
                except Exception as e:
                    print(f"   [ERROR] Unexpected error processing GPT tool call arguments: {e}")
                    return None, None
            else:
                print(f"   [ERROR] GPT called unexpected tool: {tool_call.function.name}")
                return None, None
        else:
            # Handle case where the model didn't use the tool (e.g., replied directly)
            print(f"   [ERROR] GPT did not use the required tool. Response content: {response_message.content}")
            # Potentially try to extract from content if fallback is desired, but tool use was requested.
            return None, None

    except Exception as e:
        # Catch potential API errors (authentication, connection, rate limits, etc.)
        print(f"   [ERROR] Failed to call GPT model: {type(e).__name__} - {e}")
        # Consider more specific error handling based on openai library exceptions if needed
        return None, None

# ==============================================================================
# --- Main Execution Logic ---
# ==============================================================================

if __name__ == "__main__":
    temp_cert_file_path = None # Store path instead of file object
    original_requests_ca_bundle = os.environ.get('REQUESTS_CA_BUNDLE') # Store original env var value
    original_ssl_cert_file = os.environ.get('SSL_CERT_FILE') # Store original env var value

    print("\n" + "="*60)
    print(f"--- Running Stage 3: Generate Document Summaries ---")
    print(f"--- Document Source: {DOCUMENT_SOURCE} ---")
    print(f"--- Document Type: {DOCUMENT_TYPE} ---")
    print("="*60 + "\n")

    # --- Initialize SMB Client ---
    print("[1] Initializing SMB Client...")
    if not initialize_smb_client():
        sys.exit(1) # Exit if SMB client fails
    print("-" * 60)

    # --- Define Paths ---
    print("[2] Defining NAS Paths...")
    source_base_dir_relative = os.path.join(NAS_OUTPUT_FOLDER_PATH, DOCUMENT_SOURCE).replace('\\', '/')
    source_base_dir_smb = f"//{NAS_PARAMS['ip']}/{NAS_PARAMS['share']}/{source_base_dir_relative}"
    stage1_metadata_smb_path = os.path.join(source_base_dir_smb, STAGE1_METADATA_FILENAME).replace('\\', '/')
    stage2_md_dir_smb_path = os.path.join(source_base_dir_smb, STAGE2_OUTPUT_SUBFOLDER).replace('\\', '/')
    stage3_catalog_output_smb_path = os.path.join(source_base_dir_smb, STAGE3_CATALOG_OUTPUT_FILENAME).replace('\\', '/') # Updated path
    stage3_content_output_smb_path = os.path.join(source_base_dir_smb, STAGE3_CONTENT_OUTPUT_FILENAME).replace('\\', '/') # New path
    ca_bundle_smb_path = os.path.join(f"//{NAS_PARAMS['ip']}/{NAS_PARAMS['share']}/{NAS_OUTPUT_FOLDER_PATH}", CA_BUNDLE_FILENAME).replace('\\', '/')

    print(f"   Source Base Dir (SMB): {source_base_dir_smb}")
    print(f"   Stage 1 Metadata File (SMB): {stage1_metadata_smb_path}")
    print(f"   Stage 2 MD Files Dir (SMB): {stage2_md_dir_smb_path}")
    print(f"   Stage 3 Catalog Output File (SMB): {stage3_catalog_output_smb_path}") # Updated print
    print(f"   Stage 3 Content Output File (SMB): {stage3_content_output_smb_path}") # New print
    print(f"   CA Bundle File (SMB): {ca_bundle_smb_path}")
    print("-" * 60)

    # --- Main Processing Block with Cleanup ---
    try:
        # --- Download and Set Custom CA Bundle ---
        print("[3] Setting up Custom CA Bundle...")
        try: # Inner try/except for CA bundle download/setup
            if smbclient.path.exists(ca_bundle_smb_path):
                # Create a temporary file to store the certificate
                with tempfile.NamedTemporaryFile(delete=False, suffix=".cer") as temp_cert_file:
                    print(f"   Downloading to temporary file: {temp_cert_file.name}")
                    with smbclient.open_file(ca_bundle_smb_path, mode='rb') as nas_f:
                        temp_cert_file.write(nas_f.read())
                    temp_cert_file_path = temp_cert_file.name # Store the path for cleanup

                # Set the environment variables
                if temp_cert_file_path: # Ensure path was obtained
                    os.environ['REQUESTS_CA_BUNDLE'] = temp_cert_file_path
                    os.environ['SSL_CERT_FILE'] = temp_cert_file_path
                    print(f"   Set REQUESTS_CA_BUNDLE environment variable to: {temp_cert_file_path}")
                    print(f"   Set SSL_CERT_FILE environment variable to: {temp_cert_file_path}")
            else:
                print(f"   [WARNING] CA Bundle file not found at {ca_bundle_smb_path}. Proceeding without custom CA bundle.")
        except smbclient.SambaClientError as e:
            print(f"   [ERROR] SMB Error during CA bundle handling '{ca_bundle_smb_path}': {e}. Proceeding without custom CA bundle.")
        except Exception as e:
            print(f"   [ERROR] Unexpected error during CA bundle handling '{ca_bundle_smb_path}': {e}. Proceeding without custom CA bundle.")
            # Cleanup potentially created temp file if error occurred after creation
            if temp_cert_file_path and os.path.exists(temp_cert_file_path):
                try:
                    os.remove(temp_cert_file_path)
                    print(f"   Cleaned up partially created temp CA file: {temp_cert_file_path}")
                    temp_cert_file_path = None
                except OSError: pass # Ignore cleanup error
        # --- Load Stage 1 Metadata ---
        print(f"[5] Loading Stage 1 Metadata from: {os.path.basename(stage1_metadata_smb_path)}...")
        stage1_metadata_list = read_json_from_nas(stage1_metadata_smb_path)
        if stage1_metadata_list is None:
            print("[CRITICAL ERROR] Failed to load Stage 1 metadata. Exiting.")
            sys.exit(1)
        if not isinstance(stage1_metadata_list, list):
            print(f"[CRITICAL ERROR] Stage 1 metadata is not a list. Found type: {type(stage1_metadata_list)}. Exiting.")
            sys.exit(1)

        metadata_lookup = {}
        for item in stage1_metadata_list:
            if 'file_name' in item:
                base_name = os.path.splitext(item['file_name'])[0]
                metadata_lookup[base_name] = item
            else:
                print(f"   [WARNING] Skipping metadata item due to missing 'file_name': {item}")
        print(f"   Loaded metadata for {len(metadata_lookup)} files.")
        print("-" * 60)

        # --- Load Existing Stage 3 Results (Checkpointing) ---
        print(f"[6] Loading existing Stage 3 results...")
        # Load Catalog Entries
        print(f"   Loading catalog entries from: {os.path.basename(stage3_catalog_output_smb_path)}...")
        catalog_entries = read_json_from_nas(stage3_catalog_output_smb_path)
        if catalog_entries is None:
            print("[CRITICAL ERROR] Failed to load or initialize existing catalog entries. Exiting.")
            sys.exit(1)
        if not isinstance(catalog_entries, list):
            print(f"[CRITICAL ERROR] Existing catalog entries file is not a list. Found type: {type(catalog_entries)}. Exiting.")
            sys.exit(1)
        print(f"   Found {len(catalog_entries)} existing catalog entries.")

        # Load Content Entries
        print(f"   Loading content entries from: {os.path.basename(stage3_content_output_smb_path)}...")
        content_entries = read_json_from_nas(stage3_content_output_smb_path)
        if content_entries is None:
            print("[CRITICAL ERROR] Failed to load or initialize existing content entries. Exiting.")
            sys.exit(1)
        if not isinstance(content_entries, list):
            print(f"[CRITICAL ERROR] Existing content entries file is not a list. Found type: {type(content_entries)}. Exiting.")
            sys.exit(1)
        print(f"   Found {len(content_entries)} existing content entries.")

        # Determine processed files based on catalog entries (assuming catalog is the primary indicator)
        processed_md_files = set(entry.get('processed_md_path') for entry in catalog_entries if 'processed_md_path' in entry)
        print(f"   Identified {len(processed_md_files)} already processed Markdown files (based on catalog).")
        print("-" * 60)

        # --- Find Markdown Files from Stage 2 ---
        print(f"[7] Searching for Stage 2 Markdown files in: {stage2_md_dir_smb_path}...")
        md_files_to_process = find_md_files(stage2_md_dir_smb_path)
        if not md_files_to_process:
            print("   No Markdown files found to process.")
            print("\n" + "="*60)
            print(f"--- Stage 3 Completed (No new Markdown files found) ---")
            print("="*60 + "\n")
            # No sys.exit(0) here, allow finally block to run
        else:
            print("-" * 60)

            # --- Process Each Markdown File ---
            print(f"[8] Processing {len(md_files_to_process)} Markdown files...")
            new_entries_count = 0
            skipped_count = 0
            error_count = 0
            current_token = None
            token_expiry_time = time.time() # Initialize to ensure first token fetch

            for i, md_smb_path in enumerate(md_files_to_process):
                start_time = time.time()
                print(f"\n--- Processing file {i+1}/{len(md_files_to_process)} ---")
                print(f"   MD File Path (SMB): {md_smb_path}")

                if md_smb_path in processed_md_files:
                    print("   File already processed (found in existing results). Skipping.")
                    skipped_count += 1
                    continue

                if time.time() >= token_expiry_time:
                    print("   OAuth token expired or not yet fetched. Requesting new token...")
                    current_token = get_oauth_token()
                    if not current_token:
                        print("   [ERROR] Failed to obtain OAuth token. Skipping file.")
                        error_count += 1
                        continue
                    token_expiry_time = time.time() + (50 * 60)

                try:
                    client = OpenAI(
                        base_url=GPT_CONFIG['base_url'],
                        api_key=current_token,
                    )
                except Exception as e:
                    print(f"   [ERROR] Failed to initialize OpenAI client: {e}. Skipping file.")
                    error_count += 1
                    continue

                markdown_content = read_text_from_nas(md_smb_path)
                if not markdown_content:
                    print(f"   [ERROR] Failed to read Markdown content from {md_smb_path}. Skipping file.")
                    error_count += 1
                    continue

                description, usage = call_gpt_summarizer(client, markdown_content)
                if not description or not usage:
                    print(f"   [ERROR] Failed to get summaries from GPT for {md_smb_path}. Skipping file.")
                    error_count += 1
                    continue # Skip if summarization fails

                # --- Prepare Data for Output ---
                md_filename = os.path.basename(md_smb_path)
                original_base_name = os.path.splitext(md_filename)[0]
                # Handle potential chunk suffixes added in Stage 2
                if '_chunk_' in original_base_name:
                     original_base_name = original_base_name.split('_chunk_')[0]

                original_metadata = metadata_lookup.get(original_base_name)
                if not original_metadata:
                    print(f"   [WARNING] Could not find original metadata for base name '{original_base_name}' derived from {md_filename}. Skipping file.")
                    error_count += 1
                    continue

                # Extract key fields (ensure they exist in metadata)
                doc_name = original_metadata.get('file_name', md_filename) # Fallback to md_filename if needed
                doc_base_name = os.path.splitext(doc_name)[0] # Get filename without extension
                doc_source = DOCUMENT_SOURCE # Use configured source
                doc_type = DOCUMENT_TYPE   # Use configured type

                # Create Catalog Entry
                catalog_entry = {
                    "document_source": doc_source,
                    "document_type": doc_type,
                    "document_name": doc_name,
                    "document_description": description,
                    "document_usage": usage,
                    "date_created": datetime.now(timezone.utc).isoformat(),
                    "date_last_modified": original_metadata.get('date_last_modified'),
                    "file_name": original_metadata.get('file_name'), # Redundant but matches original structure
                    "file_type": os.path.splitext(original_metadata.get('file_name', ''))[1],
                    "file_size": original_metadata.get('file_size'),
                    "file_path": original_metadata.get('file_path'),
                    "file_link": f"//{NAS_PARAMS['ip']}/{NAS_PARAMS['share']}/{original_metadata.get('file_path', '')}",
                    "processed_md_path": md_smb_path # For checkpointing
                }

                # Create Content Entry
                content_entry = {
                    "document_source": doc_source,
                    "document_type": doc_type,
                    "document_name": doc_name,
                    "section_id": 0, # As requested
                    "section_name": doc_base_name, # Use base name without extension
                    "content": markdown_content, # Use the full markdown content read from the file
                    "date_created": datetime.now(timezone.utc).isoformat() # Add creation date
                }

                # --- Append and Save Both Entries (Atomic-like operation for checkpointing) ---
                print(f"   Appending new catalog entry for: {doc_name}")
                catalog_entries.append(catalog_entry)
                print(f"   Appending new content entry for: {doc_name}")
                content_entries.append(content_entry)

                # Save Catalog Entries
                catalog_save_success = write_json_to_nas(stage3_catalog_output_smb_path, catalog_entries)
                # Save Content Entries
                content_save_success = write_json_to_nas(stage3_content_output_smb_path, content_entries)

                if catalog_save_success and content_save_success:
                    print(f"   Successfully saved updated catalog ({len(catalog_entries)}) and content ({len(content_entries)}) entries to NAS.")
                    new_entries_count += 1
                    processed_md_files.add(md_smb_path) # Mark as processed only if both saved
                else:
                    print(f"   [CRITICAL ERROR] Failed to save one or both output files to NAS after processing {md_filename}. Stopping.")
                    error_count += 1
                    # Rollback the appends for consistency before exiting
                    catalog_entries.pop()
                    content_entries.pop()
                    print(f"   Rolled back entries for {doc_name}.")
                    # Attempt to save the rolled-back state (optional, might fail again)
                    # write_json_to_nas(stage3_catalog_output_smb_path, catalog_entries)
                    # write_json_to_nas(stage3_content_output_smb_path, content_entries)
                    sys.exit(1) # Exit on critical save failure

                end_time = time.time()
                print(f"--- Finished file {i+1} (Success) ---")
                print(f"--- Time taken: {end_time - start_time:.2f} seconds ---")

            # --- Final Summary ---
            print("\n" + "="*60)
            print(f"--- Stage 3 Processing Summary ---")
            print(f"   Total Markdown files found: {len(md_files_to_process)}")
            print(f"   Files skipped (already processed): {skipped_count}")
            print(f"   New entry pairs (catalog/content) added: {new_entries_count}")
            print(f"   Errors encountered: {error_count}")
            print(f"   Total entries in '{STAGE3_CATALOG_OUTPUT_FILENAME}': {len(catalog_entries)}")
            print(f"   Total entries in '{STAGE3_CONTENT_OUTPUT_FILENAME}': {len(content_entries)}")
            print("="*60 + "\n")

            if error_count > 0:
                print(f"[WARNING] {error_count} files encountered errors during processing. Check logs above.")

        print(f"--- Stage 3 Completed ---")

    # --- Cleanup (Executes regardless of success/failure in the try block) ---
    finally:
        print("\n--- Cleaning up ---")
        # Clean up the temporary certificate file
        if temp_cert_file_path and os.path.exists(temp_cert_file_path):
            try:
                os.remove(temp_cert_file_path)
                print(f"   Removed temporary CA bundle file: {temp_cert_file_path}")
            except OSError as e:
                 print(f"   [WARNING] Failed to remove temporary CA bundle file {temp_cert_file_path}: {e}")

        # Restore original environment variables
        # Restore REQUESTS_CA_BUNDLE
        current_requests_bundle = os.environ.get('REQUESTS_CA_BUNDLE')
        if original_requests_ca_bundle is None:
            # If it didn't exist originally, remove it if we set it
            if current_requests_bundle == temp_cert_file_path:
                 print("   Unsetting REQUESTS_CA_BUNDLE environment variable.")
                 # Check if key exists before deleting
                 if 'REQUESTS_CA_BUNDLE' in os.environ:
                     del os.environ['REQUESTS_CA_BUNDLE']
        else:
            # If it existed originally, restore its value if it changed
            if current_requests_bundle != original_requests_ca_bundle:
                 print(f"   Restoring original REQUESTS_CA_BUNDLE environment variable.")
                 os.environ['REQUESTS_CA_BUNDLE'] = original_requests_ca_bundle

        # Restore SSL_CERT_FILE
        current_ssl_cert = os.environ.get('SSL_CERT_FILE')
        if original_ssl_cert_file is None:
            # If it didn't exist originally, remove it if we set it
            if current_ssl_cert == temp_cert_file_path:
                 print("   Unsetting SSL_CERT_FILE environment variable.")
                 # Check if key exists before deleting
                 if 'SSL_CERT_FILE' in os.environ:
                     del os.environ['SSL_CERT_FILE']
        else:
            # If it existed originally, restore its value if it changed
            if current_ssl_cert != original_ssl_cert_file:
                 print(f"   Restoring original SSL_CERT_FILE environment variable.")
                 os.environ['SSL_CERT_FILE'] = original_ssl_cert_file
