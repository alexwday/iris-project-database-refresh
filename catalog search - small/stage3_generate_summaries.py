# -*- coding: utf-8 -*-
"""
Stage 3: Generate Document Summaries using Custom GPT

This script processes Markdown files generated by Stage 2.
For each Markdown file, it:
1.  Authenticates with a custom GPT service using OAuth 2.0 client credentials.
2.  Sends the Markdown content to the GPT model.
3.  Instructs the model (via tool calling) to generate two summaries:
    - document_description: A brief 1-2 sentence overview.
    - document_usage: A more detailed, formatted usage description for cataloging.
4.  Retrieves metadata associated with the original source file from Stage 1 outputs.
5.  Combines the generated summaries and metadata into a catalog entry.
6.  Appends the entry to a JSON file (`3A_catalog_entries.json`) on the NAS,
    saving progress after each file to allow for resuming after interruptions.
"""

import os
import sys
import json
import time
import re  # For regex page splitting
import requests # For OAuth token request
import smbclient
import pandas as pd
import tempfile # Added for temporary certificate file
from datetime import datetime, timezone
from openai import OpenAI # Assuming standard openai library v1.x+

# ==============================================================================
# --- Configuration ---
# ==============================================================================

# --- NAS Configuration (Should match Stage 1/2 or be loaded) ---
# Network attached storage connection parameters
NAS_PARAMS = {
    "ip": "your_nas_ip",
    "share": "your_share_name",
    "user": "your_nas_user",
    "password": "your_nas_password"
}
# Base path on the NAS share where Stage 1/2 output files were stored
NAS_OUTPUT_FOLDER_PATH = "path/to/your/output_folder"

# --- Processing Configuration (Should match Stage 1/2) ---
# Define the specific document source processed in previous stages.
DOCUMENT_SOURCE = 'internal_esg' # From Stage 1/2
# Define the type for this batch of documents (used in catalog entry)
DOCUMENT_TYPE = 'POLICY_DOCUMENT' # Example: Replace with appropriate type

# --- OAuth Configuration ---
# OAuth authentication parameters
OAUTH_CONFIG = {
    "token_url": "YOUR_OAUTH_TOKEN_ENDPOINT_URL",
    "client_id": "YOUR_CLIENT_ID",
    "client_secret": "YOUR_CLIENT_SECRET",
    "scope": "api://YourResource/.default"
}

# --- Custom GPT Configuration ---
# GPT API configuration parameters
GPT_CONFIG = {
    "base_url": "YOUR_CUSTOM_GPT_API_BASE_URL",
    "model_name": "your-gpt-model-deployment-name",
    "api_version": "2024-02-01"
}

# --- Embedding Configuration ---
# Embedding model configuration
EMBEDDING_CONFIG = {
    "model_name": "text-embedding-3-large",
    "dimensions": 2000
}

# --- New System Prompt Template ---
# Using CO-STAR framework and XML tags, embedded directly
SYSTEM_PROMPT_TEMPLATE = """<CONTEXT>
<PROJECT_CONTEXT>
This project processes diverse documents (extracted from sources like '{document_source}') to create structured catalog entries for a database. These entries contain 'usage' and 'description' fields intended for an agentic RAG (Retrieval-Augmented Generation) system. The 'usage' field allows the AI agent to assess document relevance for retrieval, while the 'description' field provides a concise summary for human users browsing the catalog.
</PROJECT_CONTEXT>
</CONTEXT>

You are an expert technical writer specializing in analyzing documents and generating structured summaries optimized for both AI agent retrieval and human understanding.

<OBJECTIVE>
Your goal is to generate two distinct fields based *only* on the provided <DOCUMENT_CONTENT>:
1.  `usage`: A comprehensive, structured summary intended primarily for an AI agent. It must detail the document's core purpose, key topics, main arguments, important entities (people, places, organizations, concepts), relationships between entities, potential applications or use cases discussed, and any specific terminology, standards, or identifiers mentioned. This detail is crucial for enabling an AI agent to accurately assess the document's relevance to a query based *only* on this `usage` field in the catalog. The level of detail should align with the specified `detail_level`.
2.  `description`: A concise (1-2 sentence) human-readable summary suitable for displaying to end-users browsing the catalog. This should capture the absolute essence of the document.
</OBJECTIVE>

<STYLE>
Analytical, factual, objective, structured, and informative. Use clear and precise language. For the 'usage' field, consider using bullet points or structured text where appropriate to enhance readability for the AI agent, especially at higher detail levels.
</STYLE>

<TONE>
Neutral and professional.
</TONE>

<AUDIENCE>
- Primary: An AI retrieval agent (consuming the `usage` field).
- Secondary: End-users browsing a document catalog (reading the `description` field).
</AUDIENCE>

<TASK>
Analyze the provided document content and generate the `usage` and `description` fields according to the specifications.

<DOCUMENT_CONTENT>
{markdown_content}
</DOCUMENT_CONTENT>

<INSTRUCTIONS>
1.  Carefully read and analyze the entire <DOCUMENT_CONTENT>.
2.  Generate the `usage` string according to the <OBJECTIVE>, focusing on extracting information that aids agentic retrieval. Adapt the length and detail level based on the provided `detail_level`: '{detail_level}'.
    - 'concise': Provide a brief overview of key topics and purpose.
    - 'standard': Offer a balanced summary of topics, entities, and use cases.
    - 'detailed': Require an exhaustive analysis covering all aspects mentioned in the <OBJECTIVE> for `usage`.
3.  Generate the `description` string as a concise 1-2 sentence summary for humans, capturing the document's core essence. This field's length should *not* change based on `detail_level`.
4.  **CRITICAL:** Base both fields *exclusively* on information present within the <DOCUMENT_CONTENT>. Do not infer, add external knowledge, or hallucinate information not explicitly stated in the text.
5.  Format your response strictly as specified in <RESPONSE_FORMAT>. Do not include any preamble, conversational text, or explanations outside the required JSON structure.
</INSTRUCTIONS>
</TASK>

<RESPONSE_FORMAT>
You MUST call the `generate_catalog_fields` tool. Provide the generated `usage` and `description` strings as arguments within a JSON object.

Example JSON for the tool call arguments:
{{
  "usage": "Comprehensive, structured summary based on the document content and detail level...",
  "description": "Concise 1-2 sentence summary."
}}
</RESPONSE_FORMAT>
"""

# --- Tool Definition for GPT (Updated) ---
# Renamed tool and updated descriptions/keys
GPT_TOOL_DEFINITION = {
    "type": "function",
    "function": {
        "name": "generate_catalog_fields", # Renamed tool
        "description": "Generates the detailed 'usage' summary (for AI retrieval) and the concise 'description' summary (for humans) based on document content.",
        "parameters": {
            "type": "object",
            "properties": {
                "usage": { # Renamed key
                    "type": "string",
                    "description": "A comprehensive, structured summary detailing the document's purpose, topics, entities, relationships, and use cases, optimized for AI agent retrieval assessment. Detail level varies."
                },
                "description": { # Renamed key
                    "type": "string",
                    "description": "A very concise (1-2 sentence) human-readable summary of the document's essence for catalog display."
                }
            },
            "required": ["usage", "description"] # Updated required keys
        }
    }
}

# --- Input/Output Filenames ---
STAGE1_METADATA_FILENAME = '1C_nas_files_to_process.json'
STAGE2_OUTPUT_SUBFOLDER = '2A_processed_files'
STAGE3_CATALOG_OUTPUT_FILENAME = '3A_catalog_entries.json' # Renamed for clarity
STAGE3_CONTENT_OUTPUT_FILENAME = '3B_content_entries.json' # New output file
STAGE3_ANONYMIZATION_REPORT_FILENAME = '3C_anonymization_report.json' # New report file (standard JSON)
CA_BUNDLE_FILENAME = 'rbc-ca-bundle.cer' # Added CA bundle filename

# ==============================================================================
# --- Helper Functions ---
# ==============================================================================

def initialize_smb_client():
    """Sets up smbclient credentials."""
    try:
        smbclient.ClientConfig(username=NAS_PARAMS["user"], password=NAS_PARAMS["password"])
        print("SMB client configured successfully.")
        return True
    except Exception as e:
        print(f"[ERROR] Failed to configure SMB client: {e}")
        return False

def create_nas_directory(smb_dir_path):
    """Creates a directory on the NAS if it doesn't exist."""
    try:
        if not smbclient.path.exists(smb_dir_path):
            print(f"   Creating NAS directory: {smb_dir_path}")
            smbclient.makedirs(smb_dir_path, exist_ok=True)
            print(f"   Successfully created directory.")
        # else: # Optional: reduce verbosity
            # print(f"   NAS directory already exists: {smb_dir_path}")
        return True
    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error creating/accessing directory '{smb_dir_path}': {e}")
        return False
    except Exception as e:
        print(f"   [ERROR] Unexpected error creating/accessing NAS directory '{smb_dir_path}': {e}")
        return False

def read_json_from_nas(smb_path):
    """Reads and parses JSON data from a file path on the NAS."""
    print(f"   Attempting to read JSON from NAS path: {smb_path}")
    try:
        if not smbclient.path.exists(smb_path):
            print(f"   JSON file not found at: {smb_path}. Returning empty list.")
            # Return empty list for results files (catalog or content)
            # Check against both possible output filenames
            if STAGE3_CATALOG_OUTPUT_FILENAME in smb_path or STAGE3_CONTENT_OUTPUT_FILENAME in smb_path:
                return [] # Assume results file
            # Handle metadata file case (though it should usually exist)
            elif STAGE1_METADATA_FILENAME in smb_path:
                 print(f"   [WARNING] Metadata file {STAGE1_METADATA_FILENAME} not found. Returning empty list.")
                 return [] # Return list as metadata is expected to be a list of dicts
            else:
                 # Fallback for unexpected files, maybe return None or raise error?
                 print(f"   [WARNING] Unrecognized file type for not found handling: {smb_path}. Returning empty list.")
                 return []

        with smbclient.open_file(smb_path, mode='r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"   Successfully read and parsed JSON from: {smb_path}")
        return data
    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error reading JSON from '{smb_path}': {e}")
        return None # Indicate failure
    except json.JSONDecodeError as e:
        print(f"   [ERROR] Failed to parse JSON from '{smb_path}': {e}")
        return None # Indicate failure
    except Exception as e:
        print(f"   [ERROR] Unexpected error reading JSON from NAS '{smb_path}': {e}")
        return None # Indicate failure

def write_json_to_nas(smb_path, data):
    """Writes Python data (list/dict) as JSON to a file path on the NAS."""
    print(f"   Attempting to write JSON to NAS path: {smb_path}")
    try:
        # Ensure the directory exists first
        dir_path = os.path.dirname(smb_path)
        if not create_nas_directory(dir_path):
             return False # Failed to create directory

        # Convert Python object to JSON string with specific formatting
        # Use default=str for potential datetime objects or other non-serializables
        json_string = json.dumps(data, indent=4, default=str)

        with smbclient.open_file(smb_path, mode='w', encoding='utf-8') as f:
            f.write(json_string)
        print(f"   Successfully wrote JSON to: {smb_path}")
        return True
    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error writing JSON to '{smb_path}': {e}")
        return False
    except TypeError as e:
        print(f"   [ERROR] Failed to serialize data to JSON: {e}")
        return False
    except Exception as e:
        print(f"   [ERROR] Unexpected error writing JSON to NAS '{smb_path}': {e}")
        return False

def read_text_from_nas(smb_path):
    """Reads text content from a file path on the NAS."""
    print(f"   Attempting to read text from NAS path: {smb_path}")
    try:
        if not smbclient.path.exists(smb_path):
            print(f"   [ERROR] Text file not found at: {smb_path}")
            return None

        with smbclient.open_file(smb_path, mode='r', encoding='utf-8') as f:
            content = f.read()
        # print(f"   Successfully read text from: {smb_path}") # Reduce verbosity
        return content
    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error reading text from '{smb_path}': {e}")
        return None
    except Exception as e:
        print(f"   [ERROR] Unexpected error reading text from NAS '{smb_path}': {e}")
        return None

def find_md_files(smb_start_path):
    """Recursively finds all .md files within a given NAS directory."""
    md_files_list = []
    print(f" -> Searching for .md files in: {smb_start_path}")
    try:
        if not smbclient.path.exists(smb_start_path):
             print(f"   [ERROR] Base search path does not exist: {smb_start_path}")
             return []

        for dirpath, dirnames, filenames in smbclient.walk(smb_start_path):
            for filename in filenames:
                if filename.lower().endswith('.md'):
                    full_smb_path = os.path.join(dirpath, filename).replace('\\', '/')
                    md_files_list.append(full_smb_path)
                    # print(f"      Found: {full_smb_path}") # Optional verbosity

        print(f" <- Found {len(md_files_list)} .md files.")
        return md_files_list

    except smbclient.SambaClientError as e:
        print(f"   [ERROR] SMB Error walking directory '{smb_start_path}': {e}")
        return [] # Return empty list on error
    except Exception as e:
        print(f"   [ERROR] Unexpected error walking NAS directory '{smb_start_path}': {e}")
        return [] # Return empty list on error

def get_oauth_token():
    """Retrieves an OAuth access token using client credentials flow."""
    print("   Requesting OAuth access token...")
    token_url = OAUTH_CONFIG['token_url']
    payload = {
        'client_id': OAUTH_CONFIG['client_id'],
        'client_secret': OAUTH_CONFIG['client_secret'],
        'grant_type': 'client_credentials'
    }
    # Add scope if defined and not empty
    if OAUTH_CONFIG.get('scope'):
        payload['scope'] = OAUTH_CONFIG['scope']

    try:
        # Note: requests library automatically uses REQUESTS_CA_BUNDLE if set
        response = requests.post(token_url, data=payload)
        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
        token_data = response.json()
        access_token = token_data.get('access_token')
        if not access_token:
            print("   [ERROR] 'access_token' not found in OAuth response.")
            return None
        print("   OAuth token obtained successfully.")
        return access_token
    except requests.exceptions.RequestException as e:
        print(f"   [ERROR] Failed to get OAuth token: {e}")
        # Log more details if possible, e.g., response content for debugging
        if hasattr(e, 'response') and e.response is not None:
             print(f"   Response status: {e.response.status_code}")
             print(f"   Response body: {e.response.text}")
        return None
    except Exception as e:
        print(f"   [ERROR] Unexpected error during OAuth token request: {e}")
        return None

def call_gpt_summarizer(api_client, markdown_content, detail_level='standard', document_source='unknown'):
    """
    Calls the custom GPT model to generate summaries using tool calling.

    Args:
        api_client: The initialized OpenAI client.
        markdown_content: The text content of the document to summarize.
        detail_level (str): The desired level of detail ('concise', 'standard', 'detailed').
        document_source (str): The source identifier for context in the prompt.

    Returns:
        tuple: (description, usage, analyzer_results_data) strings/dict, or (None, None, None) on failure.
    """
    print(f"   Calling GPT model for summarization (Detail Level: {detail_level})...")
    try:
        # Format the system prompt with dynamic content
        system_prompt = SYSTEM_PROMPT_TEMPLATE.format(
            markdown_content=markdown_content,
            detail_level=detail_level,
            document_source=document_source # Pass document source for context
        )

        messages = [
            # System prompt now contains all instructions, context, and content
            {"role": "system", "content": system_prompt}
            # No separate user message needed as content is embedded in system prompt
        ]

        # Note: The openai client might need explicit SSL context/CA bundle handling
        # depending on its version and underlying HTTP library (e.g., httpx).
        # If REQUESTS_CA_BUNDLE/SSL_CERT_FILE doesn't work automatically for openai,
        # you might need to configure the client like this:
        # import httpx
        # cert_path = os.environ.get('REQUESTS_CA_BUNDLE') or os.environ.get('SSL_CERT_FILE')
        # ssl_context = httpx.create_ssl_context(verify=cert_path)
        # client = OpenAI(..., http_client=httpx.Client(verify=ssl_context))
        # For now, assume env vars might be sufficient or openai uses requests internally.

        response = api_client.chat.completions.create(
            model=GPT_CONFIG['model_name'],
            messages=messages,
            tools=[GPT_TOOL_DEFINITION],
            tool_choice={"type": "function", "function": {"name": GPT_TOOL_DEFINITION['function']['name']}}, # Force tool use with updated name
            max_tokens=2048, # Add max_tokens
            temperature=0.2,  # Add temperature
            extra_query={"is_stateful_dlp": True} # Enable entity detection/anonymization
            # Consider adjusting max_tokens based on detail_level if necessary (2048 should be sufficient for now)
        )

        # --- Process analyzer_results if they exist ---
        analyzer_results_data = {
            'found_anonymized': False,
            'entities': []
        }
        if hasattr(response, 'analyzer_results') and isinstance(response.analyzer_results, list):
            print("   Processing analyzer_results...")
            for result in response.analyzer_results:
                if isinstance(result, dict) and result.get('anonymized', False): # Check type and value
                    analyzer_results_data['found_anonymized'] = True
                    analyzer_results_data['entities'].append({
                        'entity_type': result.get('entity_type'),
                        'raw_string': result.get('raw_string')
                    })
            print(f"   Anonymized Entities Found: {analyzer_results_data['found_anonymized']}")
        else:
            print("   No analyzer_results found or not in expected format.")
        # --- End of analyzer_results processing ---

        # --- Process Response ---
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        if tool_calls:
            # Expecting only one tool call in this setup
            tool_call = tool_calls[0]
            # Check against the updated tool name
            if tool_call.function.name == GPT_TOOL_DEFINITION['function']['name']:
                try:
                    arguments = json.loads(tool_call.function.arguments)
                    # Extract using updated keys
                    usage = arguments.get('usage')
                    description = arguments.get('description')

                    if description is not None and usage is not None: # Check for presence, even if empty string is valid
                        print("   GPT successfully returned summaries via tool call.")
                        # Return in the order: description, usage, and anonymization data
                        return description, usage, analyzer_results_data
                    else:
                        # Use updated keys in error message
                        print(f"   [ERROR] GPT tool call arguments missing 'usage' or 'description'. Arguments: {arguments}")
                        return None, None, None
                except json.JSONDecodeError as e:
                    print(f"   [ERROR] Failed to parse GPT tool call arguments JSON: {e}. Arguments: {tool_call.function.arguments}")
                    return None, None, None
                except Exception as e:
                    print(f"   [ERROR] Unexpected error processing GPT tool call arguments: {e}")
                    return None, None, None
            else:
                # Use updated tool name in error message
                print(f"   [ERROR] GPT called unexpected tool: {tool_call.function.name}")
                return None, None, None
        else:
            # Handle case where the model didn't use the tool (e.g., replied directly)
            print(f"   [ERROR] GPT did not use the required tool. Response content: {response_message.content}")
            # Potentially try to extract from content if fallback is desired, but tool use was requested.
            return None, None, None

    except Exception as e:
        # Catch potential API errors (authentication, connection, rate limits, etc.)
        print(f"   [ERROR] Failed to call GPT model: {type(e).__name__} - {e}")
        # Consider more specific error handling based on openai library exceptions if needed
        return None, None, None

def split_content_by_pages_using_json(markdown_content, json_data):
    """
    Splits markdown content by pages using Azure DI JSON response spans.
    
    The JSON response contains page information with spans (offset and length)
    that specify exactly which portion of the content belongs to each page.
    
    Args:
        markdown_content: The full markdown content
        json_data: The JSON response from Azure DI containing page spans
                  Can be a single dict or a list of dicts (for chunked files)
        
    Returns:
        list: List of tuples (page_number, content)
    """
    print("   Splitting content by pages using JSON spans...")
    
    pages = []
    
    # Handle both single JSON and list of JSONs (chunked files)
    json_list = json_data if isinstance(json_data, list) else [json_data]
    
    current_offset = 0  # Track offset for chunked files
    
    for json_chunk in json_list:
        # Check if JSON has pages information
        if json_chunk and 'pages' in json_chunk and json_chunk['pages']:
            chunk_content = json_chunk.get('content', '')
            
            print(f"   Processing chunk with {len(json_chunk['pages'])} pages")
            
            for page_info in json_chunk['pages']:
                page_num = page_info.get('page_number', len(pages) + 1)
                
                # Get spans for this page
                if 'spans' in page_info and page_info['spans']:
                    # Combine all spans for this page (usually just one)
                    page_content = ""
                    for span in page_info['spans']:
                        offset = span.get('offset', 0)
                        length = span.get('length', 0)
                        
                        if offset is not None and length is not None:
                            # For chunked files, use the chunk's content
                            if len(json_list) > 1:
                                span_content = chunk_content[offset:offset + length]
                            else:
                                # For single files, use the main markdown content
                                span_content = markdown_content[offset:offset + length]
                            page_content += span_content
                    
                    if page_content.strip():
                        pages.append((page_num, page_content.strip()))
                else:
                    print(f"   Warning: No spans found for page {page_num}")
    
    if pages:
        print(f"   Successfully split into {len(pages)} pages using spans")
        return pages
    
    # Fallback to regex-based splitting if JSON doesn't have page information
    print("   No page information in JSON, falling back to regex-based splitting")
    return split_content_by_pages_regex(markdown_content)


def split_content_by_pages_regex(markdown_content):
    """
    Fallback method: Splits markdown content by page breaks using regex.
    
    Azure Document Intelligence v4 API uses:
    - <!-- PageBreak --> as the page separator
    
    Returns:
        list: List of tuples (page_number, content)
    """
    print("   Using regex-based page splitting...")
    
    # Azure DI v4 API page break pattern
    page_break_pattern = r'<!-- ?PageBreak ?-->'
    
    # Check if page breaks exist
    if re.search(page_break_pattern, markdown_content, re.IGNORECASE):
        print(f"   Found page break markers")
        
        # Split content by page breaks
        parts = re.split(page_break_pattern, markdown_content, flags=re.IGNORECASE)
        
        pages = []
        for i, part in enumerate(parts):
            if part.strip():  # Skip empty parts
                page_num = i + 1  # Page numbers start at 1
                pages.append((page_num, part.strip()))
        
        print(f"   Split into {len(pages)} pages")
        return pages
    
    # If no page markers found, treat entire content as single page
    print("   No page markers found. Treating as single page.")
    return [(1, markdown_content)]

def generate_embeddings(texts, client, max_batch_size=50):
    """
    Generates embeddings for a list of texts using OpenAI API.
    
    Args:
        texts: List of strings to embed
        client: OpenAI client instance
        max_batch_size: Maximum number of texts to process in one API call
        
    Returns:
        list: List of embedding vectors (lists of floats)
    """
    print(f"   Generating embeddings for {len(texts)} texts...")
    
    all_embeddings = []
    
    # Process in batches
    for i in range(0, len(texts), max_batch_size):
        batch = texts[i:i + max_batch_size]
        print(f"   Processing batch {i//max_batch_size + 1}/{(len(texts) + max_batch_size - 1)//max_batch_size}")
        
        try:
            # Handle empty texts
            processed_batch = [text if text.strip() else " " for text in batch]
            
            response = client.embeddings.create(
                input=processed_batch,
                model=EMBEDDING_CONFIG["model_name"],
                dimensions=EMBEDDING_CONFIG["dimensions"]
            )
            
            # Extract embeddings from response
            batch_embeddings = [data.embedding for data in response.data]
            all_embeddings.extend(batch_embeddings)
            
        except Exception as e:
            print(f"   [ERROR] Failed to generate embeddings for batch: {e}")
            # Return None embeddings for failed batch
            all_embeddings.extend([None] * len(batch))
    
    print(f"   Successfully generated {len([e for e in all_embeddings if e is not None])} embeddings")
    return all_embeddings

# ==============================================================================
# --- Main Processing Function ---
# ==============================================================================

def main_processing_stage3(stage1_metadata_smb_path, stage2_md_dir_smb_path,
                           stage3_catalog_output_smb_path, stage3_content_output_smb_path,
                           stage3_anonymization_report_smb_path, # Added report path
                           ca_bundle_smb_path, refresh_flag_smb_path):
    """Handles the core logic for Stage 3: CA bundle, loading data, processing MD files."""
    print(f"--- Starting Main Processing for Stage 3 ---")
    temp_cert_file_path = None # Store path instead of file object
    original_requests_ca_bundle = os.environ.get('REQUESTS_CA_BUNDLE') # Store original env var value
    original_ssl_cert_file = os.environ.get('SSL_CERT_FILE') # Store original env var value
    is_full_refresh = False # Flag to track refresh mode

    try:
        # --- Download and Set Custom CA Bundle ---
        print("[4] Setting up Custom CA Bundle...") # Renumbered step
        try: # Inner try/except for CA bundle download/setup
            if smbclient.path.exists(ca_bundle_smb_path):
                # Create a temporary file to store the certificate
                with tempfile.NamedTemporaryFile(delete=False, suffix=".cer") as temp_cert_file:
                    print(f"   Downloading to temporary file: {temp_cert_file.name}")
                    with smbclient.open_file(ca_bundle_smb_path, mode='rb') as nas_f:
                        temp_cert_file.write(nas_f.read())
                    temp_cert_file_path = temp_cert_file.name # Store the path for cleanup

                # Set the environment variables
                if temp_cert_file_path: # Ensure path was obtained
                    os.environ['REQUESTS_CA_BUNDLE'] = temp_cert_file_path
                    os.environ['SSL_CERT_FILE'] = temp_cert_file_path
                    print(f"   Set REQUESTS_CA_BUNDLE environment variable to: {temp_cert_file_path}")
                    print(f"   Set SSL_CERT_FILE environment variable to: {temp_cert_file_path}")
            else:
                print(f"   [WARNING] CA Bundle file not found at {ca_bundle_smb_path}. Proceeding without custom CA bundle.")
        except smbclient.SambaClientError as e:
            print(f"   [ERROR] SMB Error during CA bundle handling '{ca_bundle_smb_path}': {e}. Proceeding without custom CA bundle.")
        except Exception as e:
            print(f"   [ERROR] Unexpected error during CA bundle handling '{ca_bundle_smb_path}': {e}. Proceeding without custom CA bundle.")
            # Cleanup potentially created temp file if error occurred after creation
            if temp_cert_file_path and os.path.exists(temp_cert_file_path):
                try:
                    os.remove(temp_cert_file_path)
                    print(f"   Cleaned up partially created temp CA file: {temp_cert_file_path}")
                    temp_cert_file_path = None
                except OSError: pass # Ignore cleanup error

        # --- Check for Full Refresh Flag ---
        print(f"[5] Checking for Full Refresh flag: {os.path.basename(refresh_flag_smb_path)}...") # Renumbered
        try:
            if smbclient.path.exists(refresh_flag_smb_path):
                print("   *** FULL REFRESH MODE DETECTED ***")
                is_full_refresh = True
                # Delete existing Stage 3 output files
                print("   Deleting existing Stage 3 output files (if they exist)...")
                for file_path in [stage3_catalog_output_smb_path, stage3_content_output_smb_path, stage3_anonymization_report_smb_path]: # Added report path
                    try:
                        if smbclient.path.exists(file_path):
                            smbclient.remove(file_path)
                            print(f"      Deleted: {os.path.basename(file_path)}")
                        else:
                            print(f"      File not found (already deleted or never existed): {os.path.basename(file_path)}")
                    except smbclient.SambaClientError as rm_err:
                        print(f"      [WARNING] SMB Error deleting file {os.path.basename(file_path)}: {rm_err}")
                    except Exception as rm_err:
                        print(f"      [WARNING] Unexpected error deleting file {os.path.basename(file_path)}: {rm_err}")
            else:
                print("   Full Refresh flag not found. Running in incremental mode.")
        except smbclient.SambaClientError as e:
            print(f"   [WARNING] SMB Error checking for refresh flag file '{refresh_flag_smb_path}': {e}. Assuming incremental mode.")
        except Exception as e:
            print(f"   [WARNING] Unexpected error checking for refresh flag file '{refresh_flag_smb_path}': {e}. Assuming incremental mode.")
        print("-" * 60)

        # --- Load Stage 1 Metadata ---
        print(f"[6] Loading Stage 1 Metadata from: {os.path.basename(stage1_metadata_smb_path)}...") # Renumbered
        stage1_metadata_list = read_json_from_nas(stage1_metadata_smb_path)
        if stage1_metadata_list is None:
            print("[CRITICAL ERROR] Failed to load Stage 1 metadata. Exiting.")
            sys.exit(1)
        if not isinstance(stage1_metadata_list, list):
            print(f"[CRITICAL ERROR] Stage 1 metadata is not a list. Found type: {type(stage1_metadata_list)}. Exiting.")
            sys.exit(1)

        metadata_lookup = {}
        for item in stage1_metadata_list:
            if 'file_name' in item:
                # Use original file name from metadata as the key base
                original_file_name = item['file_name']
                base_name = os.path.splitext(original_file_name)[0]
                metadata_lookup[base_name] = item
            else:
                print(f"   [WARNING] Skipping metadata item due to missing 'file_name': {item}")
        print(f"   Loaded metadata for {len(metadata_lookup)} files.")
        print("-" * 60)

        # --- Load Existing Stage 3 Results (Checkpointing - only if not full refresh) ---
        print(f"[7] Loading existing Stage 3 results (if incremental mode)...") # Renumbered
        catalog_entries = []
        content_entries = []
        report_entries = [] # Initialize list for report entries
        processed_md_files = set()

        if not is_full_refresh:
            # Load Catalog Entries
            print(f"   Loading catalog entries from: {os.path.basename(stage3_catalog_output_smb_path)}...")
            catalog_entries = read_json_from_nas(stage3_catalog_output_smb_path)
            if catalog_entries is None:
                print("[CRITICAL ERROR] Failed to load or initialize existing catalog entries. Exiting.")
                sys.exit(1)
            if not isinstance(catalog_entries, list):
                print(f"[CRITICAL ERROR] Existing catalog entries file is not a list. Found type: {type(catalog_entries)}. Exiting.")
                sys.exit(1)
            print(f"   Found {len(catalog_entries)} existing catalog entries.")

            # Load Content Entries
            print(f"   Loading content entries from: {os.path.basename(stage3_content_output_smb_path)}...")
            content_entries = read_json_from_nas(stage3_content_output_smb_path)
            if content_entries is None:
                print("[CRITICAL ERROR] Failed to load or initialize existing content entries. Exiting.")
                sys.exit(1)
            if not isinstance(content_entries, list):
                print(f"[CRITICAL ERROR] Existing content entries file is not a list. Found type: {type(content_entries)}. Exiting.")
                sys.exit(1)
            print(f"   Found {len(content_entries)} existing content entries.")

            # Load Anonymization Report Entries
            print(f"   Loading anonymization report entries from: {os.path.basename(stage3_anonymization_report_smb_path)}...")
            report_entries = read_json_from_nas(stage3_anonymization_report_smb_path)
            if report_entries is None:
                print("[CRITICAL ERROR] Failed to load or initialize existing anonymization report entries. Exiting.")
                sys.exit(1)
            if not isinstance(report_entries, list):
                print(f"[CRITICAL ERROR] Existing anonymization report file is not a list. Found type: {type(report_entries)}. Exiting.")
                sys.exit(1)
            print(f"   Found {len(report_entries)} existing anonymization report entries.")

            # Determine processed files based on catalog entries (assuming catalog is the primary indicator)
            processed_md_files = set(entry.get('processed_md_path') for entry in catalog_entries if 'processed_md_path' in entry)
            print(f"   Identified {len(processed_md_files)} already processed Markdown files (based on catalog).")
        else:
            print("   Full refresh mode: Initializing empty results lists and skipping checkpoint loading.")
        print("-" * 60)

        # --- Find Markdown Files from Stage 2 ---
        print(f"[8] Searching for Stage 2 Markdown files in: {stage2_md_dir_smb_path}...") # Renumbered
        md_files_to_process = find_md_files(stage2_md_dir_smb_path)
        if not md_files_to_process:
            print("   No Markdown files found to process.")
            print("\n" + "="*60)
            print(f"--- Stage 3 Completed (No new Markdown files found) ---")
            print("="*60 + "\n")
            # No sys.exit(0) here, allow finally block to run
        else:
            print("-" * 60)

            # --- Process Each Markdown File ---
            print(f"[9] Processing {len(md_files_to_process)} Markdown files...") # Renumbered
            new_entries_count = 0
            skipped_count = 0
            error_count = 0
            current_token = None
            token_expiry_time = time.time() # Initialize to ensure first token fetch

            for i, md_smb_path in enumerate(md_files_to_process):
                start_time = time.time()
                print(f"\n--- Processing file {i+1}/{len(md_files_to_process)} ---")
                print(f"   MD File Path (SMB): {md_smb_path}")

                # Skip if already processed (only in incremental mode)
                if not is_full_refresh and md_smb_path in processed_md_files:
                    print("   File already processed (found in existing results). Skipping.")
                    skipped_count += 1
                    continue

                # Check token expiry
                if time.time() >= token_expiry_time:
                    print("   OAuth token expired or not yet fetched. Requesting new token...")
                    current_token = get_oauth_token()
                    if not current_token:
                        print("   [ERROR] Failed to obtain OAuth token. Skipping file.")
                        error_count += 1
                        continue
                    token_expiry_time = time.time() + (50 * 60) # Assume 50 min validity

                try:
                    client = OpenAI(
                        base_url=GPT_CONFIG['base_url'],
                        api_key=current_token,
                    )
                except Exception as e:
                    print(f"   [ERROR] Failed to initialize OpenAI client: {e}. Skipping file.")
                    error_count += 1
                    continue

                markdown_content = read_text_from_nas(md_smb_path)
                if not markdown_content:
                    print(f"   [ERROR] Failed to read Markdown content from {md_smb_path}. Skipping file.")
                    error_count += 1
                    continue

                # --- Call GPT Summarizer (with detail level and source) ---
                # Set detail level based on document source
                current_detail_level = 'standard'
                description, usage, anonymization_data = call_gpt_summarizer(client, markdown_content, current_detail_level, DOCUMENT_SOURCE)

                # Check if None was returned (indicates an error in call_gpt_summarizer)
                if description is None or usage is None or anonymization_data is None:
                    print(f"   [ERROR] Failed to get summaries or anonymization data from GPT for {md_smb_path}. Skipping file.")
                    error_count += 1
                    continue # Skip if summarization fails

                # --- Prepare Data for Output ---
                md_filename = os.path.basename(md_smb_path)
                original_base_name = os.path.splitext(md_filename)[0]
                # Handle potential chunk suffixes added in Stage 2
                if '_chunk_' in original_base_name:
                     original_base_name = original_base_name.split('_chunk_')[0]

                original_metadata = metadata_lookup.get(original_base_name)
                if not original_metadata:
                    print(f"   [WARNING] Could not find original metadata for base name '{original_base_name}' derived from {md_filename}. Skipping file.")
                    error_count += 1
                    continue

                # Extract key fields (ensure they exist in metadata)
                doc_name = original_metadata.get('file_name', md_filename) # Fallback to md_filename if needed
                doc_base_name = os.path.splitext(doc_name)[0] # Get filename without extension
                doc_source = DOCUMENT_SOURCE # Use configured source
                doc_type = DOCUMENT_TYPE   # Use configured type

                # --- Load corresponding JSON file(s) for page spans ---
                json_data = None
                
                # Check if this is a chunked file
                if '_chunk_' in original_base_name:
                    # For chunked files, we need to load all chunk JSONs
                    print(f"   Detected chunked file. Loading all chunk JSONs...")
                    json_data = []
                    
                    # Get the base directory and original file name without chunk suffix
                    base_dir = os.path.dirname(md_smb_path)
                    base_name_without_chunk = original_base_name.split('_chunk_')[0]
                    
                    # Look for all chunk JSON files
                    chunk_index = 1
                    while True:
                        chunk_json_name = f"{base_name_without_chunk}_chunk_{chunk_index}.json"
                        chunk_json_path = os.path.join(base_dir, chunk_json_name).replace('\\', '/')
                        
                        try:
                            chunk_json = read_json_from_nas(chunk_json_path)
                            if chunk_json:
                                json_data.append(chunk_json)
                                print(f"   Loaded chunk {chunk_index} JSON")
                                chunk_index += 1
                            else:
                                break  # No more chunks
                        except:
                            break  # No more chunks
                    
                    if not json_data:
                        print(f"   [WARNING] No chunk JSON files found, will use regex fallback")
                        json_data = None
                else:
                    # Single file - load single JSON
                    json_smb_path = md_smb_path.replace('.md', '.json')
                    print(f"   Looking for JSON file: {os.path.basename(json_smb_path)}")
                    
                    try:
                        json_data = read_json_from_nas(json_smb_path)
                        if json_data:
                            print(f"   Successfully loaded JSON data for page extraction")
                        else:
                            print(f"   [WARNING] Failed to load JSON data, will use regex fallback")
                    except Exception as e:
                        print(f"   [WARNING] Error loading JSON file: {e}, will use regex fallback")

                # --- Split content by pages ---
                if json_data:
                    page_contents = split_content_by_pages_using_json(markdown_content, json_data)
                else:
                    page_contents = split_content_by_pages_regex(markdown_content)
                
                # --- Generate embeddings for document_usage and document_description ---
                print("   Generating embeddings for catalog entry...")
                catalog_embeddings = generate_embeddings([usage, description], client)
                
                if len(catalog_embeddings) >= 2:
                    usage_embedding = catalog_embeddings[0]
                    description_embedding = catalog_embeddings[1]
                else:
                    print("   [WARNING] Failed to generate catalog embeddings. Using None.")
                    usage_embedding = None
                    description_embedding = None
                
                # Create Catalog Entry (with embeddings)
                catalog_entry = {
                    "document_source": doc_source,
                    "document_type": doc_type,
                    "document_name": doc_name,
                    "document_description": description, # Use the returned description
                    "document_usage": usage, # Use the returned usage
                    "document_usage_embedding": usage_embedding,  # Add embedding
                    "document_description_embedding": description_embedding,  # Add embedding
                    # Use date_created from Stage 1 metadata (original creation or fallback to modified)
                    "date_created": original_metadata.get('date_created'),
                    "date_last_modified": original_metadata.get('date_last_modified'),
                    "file_name": original_metadata.get('file_name'), # Redundant but matches original structure
                    "file_type": os.path.splitext(original_metadata.get('file_name', ''))[1],
                    "file_size": original_metadata.get('file_size'),
                    "file_path": original_metadata.get('file_path'),
                    "file_link": f"//{NAS_PARAMS['ip']}/{NAS_PARAMS['share']}/{original_metadata.get('file_path', '')}",
                    "processed_md_path": md_smb_path # For checkpointing
                }

                # Create Content Entries (one per page)
                new_content_entries = []
                for page_num, page_content in page_contents:
                    content_entry = {
                        "document_source": doc_source,
                        "document_type": doc_type,
                        "document_name": doc_name,
                        "section_id": page_num,  # Use page number as section_id
                        "section_name": f"{doc_base_name}_page_{page_num}",  # Include page number in section name
                        "section_summary": f"Page {page_num} of {doc_name}",  # Simple page summary
                        "section_content": page_content,  # Page-specific content
                        "page_number": page_num,  # Explicit page number field
                        "date_created": datetime.now(timezone.utc).isoformat()  # Add creation date
                    }
                    new_content_entries.append(content_entry)
                
                # --- Append and Save Both Entries (Atomic-like operation for checkpointing) ---
                print(f"   Appending new catalog entry for: {doc_name}")
                catalog_entries.append(catalog_entry)
                print(f"   Appending {len(new_content_entries)} content entries for: {doc_name}")
                content_entries.extend(new_content_entries)

                # Save Catalog Entries
                catalog_save_success = write_json_to_nas(stage3_catalog_output_smb_path, catalog_entries)
                # Save Content Entries
                content_save_success = write_json_to_nas(stage3_content_output_smb_path, content_entries)

                if catalog_save_success and content_save_success:
                    print(f"   Successfully saved updated catalog ({len(catalog_entries)}) and content ({len(content_entries)}) entries to NAS.")

                    # Prepare Anonymization Report Entry (Copy catalog entry and add DLP fields)
                    report_entry = catalog_entry.copy() # Create a copy
                    report_entry['dlp_anonymized_or_flagged'] = anonymization_data.get('found_anonymized', False)
                    report_entry['dlp_findings'] = []
                    if report_entry['dlp_anonymized_or_flagged']:
                        for entity in anonymization_data.get('entities', []):
                            entity_type = entity.get('entity_type', 'UNKNOWN_TYPE')
                            raw_string = entity.get('raw_string', 'UNKNOWN_VALUE')
                            report_entry['dlp_findings'].append(f"ANONYMIZED: {entity_type} - {raw_string}")
                    # TODO: Add processing for other potential flags (detection_results, content_filter_results) here if needed in the future

                    # Append and Save Anonymization Report Entry (Standard JSON list)
                    print(f"   Appending new report entry for: {doc_name}")
                    report_entries.append(report_entry)
                    report_save_success = write_json_to_nas(stage3_anonymization_report_smb_path, report_entries)

                    if report_save_success:
                        print(f"   Successfully saved updated anonymization report ({len(report_entries)}) entries to NAS.")
                    else:
                        # Log warning but don't necessarily stop the whole process? Or make it critical?
                        # For now, log warning and rollback the append for consistency.
                        print(f"   [WARNING] Failed to save anonymization report file to NAS after processing {md_filename}. Report may be out of sync.")
                        report_entries.pop() # Rollback append

                    # Only increment count and mark processed if all saves were successful
                    # (or if report save failure is considered non-critical)
                    # Assuming report save failure is non-critical for now:
                    new_entries_count += 1
                    processed_md_files.add(md_smb_path) # Mark as processed only if all saves succeed (or primary ones + report append attempted)
                else:
                    print(f"   [CRITICAL ERROR] Failed to save one or both primary output files (catalog/content) to NAS after processing {md_filename}. Stopping.")
                    error_count += 1
                    # Rollback the appends for consistency before exiting
                    catalog_entries.pop()
                    content_entries.pop()
                    print(f"   Rolled back entries for {doc_name}.")
                    # Attempt to save the rolled-back state (optional, might fail again)
                    # write_json_to_nas(stage3_catalog_output_smb_path, catalog_entries)
                    # write_json_to_nas(stage3_content_output_smb_path, content_entries)
                    sys.exit(1) # Exit on critical save failure

                end_time = time.time()
                print(f"--- Finished file {i+1} (Success) ---")
                print(f"--- Time taken: {end_time - start_time:.2f} seconds ---")

            # --- Final Summary ---
            print("\n" + "="*60)
            print(f"--- Stage 3 Processing Summary ---")
            print(f"   Total Markdown files found: {len(md_files_to_process)}")
            print(f"   Files skipped (already processed): {skipped_count}")
            print(f"   New entry pairs (catalog/content) added: {new_entries_count}")
            print(f"   Errors encountered: {error_count}")
            print(f"   Total entries in '{STAGE3_CATALOG_OUTPUT_FILENAME}': {len(catalog_entries)}")
            print(f"   Total entries in '{STAGE3_CONTENT_OUTPUT_FILENAME}': {len(content_entries)}")
            print(f"   Total entries in '{STAGE3_ANONYMIZATION_REPORT_FILENAME}': {len(report_entries)}") # Updated report info
            print("="*60 + "\n")

            if error_count > 0:
                print(f"[WARNING] {error_count} files encountered errors during processing. Check logs above.")

        print(f"--- Stage 3 Completed ---")
        print(f"--- End of Main Processing for Stage 3 ---")

    # --- Cleanup (Executes regardless of success/failure in the try block) ---
    finally:
        print("\n--- Cleaning up ---")
        # Clean up the temporary certificate file
        if temp_cert_file_path and os.path.exists(temp_cert_file_path):
            try:
                os.remove(temp_cert_file_path)
                print(f"   Removed temporary CA bundle file: {temp_cert_file_path}")
            except OSError as e:
                 print(f"   [WARNING] Failed to remove temporary CA bundle file {temp_cert_file_path}: {e}")

        # Restore original environment variables
        # Restore REQUESTS_CA_BUNDLE
        current_requests_bundle = os.environ.get('REQUESTS_CA_BUNDLE')
        if original_requests_ca_bundle is None:
            # If it didn't exist originally, remove it if we set it
            if current_requests_bundle == temp_cert_file_path:
                 print("   Unsetting REQUESTS_CA_BUNDLE environment variable.")
                 # Check if key exists before deleting
                 if 'REQUESTS_CA_BUNDLE' in os.environ:
                     del os.environ['REQUESTS_CA_BUNDLE']
        else:
            # If it existed originally, restore its value if it changed
            if current_requests_bundle != original_requests_ca_bundle:
                 print(f"   Restoring original REQUESTS_CA_BUNDLE environment variable.")
                 os.environ['REQUESTS_CA_BUNDLE'] = original_requests_ca_bundle

        # Restore SSL_CERT_FILE
        current_ssl_cert = os.environ.get('SSL_CERT_FILE')
        if original_ssl_cert_file is None:
            # If it didn't exist originally, remove it if we set it
            if current_ssl_cert == temp_cert_file_path:
                 print("   Unsetting SSL_CERT_FILE environment variable.")
                 # Check if key exists before deleting
                 if 'SSL_CERT_FILE' in os.environ:
                     del os.environ['SSL_CERT_FILE']
        else:
            # If it existed originally, restore its value if it changed
            if current_ssl_cert != original_ssl_cert_file:
                 print(f"   Restoring original SSL_CERT_FILE environment variable.")
                 os.environ['SSL_CERT_FILE'] = original_ssl_cert_file

# ==============================================================================
# --- Script Entry Point ---
# ==============================================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print(f"--- Running Stage 3: Generate Document Summaries ---")
    print(f"--- Document Source: {DOCUMENT_SOURCE} ---")
    print(f"--- Document Type: {DOCUMENT_TYPE} ---")
    print("="*60 + "\n")

    # --- Initialize SMB Client ---
    print("[1] Initializing SMB Client...")
    if not initialize_smb_client():
        sys.exit(1) # Exit if SMB client fails
    print("-" * 60)

    # --- Define Paths ---
    print("[2] Defining NAS Paths...")
    source_base_dir_relative = os.path.join(NAS_OUTPUT_FOLDER_PATH, DOCUMENT_SOURCE).replace('\\', '/')
    source_base_dir_smb = f"//{NAS_PARAMS['ip']}/{NAS_PARAMS['share']}/{source_base_dir_relative}"
    stage1_metadata_smb_path = os.path.join(source_base_dir_smb, STAGE1_METADATA_FILENAME).replace('\\', '/')
    stage2_md_dir_smb_path = os.path.join(source_base_dir_smb, STAGE2_OUTPUT_SUBFOLDER).replace('\\', '/')
    stage3_catalog_output_smb_path = os.path.join(source_base_dir_smb, STAGE3_CATALOG_OUTPUT_FILENAME).replace('\\', '/') # Updated path
    stage3_content_output_smb_path = os.path.join(source_base_dir_smb, STAGE3_CONTENT_OUTPUT_FILENAME).replace('\\', '/') # New path
    stage3_anonymization_report_smb_path = os.path.join(source_base_dir_smb, STAGE3_ANONYMIZATION_REPORT_FILENAME).replace('\\', '/') # New report path
    ca_bundle_smb_path = os.path.join(f"//{NAS_PARAMS['ip']}/{NAS_PARAMS['share']}/{NAS_OUTPUT_FOLDER_PATH}", CA_BUNDLE_FILENAME).replace('\\', '/')

    print(f"   Source Base Dir (SMB): {source_base_dir_smb}")
    print(f"   Stage 1 Metadata File (SMB): {stage1_metadata_smb_path}")
    print(f"   Stage 2 MD Files Dir (SMB): {stage2_md_dir_smb_path}")
    print(f"   Stage 3 Catalog Output File (SMB): {stage3_catalog_output_smb_path}") # Updated print
    print(f"   Stage 3 Content Output File (SMB): {stage3_content_output_smb_path}") # New print
    print(f"   Stage 3 Anonymization Report (SMB): {stage3_anonymization_report_smb_path}") # New print
    print(f"   CA Bundle File (SMB): {ca_bundle_smb_path}")
    # Add Refresh Flag Path
    refresh_flag_file_name = '_FULL_REFRESH.flag'
    refresh_flag_smb_path = os.path.join(source_base_dir_smb, refresh_flag_file_name).replace('\\', '/')
    print(f"   Refresh Flag File (SMB): {refresh_flag_smb_path}")
    print("-" * 60)

    # --- Check for Skip Flag from Stage 1 ---
    print("[3] Checking for skip flag from Stage 1...")
    skip_flag_file_name = '_SKIP_SUBSEQUENT_STAGES.flag'
    skip_flag_smb_path = os.path.join(source_base_dir_smb, skip_flag_file_name).replace('\\', '/')
    print(f"   Checking for flag file: {skip_flag_smb_path}")
    should_skip = False
    try:
        if smbclient.path.exists(skip_flag_smb_path):
            print(f"   Skip flag file found. Stage 1 indicated no files to process.")
            should_skip = True
        else:
            print(f"   Skip flag file not found. Proceeding with Stage 3.")
    except smbclient.SambaClientError as e:
        print(f"   [WARNING] SMB Error checking for skip flag file '{skip_flag_smb_path}': {e}")
        print(f"   Proceeding with Stage 3, but there might be an issue accessing NAS.")
        # Continue execution, assuming no skip if flag check fails
    except Exception as e:
        print(f"   [WARNING] Unexpected error checking for skip flag file '{skip_flag_smb_path}': {e}")
        print(f"   Proceeding with Stage 3.")
        # Continue execution
    print("-" * 60)

    # --- Execute Main Processing if Not Skipped ---
    if should_skip:
        print("\n" + "="*60)
        print(f"--- Stage 3 Skipped (No files to process from Stage 1) ---")
        print("="*60 + "\n")
    else:
        # Call the main processing function only if not skipping
        main_processing_stage3(stage1_metadata_smb_path, stage2_md_dir_smb_path,
                               stage3_catalog_output_smb_path, stage3_content_output_smb_path,
                               stage3_anonymization_report_smb_path, # Pass new report path
                               ca_bundle_smb_path, refresh_flag_smb_path)

    # Script ends naturally here if skipped or after main_processing completes
